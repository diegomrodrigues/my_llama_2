{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegomrodrigues/my_llama_2/blob/main/LLaMA%202%20SFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Custom LLaMA Implementation"
      ],
      "metadata": {
        "id": "H6nCbDvlarog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Config"
      ],
      "metadata": {
        "id": "qK6iKRA7Zeht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PretrainedConfig\n",
        "from typing import Dict, Any, Optional, Union\n",
        "\n",
        "class LlamaConfig(PretrainedConfig):\n",
        "    \"\"\"\n",
        "    Configuração para o modelo LLaMA.\n",
        "\n",
        "    Esta classe define todos os parâmetros necessários para construir e configurar\n",
        "    um modelo LLaMA. Herda de PretrainedConfig da biblioteca Transformers.\n",
        "\n",
        "    Atributos:\n",
        "        vocab_size (int): Tamanho do vocabulário do modelo.\n",
        "        hidden_size (int): Dimensão dos vetores de estado oculto e embeddings.\n",
        "        intermediate_size (int): Dimensão da camada intermediária no MLP.\n",
        "        num_hidden_layers (int): Número de camadas de transformer no modelo.\n",
        "        num_attention_heads (int): Número de cabeças de atenção em cada camada.\n",
        "        num_key_value_heads (int): Número de cabeças para key e value (para atenção agrupada).\n",
        "        hidden_act (str): Função de ativação usada no MLP.\n",
        "        max_position_embeddings (int): Número máximo de posições para embeddings.\n",
        "        initializer_range (float): Desvio padrão da distribuição normal para inicialização de pesos.\n",
        "        rms_norm_eps (float): Epsilon usado na normalização RMS.\n",
        "        use_cache (bool): Se deve usar cache para geração incremental.\n",
        "        pad_token_id (int): ID do token de padding.\n",
        "        bos_token_id (int): ID do token de início de sequência.\n",
        "        eos_token_id (int): ID do token de fim de sequência.\n",
        "        pretraining_tp (int): Grau de paralelismo de tensor usado no pré-treinamento.\n",
        "        tie_word_embeddings (bool): Se deve compartilhar pesos entre embeddings de entrada e saída.\n",
        "        rope_theta (float): Valor theta para RoPE (Rotary Position Embedding).\n",
        "        rope_scaling (Dict[str, Any]): Configuração de escala para RoPE.\n",
        "        attention_bias (bool): Se deve usar bias nos cálculos de atenção.\n",
        "        attention_dropout (float): Taxa de dropout aplicada na camada de atenção.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(\n",
        "        ...     vocab_size=32000,\n",
        "        ...     hidden_size=4096,\n",
        "        ...     intermediate_size=11008,\n",
        "        ...     num_hidden_layers=32,\n",
        "        ...     num_attention_heads=32,\n",
        "        ... )\n",
        "        >>> print(config)\n",
        "    \"\"\"\n",
        "\n",
        "    model_type = \"llama\"\n",
        "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int = 32000,\n",
        "        hidden_size: int = 4096,\n",
        "        intermediate_size: int = 11008,\n",
        "        num_hidden_layers: int = 32,\n",
        "        num_attention_heads: int = 32,\n",
        "        num_key_value_heads: Optional[int] = None,\n",
        "        hidden_act: str = \"silu\",\n",
        "        rotary_emb_base: float = 10000.0,\n",
        "        rotary_emb_fraction: float = 1.0,\n",
        "        max_position_embeddings: int = 2048,\n",
        "        initializer_range: float = 0.02,\n",
        "        rms_norm_eps: float = 1e-6,\n",
        "        use_cache: bool = True,\n",
        "        pad_token_id: int = -1,\n",
        "        bos_token_id: int = 1,\n",
        "        eos_token_id: int = 2,\n",
        "        pretraining_tp: int = 1,\n",
        "        tie_word_embeddings: bool = False,\n",
        "        rope_theta: float = 10000.0,\n",
        "        rope_scaling: Optional[Dict[str, Union[float, str]]] = None,\n",
        "        attention_bias: bool = False,\n",
        "        mlp_bias: bool = False,\n",
        "        attention_dropout: float = 0.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.hidden_size = hidden_size\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.num_key_value_heads = num_key_value_heads if num_key_value_heads is not None else num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.rotary_emb_base = rotary_emb_base\n",
        "        self.rotary_emb_fraction = rotary_emb_fraction\n",
        "        self.initializer_range = initializer_range\n",
        "        self.rms_norm_eps = rms_norm_eps\n",
        "        self.pretraining_tp = pretraining_tp\n",
        "        self.use_cache = use_cache\n",
        "        self.rope_theta = rope_theta\n",
        "        self.rope_scaling = rope_scaling\n",
        "        self.attention_bias = attention_bias\n",
        "        self.mlp_bias = mlp_bias\n",
        "        self.attention_dropout = attention_dropout\n",
        "\n",
        "        super().__init__(\n",
        "            pad_token_id=pad_token_id,\n",
        "            bos_token_id=bos_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            tie_word_embeddings=tie_word_embeddings,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def head_dim(self) -> int:\n",
        "        \"\"\"\n",
        "        Retorna a dimensão de cada cabeça de atenção.\n",
        "\n",
        "        Returns:\n",
        "            int: Dimensão de cada cabeça de atenção.\n",
        "        \"\"\"\n",
        "        return self.hidden_size // self.num_attention_heads\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Converte a configuração para um dicionário.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: Dicionário contendo todos os parâmetros da configuração.\n",
        "        \"\"\"\n",
        "        output = super().to_dict()\n",
        "        output[\"head_dim\"] = self.head_dim\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"LlamaConfig(vocab_size={self.vocab_size}, \"\n",
        "                f\"hidden_size={self.hidden_size}, \"\n",
        "                f\"intermediate_size={self.intermediate_size}, \"\n",
        "                f\"num_hidden_layers={self.num_hidden_layers}, \"\n",
        "                f\"num_attention_heads={self.num_attention_heads}, \"\n",
        "                f\"max_position_embeddings={self.max_position_embeddings})\")"
      ],
      "metadata": {
        "id": "JteD23VEpx7_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Rotary Embedding"
      ],
      "metadata": {
        "id": "wSnw58m_ZlLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class LlamaRotaryEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,  # Dimensão do embedding\n",
        "        max_position_embeddings: int = 2048,  # Comprimento máximo da sequência\n",
        "        base: float = 10000.0,  # Base para o cálculo das frequências\n",
        "        device: Optional[torch.device] = None,\n",
        "        rope_scaling: Optional[Dict[str, Union[float, str]]] = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.base = base\n",
        "\n",
        "        # Calcula as frequências inversas para RoPE\n",
        "        # Dimensão: [dim/2]\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "        if rope_scaling is not None:\n",
        "            scaling_type = rope_scaling[\"type\"]\n",
        "            scaling_factor = rope_scaling[\"factor\"]\n",
        "            if scaling_type == \"linear\":\n",
        "                self.seq_len_scaling = scaling_factor\n",
        "            else:\n",
        "                raise ValueError(f\"Tipo de scaling desconhecido: {scaling_type}\")\n",
        "        else:\n",
        "            self.seq_len_scaling = 1.0\n",
        "\n",
        "        # Cache para sequência máxima\n",
        "        self.max_seq_len_cached = max_position_embeddings\n",
        "\n",
        "        # Para garantir compatibilidade com HF\n",
        "        self._build_cache()\n",
        "\n",
        "    def _build_cache(self):\n",
        "        seq_len = self.max_seq_len_cached\n",
        "        # Dimensão: [max_seq_len_cached]\n",
        "        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
        "        if self.seq_len_scaling != 1.0:\n",
        "            t = t / self.seq_len_scaling\n",
        "\n",
        "        # Dimensão: [max_seq_len_cached, dim/2]\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "\n",
        "        # Dimensão: [max_seq_len_cached, dim]\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "\n",
        "        # Dimensão: [1, 1, max_seq_len_cached, dim]\n",
        "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, seq_len: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Calcula os embeddings rotacionais para as posições dadas.\n",
        "\n",
        "        Args:\n",
        "            x: Tensor de entrada (batch_size, seq_len, num_heads, head_dim)\n",
        "            position_ids: IDs das posições (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            Tupla de tensores cosseno e seno para os embeddings rotacionais\n",
        "        \"\"\"\n",
        "        if seq_len is None:\n",
        "            seq_len = x.shape[2]\n",
        "\n",
        "        # Verifica se é necessário recalcular o cache para sequências mais longas\n",
        "        if seq_len > self.max_seq_len_cached:\n",
        "            self._update_cache(seq_len)\n",
        "\n",
        "        # Seleciona os valores de cosseno e seno correspondentes às posições\n",
        "        # Dimensão: [1, 1, seq_len, dim]\n",
        "        cos = self.cos_cached[:, :, :seq_len, :]\n",
        "        sin = self.sin_cached[:, :, :seq_len, :]\n",
        "\n",
        "        return (cos.to(x.device), sin.to(x.device))\n",
        "\n",
        "\n",
        "    def _update_cache(self, max_position: int):\n",
        "        \"\"\"\n",
        "        Atualiza o cache de cossenos e senos para uma sequência mais longa.\n",
        "\n",
        "        Args:\n",
        "            max_position: Nova posição máxima a ser suportada\n",
        "        \"\"\"\n",
        "        self.max_seq_len_cached = max_position\n",
        "        # Dimensão: [max_seq_len_cached]\n",
        "        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
        "        # Dimensão: [max_seq_len_cached, dim/2]\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "        if self.rope_type == \"linear\":\n",
        "            freqs = freqs * self.scaling_factor\n",
        "        # Dimensão: [max_seq_len_cached, dim]\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        # Dimensão: [1, 1, max_seq_len_cached, dim]\n",
        "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n",
        "\n",
        "\n",
        "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Rotaciona metade das dimensões do tensor.\n",
        "    Usado como parte do processo de aplicação do RoPE.\n",
        "\n",
        "    Args:\n",
        "        x: Tensor de entrada\n",
        "\n",
        "    Returns:\n",
        "        Tensor com metade das dimensões rotacionadas\n",
        "    \"\"\"\n",
        "    x1, x2 = x.chunk(2, dim=-1)\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Aplica os embeddings posicionais rotacionais aos tensores de query e key.\n",
        "\n",
        "    Args:\n",
        "        q: Tensor de query\n",
        "        k: Tensor de key\n",
        "        cos: Tensor de cossenos dos embeddings rotacionais\n",
        "        sin: Tensor de senos dos embeddings rotacionais\n",
        "\n",
        "    Returns:\n",
        "        Tupla de tensores q e k com RoPE aplicado\n",
        "    \"\"\"\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed"
      ],
      "metadata": {
        "id": "SbFgzhF5rnri"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA MLP"
      ],
      "metadata": {
        "id": "NdiXFZkaZo4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional\n",
        "\n",
        "class LlamaMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementa a camada de Perceptron Multicamadas (MLP) do modelo LLaMA.\n",
        "\n",
        "    Esta classe realiza as transformações não-lineares nos estados ocultos do modelo,\n",
        "    utilizando projeções lineares e uma função de ativação. Suporta implementações\n",
        "    com e sem tensor parallelism.\n",
        "\n",
        "    Atributos:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        hidden_size (int): Tamanho do espaço oculto de entrada e saída.\n",
        "        intermediate_size (int): Tamanho do espaço intermediário onde ocorre a transformação principal.\n",
        "        gate_proj (nn.Linear): Projeção linear para o mecanismo de gate.\n",
        "        up_proj (nn.Linear): Projeção linear de expansão.\n",
        "        down_proj (nn.Linear): Projeção linear de contração.\n",
        "        act_fn (callable): Função de ativação não-linear.\n",
        "\n",
        "    Args:\n",
        "        config: Um objeto de configuração contendo os parâmetros do modelo.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(hidden_size=768, intermediate_size=3072)\n",
        "        >>> mlp = LlamaMLP(config)\n",
        "        >>> input_tensor = torch.randn(1, 10, 768)  # [batch_size, seq_length, hidden_size]\n",
        "        >>> output = mlp(input_tensor)\n",
        "        >>> print(output.shape)\n",
        "        torch.Size([1, 10, 768])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "\n",
        "        # Projeção de gate: hidden_size -> intermediate_size\n",
        "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
        "\n",
        "        # Projeção up: hidden_size -> intermediate_size\n",
        "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
        "\n",
        "        # Projeção down: intermediate_size -> hidden_size\n",
        "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
        "\n",
        "        # Função de ativação (geralmente SiLU/Swish)\n",
        "        self.act_fn = nn.SiLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward da camada MLP.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Tensor de entrada com shape [batch_size, seq_length, hidden_size].\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor de saída com shape [batch_size, seq_length, hidden_size].\n",
        "\n",
        "        Raises:\n",
        "            ValueError: Se as dimensões do tensor de entrada não forem compatíveis.\n",
        "        \"\"\"\n",
        "        # Verificação das dimensões de entrada\n",
        "        if x.dim() != 3 or x.size(-1) != self.hidden_size:\n",
        "            raise ValueError(f\"Entrada esperada de shape [batch_size, seq_length, {self.hidden_size}], \"\n",
        "                             f\"mas recebeu {x.shape}\")\n",
        "\n",
        "        if self.config.pretraining_tp > 1:\n",
        "            # Implementação para tensor parallelism (TP)\n",
        "            slice = self.intermediate_size // self.config.pretraining_tp\n",
        "\n",
        "            # Divide os pesos das projeções em fatias\n",
        "            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n",
        "            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n",
        "            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n",
        "\n",
        "            # Aplica as projeções em paralelo\n",
        "            # Cada operação: [batch_size, seq_length, slice]\n",
        "            gate_proj = torch.cat(\n",
        "                [nn.functional.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)],\n",
        "                dim=-1\n",
        "            )\n",
        "            up_proj = torch.cat(\n",
        "                [nn.functional.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)],\n",
        "                dim=-1\n",
        "            )\n",
        "\n",
        "            # Aplica a função de ativação e multiplicação elemento a elemento\n",
        "            # Dimensão: [batch_size, seq_length, intermediate_size]\n",
        "            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n",
        "\n",
        "            # Aplica a projeção down em paralelo\n",
        "            # Cada operação: [batch_size, seq_length, hidden_size // pretraining_tp]\n",
        "            down_proj = [\n",
        "                nn.functional.linear(intermediate_states[i], down_proj_slices[i])\n",
        "                for i in range(self.config.pretraining_tp)\n",
        "            ]\n",
        "\n",
        "            # Soma os resultados das projeções down\n",
        "            # Dimensão final: [batch_size, seq_length, hidden_size]\n",
        "            down_proj = sum(down_proj)\n",
        "\n",
        "        else:\n",
        "            # Implementação padrão sem tensor parallelism\n",
        "\n",
        "            # Aplica as projeções gate e up\n",
        "            # Dimensões: [batch_size, seq_length, intermediate_size]\n",
        "            gate_proj = self.gate_proj(x)\n",
        "            up_proj = self.up_proj(x)\n",
        "\n",
        "            # Aplica a função de ativação no gate e multiplica pelo resultado de up\n",
        "            # Dimensão: [batch_size, seq_length, intermediate_size]\n",
        "            intermediate_states = self.act_fn(gate_proj) * up_proj\n",
        "\n",
        "            # Aplica a projeção down\n",
        "            # Dimensão final: [batch_size, seq_length, hidden_size]\n",
        "            down_proj = self.down_proj(intermediate_states)\n",
        "\n",
        "        return down_proj\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"LlamaMLP(hidden_size={self.hidden_size}, \"\n",
        "                f\"intermediate_size={self.intermediate_size}, \"\n",
        "                f\"act_fn={self.act_fn.__class__.__name__})\")"
      ],
      "metadata": {
        "id": "jf2uEPW8vBiO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA RMS Norm"
      ],
      "metadata": {
        "id": "4h29-LUbZul5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LlamaRMSNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    LlamaRMSNorm é uma variante de normalização de camada utilizada no modelo LLaMA.\n",
        "\n",
        "    Esta normalização usa a raiz quadrada da média dos quadrados (RMS) para normalizar\n",
        "    os inputs, em vez da média e variância usadas na normalização de camada padrão.\n",
        "\n",
        "    Atributos:\n",
        "        weight (nn.Parameter): Parâmetro aprendível para escala.\n",
        "        variance_epsilon (float): Pequeno valor adicionado ao denominador para estabilidade numérica.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): Dimensão do espaço oculto a ser normalizado.\n",
        "        eps (float, opcional): Epsilon para estabilidade numérica. Padrão é 1e-6.\n",
        "\n",
        "    Forma do Input:\n",
        "        - Input: (batch_size, seq_length, hidden_size)\n",
        "        - Output: (batch_size, seq_length, hidden_size)\n",
        "\n",
        "    Exemplo:\n",
        "        >>> rms_norm = LlamaRMSNorm(hidden_size=768, eps=1e-6)\n",
        "        >>> input_tensor = torch.randn(32, 50, 768)  # (batch_size, seq_length, hidden_size)\n",
        "        >>> normalized_tensor = rms_norm(input_tensor)\n",
        "        >>> print(normalized_tensor.shape)\n",
        "        torch.Size([32, 50, 768])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Aplica a normalização RMS ao tensor de entrada.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Tensor de entrada a ser normalizado.\n",
        "                Shape: (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor normalizado.\n",
        "                Shape: (batch_size, seq_length, hidden_size)\n",
        "        \"\"\"\n",
        "        input_dtype = hidden_states.dtype\n",
        "        hidden_states = hidden_states.to(torch.float32)\n",
        "\n",
        "        # Calcula a variância (média dos quadrados)\n",
        "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
        "\n",
        "        # Normaliza usando RMS\n",
        "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
        "\n",
        "        # Aplica o peso aprendível\n",
        "        return (self.weight * hidden_states).to(input_dtype)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        \"\"\"\n",
        "        Retorna uma representação de string dos principais parâmetros.\n",
        "\n",
        "        Returns:\n",
        "            str: String representando os parâmetros do módulo.\n",
        "        \"\"\"\n",
        "        return f\"hidden_size={self.weight.numel()}, eps={self.variance_epsilon}\""
      ],
      "metadata": {
        "id": "y7nm-P9EU2Sb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Attention"
      ],
      "metadata": {
        "id": "p1247aOeZxZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vanilla Attention"
      ],
      "metadata": {
        "id": "olhbgD_QZ1Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class LlamaAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementa o mecanismo de atenção multi-cabeça do modelo LLaMA.\n",
        "\n",
        "    Esta classe realiza a operação de auto-atenção, permitindo que o modelo foque\n",
        "    em diferentes partes da sequência de entrada. Suporta diferentes implementações\n",
        "    de atenção e otimizações como grouped-query attention.\n",
        "\n",
        "    Atributos:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        layer_idx (int): Índice da camada atual.\n",
        "        hidden_size (int): Dimensão do espaço oculto.\n",
        "        num_heads (int): Número de cabeças de atenção.\n",
        "        head_dim (int): Dimensão de cada cabeça de atenção.\n",
        "        num_key_value_heads (int): Número de cabeças para key e value (pode ser menor que num_heads).\n",
        "        max_position_embeddings (int): Número máximo de posições para embeddings.\n",
        "        rotary_emb (LlamaRotaryEmbedding): Instância para aplicar embeddings rotacionais.\n",
        "\n",
        "    Args:\n",
        "        config: Um objeto de configuração contendo os parâmetros do modelo.\n",
        "        layer_idx (Optional[int]): Índice da camada. Necessário para algumas otimizações.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(hidden_size=512, num_attention_heads=8)\n",
        "        >>> attention = LlamaAttention(config, layer_idx=0)\n",
        "        >>> hidden_states = torch.randn(1, 10, 512)  # [batch_size, seq_length, hidden_size]\n",
        "        >>> attention_mask = torch.ones(1, 1, 10, 10)  # [batch_size, 1, seq_length, seq_length]\n",
        "        >>> output, _ = attention(hidden_states, attention_mask=attention_mask)\n",
        "        >>> print(output.shape)\n",
        "        torch.Size([1, 10, 512])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "        # Verifica se as dimensões são compatíveis\n",
        "        if self.head_dim * self.num_heads != self.hidden_size:\n",
        "            raise ValueError(f\"hidden_size deve ser divisível por num_heads. \"\n",
        "                             f\"Got {self.hidden_size} e {self.num_heads}.\")\n",
        "\n",
        "        # Projections para query, key, value e output\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_position_embeddings=self.max_position_embeddings,\n",
        "            base=config.rotary_emb_base,\n",
        "            rope_scaling=config.rope_scaling\n",
        "        )\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        \"\"\"Reshape and transpose tensor for attention computation.\"\"\"\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: bool = False,\n",
        "        use_cache: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do mecanismo de atenção.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Estados ocultos de entrada. Shape [batch_size, seq_length, hidden_size]\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção. Shape [batch_size, 1, tgt_seq_length, src_seq_length]\n",
        "            position_ids (Optional[torch.LongTensor]): IDs das posições. Shape [batch_size, seq_length]\n",
        "            past_key_value (Optional[Tuple[torch.Tensor]]): Cache de estados passados para geração autoregressiva.\n",
        "            output_attentions (bool): Se True, retorna os pesos de atenção.\n",
        "            use_cache (bool): Se True, retorna o cache para uso futuro.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "                - Estados ocultos atualizados\n",
        "                - Pesos de atenção (opcional)\n",
        "                - Novo cache de estados (opcional)\n",
        "        \"\"\"\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        # Calcula query, key, value\n",
        "        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, head_dim]\n",
        "\n",
        "        # Aplica RoPE (Rotary Position Embedding)\n",
        "        cos, sin = self.rotary_emb(query_states)\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "        # Lida com o cache de estados passados para geração autoregressiva\n",
        "        if past_key_value is not None:\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "\n",
        "        past_key_value = (key_states, value_states) if use_cache else None\n",
        "\n",
        "        # Repete key e value para cada grupo de query em grouped-query attention\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, head_dim]\n",
        "\n",
        "        # Calcula os scores de atenção\n",
        "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, seq_length]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attn_weights = attn_weights + attention_mask\n",
        "\n",
        "        # Normaliza os pesos de atenção\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "\n",
        "        # Calcula o output da atenção\n",
        "        attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, head_dim]\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
        "\n",
        "        # [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Projeção final\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        if not output_attentions:\n",
        "            attn_weights = None\n",
        "\n",
        "        return attn_output, attn_weights, past_key_value\n",
        "\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Repete os estados de key e value para grouped-query attention.\n",
        "\n",
        "    Args:\n",
        "        hidden_states (torch.Tensor): Estados de entrada [batch, num_key_value_heads, seqlen, head_dim]\n",
        "        n_rep (int): Número de repetições\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Estados repetidos [batch, num_attention_heads, seqlen, head_dim]\n",
        "    \"\"\"\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
      ],
      "metadata": {
        "id": "lSN7BH0Hv7VF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sdpa Attention"
      ],
      "metadata": {
        "id": "2ZKJw_elZ3s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple\n",
        "import warnings\n",
        "\n",
        "class LlamaSdpaAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementação otimizada do mecanismo de atenção do LLaMA usando\n",
        "    scaled_dot_product_attention (SDPA) do PyTorch.\n",
        "\n",
        "    Esta classe implementa a atenção multi-cabeça com suporte a\n",
        "    Rotary Position Embedding (RoPE) e atenção agrupada.\n",
        "\n",
        "    Atributos:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        layer_idx (int): Índice da camada atual.\n",
        "        hidden_size (int): Dimensão do espaço oculto.\n",
        "        num_heads (int): Número de cabeças de atenção.\n",
        "        head_dim (int): Dimensão de cada cabeça de atenção.\n",
        "        num_key_value_heads (int): Número de cabeças para key e value (pode ser menor que num_heads).\n",
        "        max_position_embeddings (int): Número máximo de posições para embeddings.\n",
        "        rotary_emb (LlamaRotaryEmbedding): Instância para aplicar embeddings rotacionais.\n",
        "\n",
        "    Args:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        layer_idx (Optional[int]): Índice da camada. Necessário para algumas otimizações.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
        "            raise ValueError(\n",
        "                f\"hidden_size deve ser divisível por num_heads. \"\n",
        "                f\"Got {self.hidden_size} e {self.num_heads}.\"\n",
        "            )\n",
        "\n",
        "        # Projeções lineares para Q, K, V e O\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_position_embeddings=self.max_position_embeddings,\n",
        "            base=config.rope_theta,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: bool = False,\n",
        "        use_cache: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do mecanismo de atenção.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Estados ocultos de entrada.\n",
        "                Shape: [batch_size, seq_length, hidden_size]\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção.\n",
        "                Shape: [batch_size, 1, tgt_seq_length, src_seq_length]\n",
        "            position_ids (Optional[torch.LongTensor]): IDs das posições.\n",
        "                Shape: [batch_size, seq_length]\n",
        "            past_key_value (Optional[Tuple[torch.Tensor]]): Cache de estados passados.\n",
        "            output_attentions (bool): Se True, retorna os pesos de atenção.\n",
        "            use_cache (bool): Se True, retorna o cache para uso futuro.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "                - Estados ocultos atualizados\n",
        "                - Pesos de atenção (opcional)\n",
        "                - Novo cache de estados (opcional)\n",
        "        \"\"\"\n",
        "        # Obtém as dimensões do tensor de entrada\n",
        "        # hidden_states shape: [batch_size, seq_length, hidden_size]\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        if self.config.pretraining_tp > 1:\n",
        "            # Implementação para tensor parallelism\n",
        "            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n",
        "            query_slices = self.q_proj.weight.split((self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0)\n",
        "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
        "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
        "\n",
        "            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            query_states = torch.cat(query_states, dim=-1)\n",
        "\n",
        "            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            key_states = torch.cat(key_states, dim=-1)\n",
        "\n",
        "            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            value_states = torch.cat(value_states, dim=-1)\n",
        "\n",
        "        else:\n",
        "            # Projeções Q, K, V padrão\n",
        "            # Resultado: [batch_size, seq_length, num_heads * head_dim]\n",
        "            query_states = self.q_proj(hidden_states)\n",
        "            key_states = self.k_proj(hidden_states)\n",
        "            value_states = self.v_proj(hidden_states)\n",
        "\n",
        "        # Reshape e transpõe Q, K, V\n",
        "        # Resultado: [batch_size, num_heads, seq_length, head_dim]\n",
        "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calcula os embeddings rotacionais\n",
        "        # cos e sin: [1, seq_length, head_dim]\n",
        "        cos, sin = self.rotary_emb(value_states, seq_len=q_len)\n",
        "\n",
        "        # Aplica RoPE (Rotary Position Embedding) a Q e K\n",
        "        # query_states, key_states: [batch_size, num_heads, seq_length, head_dim]\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
        "\n",
        "        # Lida com o cache de estados passados para geração autoregressiva\n",
        "        if past_key_value is not None:\n",
        "            # Concatena estados passados com os atuais\n",
        "            # key_states, value_states: [batch_size, num_heads, seq_length + past_length, head_dim]\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "\n",
        "        # Prepara o cache para a próxima iteração se necessário\n",
        "        past_key_value = (key_states, value_states) if use_cache else None\n",
        "\n",
        "        # Repete K e V para atenção agrupada (grouped-query attention)\n",
        "        # key_states, value_states: [batch_size, num_heads, seq_length, head_dim]\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        # Aplica a atenção usando scaled_dot_product_attention\n",
        "        # attn_output: [batch_size, num_heads, seq_length, head_dim]\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            query_states, key_states, value_states,\n",
        "            attn_mask=attention_mask,\n",
        "            dropout_p=self.config.attention_dropout if self.training else 0.0,\n",
        "            is_causal=False\n",
        "        )\n",
        "\n",
        "        # Reorganiza o tensor de saída\n",
        "        # attn_output: [batch_size, seq_length, num_heads * head_dim]\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
        "\n",
        "        # Aplica a projeção de saída\n",
        "        # attn_output: [batch_size, seq_length, hidden_size]\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        if output_attentions:\n",
        "            warnings.warn(\"output_attentions=True não é suportado para SDPA no momento.\")\n",
        "            attn_weights = None\n",
        "        else:\n",
        "            attn_weights = None\n",
        "\n",
        "        return attn_output, attn_weights, past_key_value"
      ],
      "metadata": {
        "id": "LDQ8ZVGbXlNS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Flash Attention 2"
      ],
      "metadata": {
        "id": "toLzxx4MZ9-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash_attn --quiet"
      ],
      "metadata": {
        "id": "9swlCZ6WYbwA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, Tuple\n",
        "from flash_attn import flash_attn_func, flash_attn_varlen_func\n",
        "import warnings\n",
        "\n",
        "class LlamaFlashAttention2(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementação do mecanismo de atenção do LLaMA usando Flash Attention 2.\n",
        "    Esta versão é otimizada para eficiência em memória e velocidade.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
        "            raise ValueError(f\"hidden_size deve ser divisível por num_heads.\")\n",
        "\n",
        "        # Inicializa as projeções lineares para Q, K, V e O\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_position_embeddings=self.max_position_embeddings,\n",
        "            base=config.rope_theta,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: bool = False,\n",
        "        use_cache: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        # hidden_states shape: [batch_size, seq_length, hidden_size]\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        # Aplica as projeções lineares para Q, K, V\n",
        "        # Shapes: [batch_size, seq_length, num_heads * head_dim]\n",
        "        query_states = self.q_proj(hidden_states)\n",
        "        key_states = self.k_proj(hidden_states)\n",
        "        value_states = self.v_proj(hidden_states)\n",
        "\n",
        "        # Reshape e transpõe para [batch_size, num_heads, seq_length, head_dim]\n",
        "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calcula os embeddings rotacionais\n",
        "        cos, sin = self.rotary_emb(value_states, seq_len=q_len)\n",
        "        # Aplica RoPE (Rotary Position Embedding)\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
        "\n",
        "        # Lida com o cache de estados passados para geração autoregressiva\n",
        "        if past_key_value is not None:\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        past_key_value = (key_states, value_states) if use_cache else None\n",
        "\n",
        "        # Repete K e V para atenção agrupada (grouped-query attention)\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        # Prepara os tensores para Flash Attention\n",
        "        q, k, v = query_states, key_states, value_states\n",
        "\n",
        "        # Converte q, k, v para o formato esperado por Flash Attention\n",
        "        # [batch_size, seq_length, num_heads, head_dim]\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Aplica Flash Attention\n",
        "        if attention_mask is None:\n",
        "            # Usa a versão padrão do Flash Attention quando não há máscara\n",
        "            attn_output = flash_attn_func(q, k, v, dropout_p=self.config.attention_dropout if self.training else 0.0, causal=True)\n",
        "        else:\n",
        "            # Usa a versão com comprimento variável quando há máscara\n",
        "            attn_output, _ = flash_attn_varlen_func(\n",
        "                q, k, v,\n",
        "                cu_seqlens_q=attention_mask,\n",
        "                cu_seqlens_k=attention_mask,\n",
        "                max_seqlen_q=q_len,\n",
        "                max_seqlen_k=q_len,\n",
        "                dropout_p=self.config.attention_dropout if self.training else 0.0,\n",
        "                causal=True\n",
        "            )\n",
        "\n",
        "        # Reshape e aplica a projeção de saída\n",
        "        # [batch_size, seq_length, hidden_size]\n",
        "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        if output_attentions:\n",
        "            warnings.warn(\"output_attentions=True não é suportado para Flash Attention.\")\n",
        "            attn_weights = None\n",
        "        else:\n",
        "            attn_weights = None\n",
        "\n",
        "        return attn_output, attn_weights, past_key_value"
      ],
      "metadata": {
        "id": "v3AqKNbBYCuj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Decoder Layer"
      ],
      "metadata": {
        "id": "eDCXFvglaDFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class LlamaDecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementa uma camada do decodificador do modelo LLaMA.\n",
        "\n",
        "    Esta classe combina os mecanismos de atenção e feed-forward network (MLP),\n",
        "    formando um bloco completo do transformer decodificador. Inclui normalizações\n",
        "    de camada e conexões residuais.\n",
        "\n",
        "    Atributos:\n",
        "        hidden_size (int): Dimensão do espaço oculto.\n",
        "        self_attn (LlamaAttention): Mecanismo de auto-atenção.\n",
        "        mlp (LlamaMLP): Rede feed-forward.\n",
        "        input_layernorm (LlamaRMSNorm): Normalização de camada para entrada.\n",
        "        post_attention_layernorm (LlamaRMSNorm): Normalização após a atenção.\n",
        "\n",
        "    Args:\n",
        "        config (LlamaConfig): Configuração do modelo LLaMA.\n",
        "        layer_idx (int): Índice da camada atual.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(hidden_size=512, intermediate_size=2048, num_attention_heads=8)\n",
        "        >>> layer = LlamaDecoderLayer(config, layer_idx=0)\n",
        "        >>> hidden_states = torch.randn(1, 10, 512)  # [batch_size, seq_length, hidden_size]\n",
        "        >>> attention_mask = torch.ones(1, 1, 10, 10)  # [batch_size, 1, seq_length, seq_length]\n",
        "        >>> outputs = layer(hidden_states, attention_mask=attention_mask)\n",
        "        >>> print(outputs[0].shape)\n",
        "        torch.Size([1, 10, 512])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: LlamaConfig):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.self_attn = LlamaAttention(config=config)\n",
        "        self.mlp = LlamaMLP(config)\n",
        "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward de uma camada do decodificador.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Estados ocultos de entrada.\n",
        "                Shape [batch_size, seq_length, hidden_size]\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção.\n",
        "                Shape [batch_size, 1, tgt_seq_length, src_seq_length]\n",
        "            position_ids (Optional[torch.LongTensor]): IDs das posições.\n",
        "                Shape [batch_size, seq_length]\n",
        "            past_key_value (Optional[Tuple[torch.Tensor]]): Cache de estados passados para\n",
        "                geração autoregressiva.\n",
        "            output_attentions (Optional[bool]): Se True, retorna os pesos de atenção.\n",
        "            use_cache (Optional[bool]): Se True, retorna o cache para uso futuro.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "                - Estados ocultos atualizados\n",
        "                - Tupla contendo os novos cache de estados (se use_cache=True)\n",
        "        \"\"\"\n",
        "        # Shape de hidden_states: [batch_size, seq_length, hidden_size]\n",
        "        residual = hidden_states\n",
        "\n",
        "        # Normalização de camada na entrada\n",
        "        hidden_states = self.input_layernorm(hidden_states)\n",
        "        # Shape após normalização: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Self Attention\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_value=past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "            use_cache=use_cache,\n",
        "        )\n",
        "        # Shape após atenção: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Conexão residual após a atenção\n",
        "        hidden_states = residual + hidden_states\n",
        "        # Shape após conexão residual: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Normalização de camada após a atenção\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
        "        # Shape após normalização: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # MLP (Feed-Forward Network)\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        # Shape após MLP: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Conexão residual após o MLP\n",
        "        hidden_states = residual + hidden_states\n",
        "        # Shape final: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights,)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"LlamaDecoderLayer(hidden_size={self.hidden_size})\""
      ],
      "metadata": {
        "id": "PQ5ZrqsbxQie"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Pre-Trained Model"
      ],
      "metadata": {
        "id": "-IK_BO3TaHfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedModel\n",
        "from transformers.modeling_utils import PretrainedConfig\n",
        "from typing import Union, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LlamaPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Classe base abstrata para modelos pré-treinados LLaMA.\n",
        "\n",
        "    Esta classe herda de `PreTrainedModel` e implementa funcionalidades específicas\n",
        "    para modelos LLaMA, incluindo inicialização de pesos e configurações de otimização.\n",
        "\n",
        "    Atributos:\n",
        "        config_class (Type[PretrainedConfig]): Classe de configuração para modelos LLaMA.\n",
        "        base_model_prefix (str): Prefixo usado para nomear o modelo base.\n",
        "        supports_gradient_checkpointing (bool): Indica suporte a checkpointing de gradiente.\n",
        "        _no_split_modules (List[str]): Lista de módulos que não devem ser divididos durante\n",
        "                                       o processamento paralelo.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> from transformers import LlamaConfig\n",
        "        >>> class MyLlamaModel(LlamaPreTrainedModel):\n",
        "        ...     def __init__(self, config):\n",
        "        ...         super().__init__(config)\n",
        "        ...         # Implementação do modelo\n",
        "        ...\n",
        "        >>> config = LlamaConfig()\n",
        "        >>> model = MyLlamaModel(config)\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = LlamaConfig\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _no_split_modules = [\"LlamaDecoderLayer\"]\n",
        "    _skip_keys_device_placement = [\"past_key_values\"]\n",
        "    _supports_flash_attn_2 = True\n",
        "    _supports_sdpa = True\n",
        "    _supports_cache_class = True\n",
        "    _supports_quantized_cache = True\n",
        "    _supports_static_cache = True\n",
        "\n",
        "    def __init__(self, config: LlamaConfig, *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "\n",
        "    def _init_weights(self, module: nn.Module):\n",
        "        \"\"\"\n",
        "        Inicializa os pesos do módulo.\n",
        "\n",
        "        Esta função é chamada para cada submódulo durante a inicialização do modelo.\n",
        "        Implementa a estratégia de inicialização de pesos específica para modelos LLaMA.\n",
        "\n",
        "        Args:\n",
        "            module (nn.Module): O módulo cujos pesos serão inicializados.\n",
        "        \"\"\"\n",
        "        std = self.config.initializer_range\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module: nn.Module, value: bool = False):\n",
        "        \"\"\"\n",
        "        Configura o checkpointing de gradiente para o módulo.\n",
        "\n",
        "        O checkpointing de gradiente pode ser usado para economizar memória durante o treinamento,\n",
        "        recalculando os gradientes durante a passagem backward em vez de armazená-los.\n",
        "\n",
        "        Args:\n",
        "            module (nn.Module): O módulo para configurar o checkpointing.\n",
        "            value (bool): Se True, ativa o checkpointing de gradiente.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (LlamaDecoderLayer, LlamaModel)):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "    def gradient_checkpointing_enable(self):\n",
        "        \"\"\"\n",
        "        Ativa o checkpointing de gradiente para todo o modelo.\n",
        "        \"\"\"\n",
        "        self.apply(lambda module: self._set_gradient_checkpointing(module, value=True))\n",
        "\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        \"\"\"\n",
        "        Desativa o checkpointing de gradiente para todo o modelo.\n",
        "        \"\"\"\n",
        "        self.apply(lambda module: self._set_gradient_checkpointing(module, value=False))\n",
        "\n",
        "    def enable_input_require_grads(self):\n",
        "        \"\"\"\n",
        "        Configura o modelo para permitir gradientes nos inputs.\n",
        "\n",
        "        Isso é necessário para técnicas como adversarial training.\n",
        "        \"\"\"\n",
        "        def make_inputs_require_grads(module, input, output):\n",
        "            output.requires_grad_(True)\n",
        "\n",
        "        self._require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n",
        "\n",
        "    def disable_input_require_grads(self):\n",
        "        \"\"\"\n",
        "        Remove a configuração que permite gradientes nos inputs.\n",
        "        \"\"\"\n",
        "        self._require_grads_hook.remove()\n",
        "\n",
        "    def get_position_embeddings(self) -> Optional[Union[nn.Embedding, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Retorna as embeddings de posição do modelo, se existirem.\n",
        "\n",
        "        Returns:\n",
        "            Optional[Union[nn.Embedding, torch.Tensor]]: As embeddings de posição ou None.\n",
        "        \"\"\"\n",
        "        if hasattr(self, \"rotary_emb\"):\n",
        "            return self.rotary_emb\n",
        "        return None\n",
        "\n",
        "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
        "        \"\"\"\n",
        "        Redimensiona as embeddings de posição do modelo.\n",
        "\n",
        "        Args:\n",
        "            new_num_position_embeddings (int): O novo número de posições.\n",
        "\n",
        "        Raises:\n",
        "            NotImplementedError: Esta funcionalidade não está implementada para modelos LLaMA.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\n",
        "            f\"{self.__class__.__name__} não suporta o redimensionamento das embeddings de posição.\"\n",
        "        )\n",
        "\n",
        "    def get_output_embeddings(self) -> Optional[nn.Module]:\n",
        "        \"\"\"\n",
        "        Retorna as embeddings de saída do modelo, se existirem.\n",
        "\n",
        "        Returns:\n",
        "            Optional[nn.Module]: As embeddings de saída ou None.\n",
        "        \"\"\"\n",
        "        return None  # LLaMA não usa embeddings de saída por padrão\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings: Optional[nn.Module]):\n",
        "        \"\"\"\n",
        "        Define novas embeddings de saída para o modelo.\n",
        "\n",
        "        Args:\n",
        "            new_embeddings (Optional[nn.Module]): As novas embeddings de saída.\n",
        "\n",
        "        Raises:\n",
        "            NotImplementedError: Esta funcionalidade não está implementada para modelos LLaMA.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\n",
        "            f\"{self.__class__.__name__} não suporta a mudança das embeddings de saída.\"\n",
        "        )"
      ],
      "metadata": {
        "id": "EcLz7dF_xjtp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Model"
      ],
      "metadata": {
        "id": "oWIpC2lxaLN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, Tuple, Union, List\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
        "\n",
        "class LlamaModel(LlamaPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Modelo base LLaMA.\n",
        "\n",
        "    Esta classe implementa a estrutura principal do modelo LLaMA, incluindo\n",
        "    as camadas de embedding, as camadas do decodificador e a normalização final.\n",
        "\n",
        "    Atributos:\n",
        "        config (LlamaConfig): Configuração do modelo.\n",
        "        padding_idx (int): Índice do token de padding.\n",
        "        vocab_size (int): Tamanho do vocabulário.\n",
        "        embed_tokens (nn.Embedding): Camada de embedding para tokens.\n",
        "        layers (nn.ModuleList): Lista de camadas do decodificador.\n",
        "        norm (LlamaRMSNorm): Camada de normalização final.\n",
        "        gradient_checkpointing (bool): Se o checkpointing de gradiente está ativado.\n",
        "\n",
        "    Args:\n",
        "        config (LlamaConfig): Configuração do modelo LLaMA.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> from transformers import LlamaConfig\n",
        "        >>> config = LlamaConfig()\n",
        "        >>> model = LlamaModel(config)\n",
        "        >>> input_ids = torch.randint(0, config.vocab_size, (1, 10))\n",
        "        >>> outputs = model(input_ids)\n",
        "        >>> last_hidden_states = outputs.last_hidden_state\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: LlamaConfig):\n",
        "        super().__init__(config)\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
        "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _i1 in range(config.num_hidden_layers)])\n",
        "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do modelo LLaMA.\n",
        "\n",
        "        Args:\n",
        "            input_ids (Optional[torch.LongTensor]): Tensor de IDs de tokens de entrada.\n",
        "                Shape: (batch_size, sequence_length)\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção para os tokens de entrada.\n",
        "                Shape: (batch_size, sequence_length)\n",
        "            position_ids (Optional[torch.LongTensor]): IDs de posição para os tokens de entrada.\n",
        "                Shape: (batch_size, sequence_length)\n",
        "            past_key_values (Optional[List[torch.FloatTensor]]): Lista de tensores contendo estados passados\n",
        "                para uso em geração incremental.\n",
        "            inputs_embeds (Optional[torch.FloatTensor]): Embeddings pré-computados para substituir input_ids.\n",
        "                Shape: (batch_size, sequence_length, hidden_size)\n",
        "            use_cache (Optional[bool]): Se deve retornar um cache para geração incremental.\n",
        "            output_attentions (Optional[bool]): Se deve retornar todas as atenções.\n",
        "            output_hidden_states (Optional[bool]): Se deve retornar todos os estados ocultos.\n",
        "            return_dict (Optional[bool]): Se deve retornar um dicionário ao invés de uma tupla.\n",
        "\n",
        "        Returns:\n",
        "            Union[Tuple, BaseModelOutputWithPast]: Saída do modelo, incluindo últimos estados ocultos,\n",
        "                past_key_values (se use_cache=True), e opcionalmente todos os estados ocultos e atenções.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # Recuperar embeddings de entrada\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"Você não pode especificar tanto input_ids quanto inputs_embeds ao mesmo tempo\")\n",
        "        elif input_ids is not None:\n",
        "            # input_ids shape: (batch_size, sequence_length)\n",
        "            batch_size, seq_length = input_ids.shape\n",
        "            inputs_embeds = self.embed_tokens(input_ids)\n",
        "            # inputs_embeds shape: (batch_size, sequence_length, hidden_size)\n",
        "        elif inputs_embeds is not None:\n",
        "            batch_size, seq_length, _ = inputs_embeds.shape\n",
        "        else:\n",
        "            raise ValueError(\"Você deve especificar ou input_ids ou inputs_embeds\")\n",
        "\n",
        "        # Gerar position_ids se não fornecidos\n",
        "        if position_ids is None:\n",
        "            # position_ids shape: (batch_size, sequence_length)\n",
        "            position_ids = torch.arange(seq_length, dtype=torch.long, device=inputs_embeds.device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Gerar máscara de atenção se não fornecida\n",
        "        if attention_mask is None:\n",
        "            # attention_mask shape: (batch_size, sequence_length)\n",
        "            attention_mask = torch.ones((batch_size, seq_length), device=inputs_embeds.device)\n",
        "\n",
        "        # Converter máscara de atenção para o formato correto (expandido para todas as cabeças)\n",
        "        # extended_attention_mask shape: (batch_size, 1, 1, sequence_length)\n",
        "        extended_attention_mask = self.get_extended_attention_mask(attention_mask, (batch_size, seq_length))\n",
        "\n",
        "        # hidden_states shape: (batch_size, sequence_length, hidden_size)\n",
        "        hidden_states = inputs_embeds\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "\n",
        "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "                layer_outputs = self._gradient_checkpointing_func(\n",
        "                    decoder_layer.__call__,\n",
        "                    hidden_states,\n",
        "                    extended_attention_mask,\n",
        "                    position_ids,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                    use_cache,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=extended_attention_mask,\n",
        "                    position_ids=position_ids,\n",
        "                    past_key_value=past_key_value,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                )\n",
        "\n",
        "            # layer_outputs[0] shape: (batch_size, sequence_length, hidden_size)\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                # next_decoder_cache shape (para cada camada):\n",
        "                # (2, batch_size, num_heads, sequence_length, head_dim)\n",
        "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                # all_self_attentions shape (para cada camada):\n",
        "                # (batch_size, num_heads, sequence_length, sequence_length)\n",
        "                all_self_attentions += (layer_outputs[1],)\n",
        "\n",
        "        # Normalização final\n",
        "        # hidden_states shape: (batch_size, sequence_length, hidden_size)\n",
        "        hidden_states = self.norm(hidden_states)\n",
        "\n",
        "        # Adicionar últimos estados ocultos\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        next_cache = next_decoder_cache if use_cache else None\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attentions] if v is not None)\n",
        "\n",
        "        return BaseModelOutputWithPast(\n",
        "            last_hidden_state=hidden_states,  # (batch_size, sequence_length, hidden_size)\n",
        "            past_key_values=next_cache,       # Lista de tensores, cada um com shape:\n",
        "                                              # (2, batch_size, num_heads, sequence_length, head_dim)\n",
        "            hidden_states=all_hidden_states,  # Tupla de tensores, cada um com shape:\n",
        "                                              # (batch_size, sequence_length, hidden_size)\n",
        "            attentions=all_self_attentions,   # Tupla de tensores, cada um com shape:\n",
        "                                              # (batch_size, num_heads, sequence_length, sequence_length)\n",
        "        )"
      ],
      "metadata": {
        "id": "2IUNbEY6208o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Implmentations"
      ],
      "metadata": {
        "id": "600DUO0saOKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Causal Language Modeling"
      ],
      "metadata": {
        "id": "QerDXIzeaUHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from typing import Optional, Tuple, Union, List\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "\n",
        "class LlamaForCausalLM(LlamaPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Modelo LLaMA para Modelagem de Linguagem Causal.\n",
        "\n",
        "    Esta classe implementa o modelo LLaMA específico para tarefas de geração de texto,\n",
        "    adicionando uma camada de saída linear (lm_head) ao modelo base LLaMA.\n",
        "\n",
        "    Atributos:\n",
        "        model (LlamaModel): O modelo base LLaMA.\n",
        "        lm_head (nn.Linear): Camada linear para projetar estados ocultos no espaço do vocabulário.\n",
        "        vocab_size (int): Tamanho do vocabulário do modelo.\n",
        "\n",
        "    Args:\n",
        "        config (LlamaConfig): Configuração do modelo LLaMA.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> from transformers import LlamaConfig, LlamaTokenizer\n",
        "        >>> config = LlamaConfig()\n",
        "        >>> model = LlamaForCausalLM(config)\n",
        "        >>> tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "        >>> inputs = tokenizer(\"Olá, como vai?\", return_tensors=\"pt\")\n",
        "        >>> outputs = model(**inputs)\n",
        "        >>> logits = outputs.logits\n",
        "    \"\"\"\n",
        "\n",
        "    _tied_weights_keys = [\"lm_head.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = LlamaModel(config)\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Inicializa pesos e aplica processamento final\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.embed_tokens = value\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def set_decoder(self, decoder):\n",
        "        self.model = decoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do modelo.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.LongTensor): IDs dos tokens de entrada.\n",
        "            attention_mask (torch.Tensor, opcional): Máscara de atenção.\n",
        "            position_ids (torch.LongTensor, opcional): IDs das posições.\n",
        "            past_key_values (List[torch.FloatTensor], opcional): Valores passados para uso em geração incremental.\n",
        "            inputs_embeds (torch.FloatTensor, opcional): Embeddings de entrada pré-computados.\n",
        "            labels (torch.LongTensor, opcional): Rótulos para cálculo de perda.\n",
        "            use_cache (bool, opcional): Se deve usar cache para geração incremental.\n",
        "            output_attentions (bool, opcional): Se deve retornar todas as atenções.\n",
        "            output_hidden_states (bool, opcional): Se deve retornar todos os estados ocultos.\n",
        "            return_dict (bool, opcional): Se deve retornar um dicionário ou tupla.\n",
        "\n",
        "        Returns:\n",
        "            Union[Tuple, CausalLMOutputWithPast]: Saída do modelo, incluindo logits, loss (se labels fornecidos),\n",
        "                                                  past_key_values (se use_cache=True), e opcionalmente\n",
        "                                                  hidden_states e attentions.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "        logits = self.lm_head(hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
        "            shift_labels = shift_labels.view(-1)\n",
        "            # Enable model parallelism\n",
        "            shift_labels = shift_labels.to(shift_logits.device)\n",
        "            loss = loss_fct(shift_logits, shift_labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
        "    ):\n",
        "        if past_key_values:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
        "        if inputs_embeds is not None and past_key_values is None:\n",
        "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
        "        else:\n",
        "            model_inputs = {\"input_ids\": input_ids}\n",
        "\n",
        "        model_inputs.update(\n",
        "            {\n",
        "                \"past_key_values\": past_key_values,\n",
        "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"position_ids\": kwargs.get(\"position_ids\"),\n",
        "            }\n",
        "        )\n",
        "        return model_inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past_key_values, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past_key_values:\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
        "            )\n",
        "        return reordered_past"
      ],
      "metadata": {
        "id": "ZQX07y71ye30"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Fine-Tunning Trainer"
      ],
      "metadata": {
        "id": "k3E3eexBAvmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import LlamaTokenizer\n",
        "from typing import Dict, List, Optional, Union\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "\n",
        "class LlamaSFTTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    LlamaSFTTrainer é uma classe personalizada para treinar modelos LlamaForCausalLM.\n",
        "    Herda da classe Trainer do Hugging Face e implementa funcionalidades específicas\n",
        "    para o treinamento de modelos LLaMA.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: LlamaForCausalLM,\n",
        "        args: TrainingArguments,\n",
        "        train_dataset: Optional[Dataset] = None,\n",
        "        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n",
        "        tokenizer: Optional[LlamaTokenizer] = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(model, args, train_dataset, eval_dataset, tokenizer, **kwargs)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        \"\"\"\n",
        "        Calcula a perda para o modelo LLaMA.\n",
        "        \"\"\"\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # Passa os inputs pelo modelo\n",
        "        # inputs: Dict[str, torch.Tensor] onde cada tensor tem shape [batch_size, seq_length]\n",
        "        # outputs: Objeto contendo loss e logits, ambos com shape [batch_size, seq_length, vocab_size]\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        if labels is not None and self.label_smoother:\n",
        "            loss = self.label_smoother(outputs, labels)\n",
        "        else:\n",
        "            loss = outputs.loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def prediction_step(\n",
        "        self,\n",
        "        model: LlamaForCausalLM,\n",
        "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
        "        prediction_loss_only: bool,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Realiza um passo de predição/avaliação no modelo.\n",
        "        \"\"\"\n",
        "        # Se não estiver gerando predições ou se estiver calculando apenas a perda, usa o método da classe pai\n",
        "        if not self.args.predict_with_generate or prediction_loss_only:\n",
        "            return super().prediction_step(\n",
        "                model,\n",
        "                inputs,\n",
        "                prediction_loss_only=prediction_loss_only,\n",
        "                ignore_keys=ignore_keys\n",
        "            )\n",
        "\n",
        "        # Prepara os inputs para o modelo\n",
        "        has_labels = \"labels\" in inputs\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        # Configura os parâmetros para geração de texto\n",
        "        gen_kwargs = self._gen_kwargs.copy()\n",
        "        if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:\n",
        "            gen_kwargs[\"max_length\"] = self.model.config.max_length\n",
        "\n",
        "        gen_kwargs[\"num_beams\"] = gen_kwargs.get(\"num_beams\", self.model.config.num_beams)\n",
        "\n",
        "        # Gera tokens usando o modelo\n",
        "        # input_ids: [batch_size, seq_length]\n",
        "        # attention_mask: [batch_size, seq_length]\n",
        "        # generated_tokens: [batch_size, max_length]\n",
        "        generated_tokens = self.model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            **gen_kwargs,\n",
        "        )\n",
        "\n",
        "        # Faz o padding dos tokens gerados se necessário\n",
        "        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
        "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
        "            # generated_tokens após padding: [batch_size, max_length]\n",
        "\n",
        "        # Calcula a perda se houver labels\n",
        "        with torch.no_grad():\n",
        "            if has_labels:\n",
        "                with self.autocast_smart_context_manager():\n",
        "                    # outputs: Objeto contendo loss e logits, ambos com shape [batch_size, seq_length, vocab_size]\n",
        "                    outputs = model(**inputs)\n",
        "                if self.label_smoother is not None:\n",
        "                    # loss: escalar tensor []\n",
        "                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n",
        "                else:\n",
        "                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
        "            else:\n",
        "                loss = None\n",
        "\n",
        "        if self.args.prediction_loss_only:\n",
        "            return (loss, None, None)\n",
        "\n",
        "\n",
        "        # Prepara as labels para retorno\n",
        "        labels = inputs[\"labels\"]\n",
        "        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n",
        "            labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n",
        "            # labels após padding: [batch_size, max_length]\n",
        "\n",
        "        # Retorna a perda, tokens gerados e labels\n",
        "        return (loss, generated_tokens, labels)\n",
        "\n",
        "    def _pad_tensors_to_max_len(self, tensor, max_length):\n",
        "        \"\"\"\n",
        "        Faz o padding de um tensor para um comprimento máximo especificado.\n",
        "        \"\"\"\n",
        "        # Verifica se o tokenizer está disponível\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"Tokenizer é necessário para fazer o padding dos tensores até o comprimento máximo.\")\n",
        "\n",
        "        # Se o tensor já tem o comprimento máximo ou maior, retorna sem modificação\n",
        "        if tensor.shape[-1] >= max_length:\n",
        "            return tensor\n",
        "\n",
        "        # Faz o padding do tensor\n",
        "        # tensor: [batch_size, seq_length]\n",
        "        padded_tensor = self.tokenizer.pad(\n",
        "            {\"input_ids\": tensor},\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )[\"input_ids\"]\n",
        "        # padded_tensor: [batch_size, max_length]\n",
        "\n",
        "        return padded_tensor\n"
      ],
      "metadata": {
        "id": "Xge7QBIQAvAl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "74d12LiJal2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from huggingface_hub import notebook_login\n",
        "#notebook_login()"
      ],
      "metadata": {
        "id": "6iyvJX3GfOfk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar modelo e tokenizador\n",
        "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Preparar entrada\n",
        "input_text = \"Olá, como você está?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Gerar texto\n",
        "outputs = model.forward(**inputs)\n",
        "\n",
        "print(outputs)\n",
        "# Decodificar saída\n",
        "#generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "#print(generated_text)"
      ],
      "metadata": {
        "id": "3RteoxBV1Aqc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "79c2a093329b467482392a91f39e7b96",
            "3f4dd9535a5b43b5bdda12094b17174e",
            "9b30d495f7b9487eaa117b4ee0503810",
            "c0394f80943b416389a94f02b81bf3ae",
            "63cce58a89df465a9150aff966b8264b",
            "7f201ae4521948d8a36069c7cc274ba4",
            "c7e82569b4ad421eb60da92949823174",
            "9118771e7f594188b7c119d4084e0aec",
            "2d039988296c427ebebe0d9c6bca0400",
            "3be918aa0cda4c72abb840f2f15f9838",
            "f007e9640ebf4eab9e506dbd84b705b9"
          ]
        },
        "outputId": "9d3824c4-26f7-447c-a747-f486167b9cf4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:546: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly by `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation, and ensure your `input_ids` input does not have negative values.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79c2a093329b467482392a91f39e7b96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CausalLMOutputWithPast(loss=None, logits=tensor([[[ -5.5803,  -3.3816,   3.7435,  ...,  -2.7620,  -3.6058,  -2.2001],\n",
            "         [ -8.2072, -10.3899,   2.5978,  ...,  -4.4890,  -5.0892,  -4.8157],\n",
            "         [-11.1266, -12.0699,   2.6896,  ...,  -5.1732, -10.1730,  -7.2766],\n",
            "         ...,\n",
            "         [-10.5673, -12.1708,   2.4443,  ...,  -4.5366,  -9.8937,  -6.7618],\n",
            "         [-10.9050, -12.1531,   1.3310,  ...,  -4.1111,  -9.0825,  -5.7634],\n",
            "         [-11.5284, -14.5160,   3.7716,  ...,  -5.0953, -10.0387,  -6.0594]]],\n",
            "       grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-0.4510, -0.0164,  0.0498,  ...,  0.0514, -0.0235, -0.1267],\n",
            "          [-0.0755, -0.3226,  0.1255,  ...,  0.2380,  0.0416,  0.3256],\n",
            "          [-0.1903,  0.1615, -0.0026,  ...,  0.4607, -0.2466,  0.4687],\n",
            "          ...,\n",
            "          [ 0.1571, -0.0498,  0.1105,  ...,  0.3303, -0.1048,  0.3284],\n",
            "          [-0.2380,  0.1337,  0.0568,  ...,  0.3247,  0.0369,  0.4885],\n",
            "          [ 0.6098, -0.6256,  0.4934,  ...,  0.1485, -0.0882,  0.4137]],\n",
            "\n",
            "         [[ 1.3476,  1.0810, -0.4292,  ...,  0.4777, -0.3054,  0.4798],\n",
            "          [ 0.4475,  0.5603, -0.2744,  ..., -0.7398,  0.5128, -0.6795],\n",
            "          [-0.3072,  0.3117,  0.4818,  ...,  0.6538, -0.0325,  0.5554],\n",
            "          ...,\n",
            "          [ 0.7524, -0.1842, -0.2342,  ...,  0.4016,  0.1005,  0.3052],\n",
            "          [ 0.7698,  0.2076, -0.5215,  ..., -0.0551,  0.3655, -0.1247],\n",
            "          [-1.0345,  0.4083,  0.1703,  ...,  0.4160, -0.1500,  0.4456]],\n",
            "\n",
            "         [[ 0.0030, -0.2562, -0.4154,  ..., -0.1226,  0.4979,  0.7620],\n",
            "          [-0.0181,  0.3199,  0.1450,  ...,  0.4677,  0.3586,  0.2905],\n",
            "          [-0.3006,  0.2555, -0.3964,  ..., -0.8402, -0.9562, -0.9344],\n",
            "          ...,\n",
            "          [ 0.1765, -0.2941,  0.3717,  ..., -0.8212, -0.9296, -0.8916],\n",
            "          [-0.2938, -0.5951, -0.0583,  ..., -0.5895, -0.7228, -0.7009],\n",
            "          [-0.0369,  0.0128,  0.0653,  ...,  1.1278,  1.2307,  1.1823]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0239, -0.0053,  0.0390,  ...,  0.4867,  0.9620, -0.4022],\n",
            "          [-0.0939,  0.0837, -0.0807,  ..., -0.2934,  0.0218, -0.4538],\n",
            "          [ 0.0722,  0.3020, -0.0217,  ..., -0.3333,  0.2746, -0.4837],\n",
            "          ...,\n",
            "          [-0.2979, -0.2056,  0.0634,  ..., -0.1554,  0.6519, -0.2524],\n",
            "          [-0.0828, -0.0846,  0.0413,  ..., -0.2337,  1.1036,  0.0118],\n",
            "          [-0.1969,  0.3354, -0.7097,  ..., -0.1344, -0.7522, -0.8480]],\n",
            "\n",
            "         [[ 0.1723, -0.5391, -0.3137,  ..., -0.2754,  0.1376,  0.1413],\n",
            "          [-0.1115,  0.3803, -0.7804,  ...,  0.2479, -0.1811, -0.1770],\n",
            "          [-0.3359,  0.5605,  0.2175,  ...,  0.4208, -0.2568, -0.2535],\n",
            "          ...,\n",
            "          [ 0.3830, -0.6329, -0.1090,  ...,  0.3653, -0.1805, -0.1755],\n",
            "          [ 0.1924, -0.5710, -1.2232,  ...,  0.4886, -0.3067, -0.2987],\n",
            "          [ 0.4380, -0.2578, -0.2006,  ..., -0.3374,  0.0309,  0.0306]],\n",
            "\n",
            "         [[-0.4470,  1.2264,  0.0057,  ...,  0.7097,  0.2183,  0.2774],\n",
            "          [-0.2668, -0.8094, -0.8961,  ...,  0.7169, -0.3354,  0.3397],\n",
            "          [ 0.7173, -0.0345,  0.2092,  ...,  0.8200, -0.6625,  0.0402],\n",
            "          ...,\n",
            "          [-0.8975, -0.0671,  0.1347,  ...,  1.0689, -0.6564,  0.2050],\n",
            "          [-0.7257, -0.8066, -0.6352,  ...,  1.0857, -0.9362,  0.2621],\n",
            "          [ 0.0974,  0.3498, -0.0173,  ..., -0.9874,  0.3904, -0.4810]]]],\n",
            "       grad_fn=<AddBackward0>), tensor([[[[-0.0060, -0.0063,  0.0056,  ...,  0.0014,  0.0202, -0.0054],\n",
            "          [-0.0091, -0.0076, -0.0036,  ..., -0.0025, -0.0022,  0.0069],\n",
            "          [-0.0010, -0.0008, -0.0032,  ...,  0.0005,  0.0132, -0.0002],\n",
            "          ...,\n",
            "          [ 0.0076, -0.0004,  0.0048,  ..., -0.0081,  0.0059, -0.0022],\n",
            "          [-0.0026,  0.0022, -0.0074,  ..., -0.0084, -0.0013, -0.0029],\n",
            "          [-0.0138, -0.0123, -0.0011,  ..., -0.0105,  0.0032,  0.0066]],\n",
            "\n",
            "         [[ 0.0020,  0.0040, -0.0097,  ..., -0.0047,  0.0070, -0.0190],\n",
            "          [ 0.0064, -0.0043, -0.0031,  ..., -0.0022, -0.0145, -0.0073],\n",
            "          [ 0.0007,  0.0021, -0.0015,  ..., -0.0078,  0.0057, -0.0003],\n",
            "          ...,\n",
            "          [ 0.0074, -0.0086, -0.0010,  ..., -0.0115, -0.0066, -0.0026],\n",
            "          [ 0.0123, -0.0072, -0.0003,  ...,  0.0023, -0.0118, -0.0087],\n",
            "          [ 0.0095,  0.0062,  0.0051,  ..., -0.0006,  0.0072,  0.0114]],\n",
            "\n",
            "         [[-0.0038, -0.0064,  0.0142,  ...,  0.0050, -0.0147,  0.0070],\n",
            "          [-0.0047,  0.0025, -0.0127,  ..., -0.0092, -0.0026, -0.0180],\n",
            "          [-0.0050,  0.0044,  0.0052,  ..., -0.0003,  0.0116,  0.0057],\n",
            "          ...,\n",
            "          [ 0.0097, -0.0045, -0.0031,  ...,  0.0083,  0.0088, -0.0032],\n",
            "          [ 0.0013,  0.0047,  0.0017,  ..., -0.0050, -0.0070, -0.0095],\n",
            "          [-0.0001, -0.0012, -0.0052,  ...,  0.0015,  0.0039, -0.0019]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0005, -0.0022, -0.0157,  ...,  0.0208,  0.0099, -0.0051],\n",
            "          [ 0.0066,  0.0367,  0.0117,  ...,  0.0056, -0.0027,  0.0021],\n",
            "          [-0.0096, -0.0678,  0.0170,  ...,  0.0584, -0.0200,  0.0123],\n",
            "          ...,\n",
            "          [ 0.0106,  0.0099,  0.0181,  ...,  0.0436, -0.0199,  0.0186],\n",
            "          [-0.0322, -0.0478, -0.0954,  ...,  0.0788,  0.0223, -0.0435],\n",
            "          [-0.0561, -0.0032, -0.0443,  ..., -0.0034, -0.0343,  0.0878]],\n",
            "\n",
            "         [[ 0.0005,  0.0156, -0.0103,  ..., -0.0086, -0.0452, -0.0250],\n",
            "          [-0.0014,  0.0114,  0.0083,  ...,  0.0072, -0.0087, -0.0002],\n",
            "          [-0.0062, -0.0071,  0.0040,  ...,  0.0017, -0.0046,  0.0035],\n",
            "          ...,\n",
            "          [ 0.0046, -0.0014,  0.0052,  ..., -0.0079, -0.0092,  0.0109],\n",
            "          [-0.0095,  0.0114, -0.0228,  ..., -0.0001, -0.0077, -0.0011],\n",
            "          [ 0.0096, -0.0080,  0.0043,  ...,  0.0119,  0.0032, -0.0024]],\n",
            "\n",
            "         [[-0.0039, -0.0039,  0.0005,  ...,  0.0015, -0.0007, -0.0015],\n",
            "          [ 0.0047,  0.0028,  0.0027,  ..., -0.0054,  0.0059,  0.0102],\n",
            "          [ 0.0060, -0.0020, -0.0069,  ..., -0.0002,  0.0055, -0.0079],\n",
            "          ...,\n",
            "          [ 0.0013,  0.0008, -0.0044,  ..., -0.0083,  0.0038, -0.0085],\n",
            "          [ 0.0142, -0.0048, -0.0186,  ..., -0.0034, -0.0067,  0.0032],\n",
            "          [-0.0062, -0.0034,  0.0005,  ..., -0.0060, -0.0082,  0.0063]]]],\n",
            "       grad_fn=<TransposeBackward0>)), (tensor([[[[-2.0158e-01,  1.0895e-01, -3.8341e-01,  ...,  1.3270e+00,\n",
            "            1.1328e+00, -7.9110e-01],\n",
            "          [-1.5435e+00,  1.8765e-01,  2.3845e-01,  ..., -6.0730e-01,\n",
            "           -9.2459e-01,  2.9277e-01],\n",
            "          [-6.3133e-01,  1.9459e-01, -9.7806e-01,  ..., -6.7605e-01,\n",
            "           -9.0132e-01,  5.5299e-01],\n",
            "          ...,\n",
            "          [-3.6343e-01,  3.6323e-02,  7.8316e-01,  ..., -8.7001e-01,\n",
            "           -8.2808e-01,  3.1409e-01],\n",
            "          [-1.9639e+00,  4.8909e-01,  1.6332e+00,  ..., -1.7809e+00,\n",
            "           -8.9394e-01,  5.4171e-01],\n",
            "          [-1.5233e+00,  4.2455e-02,  1.5956e+00,  ..., -5.5363e-01,\n",
            "           -6.9406e-01,  9.0641e-02]],\n",
            "\n",
            "         [[-1.2844e-01,  6.0386e-02, -8.7866e-02,  ...,  1.5148e+00,\n",
            "            1.2790e+00,  1.0797e+00],\n",
            "          [ 6.7599e-01, -1.0371e-01, -1.1714e+00,  ..., -8.7950e-01,\n",
            "           -6.0619e-01, -3.4362e-01],\n",
            "          [ 9.1172e-01,  1.9728e-01, -8.4198e-01,  ..., -7.0039e-01,\n",
            "           -8.5669e-01, -5.1806e-01],\n",
            "          ...,\n",
            "          [ 4.0320e-02, -3.7054e-01,  8.0511e-01,  ..., -3.2746e-01,\n",
            "           -8.0797e-01, -5.1140e-01],\n",
            "          [ 8.0759e-01, -6.0962e-01,  4.8086e-01,  ..., -6.1526e-01,\n",
            "           -3.1615e-01, -4.9671e-01],\n",
            "          [-5.7141e-02, -1.1972e-01, -1.0891e-01,  ..., -6.4791e-01,\n",
            "           -1.6660e-01, -5.5724e-02]],\n",
            "\n",
            "         [[-8.9824e-02, -7.1767e-02,  4.3662e-01,  ...,  1.2809e+00,\n",
            "            4.7118e-01,  1.2367e-01],\n",
            "          [ 1.8335e-01, -2.2400e-01, -1.8106e-01,  ...,  1.0619e+00,\n",
            "            4.9003e-01,  2.9450e-01],\n",
            "          [-1.4954e-01, -1.4715e-01,  8.5696e-02,  ...,  1.1023e+00,\n",
            "            6.0383e-01,  5.7587e-01],\n",
            "          ...,\n",
            "          [ 4.5673e-01,  1.6576e-01,  1.8672e-03,  ...,  1.0224e+00,\n",
            "            5.2899e-01,  6.0375e-01],\n",
            "          [ 2.4463e-01,  6.7789e-02,  7.1939e-02,  ...,  1.0143e+00,\n",
            "            5.4071e-01,  5.9614e-01],\n",
            "          [-9.3780e-02, -1.5318e-01,  1.2735e-04,  ...,  1.2359e+00,\n",
            "            6.0529e-01,  5.9400e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.8192e-01,  1.7988e-01, -1.9795e-01,  ...,  1.6100e+00,\n",
            "           -1.0269e+00,  3.2521e-01],\n",
            "          [-4.3051e-02, -3.8819e-02,  1.4180e-01,  ...,  2.1407e-01,\n",
            "           -3.5626e-01,  3.2943e-01],\n",
            "          [-3.6920e-01,  1.1652e-01,  6.4363e-01,  ...,  6.3462e-02,\n",
            "           -1.9007e-01,  4.1620e-01],\n",
            "          ...,\n",
            "          [ 1.6066e-01, -2.1611e-02, -3.6288e-01,  ..., -8.5086e-02,\n",
            "           -5.1755e-04,  2.6110e-01],\n",
            "          [-5.8398e-02,  2.4755e-02, -1.2884e-01,  ..., -2.5606e-01,\n",
            "           -7.0530e-02,  1.9283e-01],\n",
            "          [ 5.7045e-01, -4.3463e-02,  2.2753e-01,  ...,  7.9801e-03,\n",
            "           -1.9938e-01,  8.2853e-02]],\n",
            "\n",
            "         [[ 1.1532e-02, -7.5322e-02, -9.9586e-02,  ..., -8.3167e-01,\n",
            "            1.0370e+00,  5.4114e-01],\n",
            "          [-7.4845e-02, -5.5136e-01, -2.3224e-02,  ..., -3.2326e-01,\n",
            "           -6.2057e-02, -2.6865e-02],\n",
            "          [-6.3103e-01, -2.7225e-01, -4.5625e-01,  ...,  6.4209e-02,\n",
            "           -3.2407e-01, -1.5343e-01],\n",
            "          ...,\n",
            "          [ 3.2676e-01,  1.1550e-01,  4.7540e-01,  ...,  9.7111e-02,\n",
            "           -4.6475e-01, -6.4529e-01],\n",
            "          [ 1.2853e-02, -5.6156e-01,  3.8441e-01,  ...,  3.2672e-01,\n",
            "           -5.7875e-01, -2.4987e-01],\n",
            "          [-3.4338e-01, -2.5702e-01,  2.9373e-02,  ..., -2.3309e-02,\n",
            "            9.3633e-02, -7.9344e-01]],\n",
            "\n",
            "         [[ 2.8063e-01, -7.5503e-02,  2.1627e-01,  ..., -1.0015e-01,\n",
            "            9.9938e-02, -1.0642e-01],\n",
            "          [-2.8328e-01,  6.5477e-01,  6.4007e-01,  ...,  1.1585e+00,\n",
            "           -1.2297e+00,  8.0644e-01],\n",
            "          [ 7.0125e-01,  4.4528e-01,  3.5106e-01,  ...,  1.2957e+00,\n",
            "           -1.3412e+00,  9.2043e-01],\n",
            "          ...,\n",
            "          [-9.2519e-01, -4.1737e-01, -4.2115e-01,  ...,  1.2484e+00,\n",
            "           -1.2690e+00,  9.0667e-01],\n",
            "          [-6.4624e-01, -7.0122e-02,  6.7068e-02,  ...,  1.3292e+00,\n",
            "           -1.3674e+00,  9.4875e-01],\n",
            "          [-1.5428e-01,  1.3434e-01,  5.2869e-02,  ...,  1.0074e+00,\n",
            "           -1.0193e+00,  6.8853e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 5.3098e-02, -2.0371e-02, -2.7326e-04,  ...,  1.6543e-02,\n",
            "           -3.4290e-02,  5.7353e-02],\n",
            "          [ 3.2532e-02,  1.0184e-01, -2.5173e-02,  ...,  3.8801e-02,\n",
            "            6.1604e-02,  5.3297e-03],\n",
            "          [ 5.4437e-02,  5.1121e-02, -1.6019e-02,  ..., -5.1379e-02,\n",
            "           -4.8333e-03,  8.7226e-03],\n",
            "          ...,\n",
            "          [ 8.4884e-02,  1.5147e-02,  2.1524e-02,  ..., -9.2305e-02,\n",
            "            4.7549e-02,  2.0698e-04],\n",
            "          [-6.0378e-02,  1.5214e-01, -3.0595e-02,  ..., -2.9491e-02,\n",
            "            1.0338e-01,  5.5006e-02],\n",
            "          [ 9.1865e-02, -7.9928e-02,  2.8252e-02,  ...,  4.1906e-02,\n",
            "            5.7170e-02,  2.2688e-02]],\n",
            "\n",
            "         [[ 1.5768e-03,  2.1587e-02, -1.0120e-02,  ...,  9.3443e-03,\n",
            "           -4.7119e-03,  1.4151e-02],\n",
            "          [ 9.8134e-03,  1.4266e-02,  1.5822e-02,  ...,  7.7227e-03,\n",
            "           -5.0393e-04,  1.7835e-02],\n",
            "          [ 1.3559e-02, -2.9552e-02, -1.1504e-02,  ..., -8.7674e-03,\n",
            "            1.7718e-02,  1.1394e-02],\n",
            "          ...,\n",
            "          [ 1.1879e-02, -3.9831e-02, -9.9141e-03,  ..., -8.0206e-06,\n",
            "            9.3696e-03,  7.0547e-03],\n",
            "          [ 1.1195e-02, -3.5634e-02, -7.5938e-03,  ..., -1.3268e-02,\n",
            "            1.3706e-02,  8.2597e-03],\n",
            "          [ 5.8053e-03, -2.2684e-02, -1.6144e-02,  ...,  2.3477e-02,\n",
            "           -8.6812e-04,  2.0595e-02]],\n",
            "\n",
            "         [[ 5.4520e-02,  8.1585e-03, -1.0304e-02,  ..., -8.2801e-02,\n",
            "           -9.6814e-02, -1.6762e-05],\n",
            "          [ 3.3259e-02,  5.6297e-03,  8.9746e-02,  ...,  2.1998e-01,\n",
            "           -2.5216e-04, -7.0516e-02],\n",
            "          [-4.8886e-02,  2.6374e-02,  3.3669e-02,  ...,  2.4238e-02,\n",
            "            1.0409e-01,  4.8281e-02],\n",
            "          ...,\n",
            "          [-3.4781e-02,  9.0586e-03, -3.4266e-02,  ...,  8.1777e-02,\n",
            "            8.8937e-02, -2.1667e-02],\n",
            "          [ 1.4351e-02,  4.8561e-02,  5.5506e-02,  ...,  7.8116e-02,\n",
            "            5.1083e-02, -4.6499e-02],\n",
            "          [ 3.7328e-02, -9.9883e-04, -1.7810e-01,  ..., -6.2529e-02,\n",
            "           -7.7260e-03,  6.2104e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.6058e-02,  7.8969e-03,  2.9070e-03,  ..., -3.7148e-02,\n",
            "           -1.5219e-03, -1.0113e-02],\n",
            "          [-1.9596e-02,  1.2498e-02,  1.2707e-02,  ..., -1.9408e-02,\n",
            "           -3.8706e-02,  1.7929e-03],\n",
            "          [-1.6828e-02,  1.5273e-02, -2.6167e-02,  ...,  1.4770e-02,\n",
            "            2.0749e-02,  3.2146e-02],\n",
            "          ...,\n",
            "          [ 2.0305e-02,  1.5504e-02, -3.5780e-03,  ...,  2.3198e-02,\n",
            "            5.3697e-02,  4.8641e-03],\n",
            "          [-1.6934e-02, -6.9767e-03,  2.9286e-02,  ..., -2.8503e-02,\n",
            "            3.8488e-03,  9.8222e-04],\n",
            "          [-2.8876e-02,  8.8744e-03,  9.3297e-03,  ..., -6.5743e-03,\n",
            "           -6.4269e-03, -1.1551e-02]],\n",
            "\n",
            "         [[-7.7913e-02,  4.3848e-02, -1.0697e-01,  ...,  1.7412e-02,\n",
            "           -3.9735e-03,  5.6314e-03],\n",
            "          [ 3.8963e-02,  1.2283e-02,  2.3790e-01,  ...,  1.5574e-02,\n",
            "            1.7853e-02,  7.6989e-03],\n",
            "          [-1.1377e-01,  7.1307e-02,  1.3748e-02,  ..., -2.6104e-02,\n",
            "           -1.5248e-02,  2.5139e-02],\n",
            "          ...,\n",
            "          [-1.1029e-01,  6.5847e-02, -4.0054e-02,  ..., -7.3666e-03,\n",
            "           -8.1024e-02,  8.0421e-03],\n",
            "          [ 4.7793e-02,  3.2563e-02,  1.0212e-01,  ..., -1.8626e-02,\n",
            "           -1.5706e-02,  1.6406e-02],\n",
            "          [-5.7404e-02, -7.6397e-02, -3.2293e-01,  ..., -7.0508e-03,\n",
            "           -2.7919e-02, -2.3538e-02]],\n",
            "\n",
            "         [[-7.3608e-03,  2.1151e-02,  2.8705e-03,  ..., -9.4323e-03,\n",
            "            8.9142e-03,  3.2130e-04],\n",
            "          [-4.0728e-03,  9.9079e-03,  1.1002e-02,  ..., -2.0227e-03,\n",
            "           -1.4947e-02,  3.0645e-02],\n",
            "          [ 2.7667e-02, -9.9903e-04,  2.8479e-03,  ..., -2.0564e-02,\n",
            "            4.0507e-03,  4.2362e-03],\n",
            "          ...,\n",
            "          [-4.6240e-03, -2.1602e-02, -1.6918e-02,  ...,  1.3742e-03,\n",
            "            1.2152e-02, -3.1266e-02],\n",
            "          [ 2.8229e-02,  1.1104e-02,  1.6262e-02,  ..., -1.2799e-02,\n",
            "            9.0263e-03, -1.1584e-02],\n",
            "          [ 2.6052e-02,  1.4022e-02, -3.4817e-03,  ..., -6.5534e-03,\n",
            "            1.7814e-02,  1.6514e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.3028e-02, -3.3792e-03, -2.0662e-03,  ..., -2.3012e-01,\n",
            "           -1.4602e-01,  2.0887e-01],\n",
            "          [-5.2037e-01, -1.8177e-01,  1.1829e-01,  ...,  1.1843e-01,\n",
            "           -8.3525e-01,  1.0006e+00],\n",
            "          [-1.7227e+00,  4.5797e-01,  3.4719e-01,  ...,  1.4908e+00,\n",
            "            8.0467e-02,  5.0286e-02],\n",
            "          ...,\n",
            "          [ 1.3283e+00, -1.0591e-01, -2.8032e-01,  ...,  1.0759e+00,\n",
            "            7.8134e-01, -1.0337e-01],\n",
            "          [ 3.3479e-01, -7.7397e-01, -4.3955e-01,  ..., -1.1436e-02,\n",
            "            2.1743e-01, -7.4340e-01],\n",
            "          [-7.5630e-01, -8.7890e-01,  2.1164e-01,  ...,  4.2274e-01,\n",
            "            2.4933e-01, -6.3393e-01]],\n",
            "\n",
            "         [[-2.3667e-04, -1.8001e-03,  1.0325e-02,  ...,  3.3241e-03,\n",
            "            3.8129e-01, -3.8581e-01],\n",
            "          [-3.7790e-03,  3.8997e-01,  2.3223e-02,  ..., -4.4650e-01,\n",
            "           -1.5155e+00,  1.5972e+00],\n",
            "          [ 4.2818e-01, -7.1232e-03,  5.6100e-01,  ..., -3.8220e-01,\n",
            "           -1.6488e+00,  1.8822e+00],\n",
            "          ...,\n",
            "          [-2.5469e-02,  1.8773e-02, -2.2765e-01,  ..., -1.0335e-01,\n",
            "           -1.6164e+00,  1.7820e+00],\n",
            "          [ 1.7841e-01,  2.5647e-01, -2.5559e-01,  ..., -2.4517e-01,\n",
            "           -1.6548e+00,  1.8613e+00],\n",
            "          [-2.3123e-01,  1.8115e-01,  2.8308e-02,  ..., -4.7267e-01,\n",
            "           -2.0581e+00,  2.0382e+00]],\n",
            "\n",
            "         [[ 1.6637e-02,  1.1601e-03,  8.4415e-03,  ..., -1.6057e-01,\n",
            "            4.2543e-01, -4.1076e-01],\n",
            "          [ 2.1951e+00, -1.3034e+00,  6.9577e-01,  ...,  1.0290e+00,\n",
            "           -3.1231e-01,  1.7332e+00],\n",
            "          [ 1.3378e+00, -1.9864e+00, -3.6994e-01,  ..., -8.6207e-03,\n",
            "           -1.3416e+00,  2.7927e+00],\n",
            "          ...,\n",
            "          [ 4.3617e-01,  9.8368e-01,  3.1211e-01,  ...,  8.1935e-01,\n",
            "           -2.7732e+00,  1.8223e+00],\n",
            "          [ 1.6359e+00, -8.7389e-02,  2.1412e-01,  ...,  1.8432e+00,\n",
            "           -2.6328e+00,  2.4435e+00],\n",
            "          [ 1.6411e+00, -5.6781e-01,  1.5192e-02,  ...,  1.8104e+00,\n",
            "           -2.0168e+00,  2.2548e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.9321e-03,  5.2216e-03,  1.5180e-02,  ...,  4.1295e-02,\n",
            "            4.4033e-02,  9.5111e-02],\n",
            "          [ 8.1601e-01, -9.8782e-02,  5.4513e-01,  ..., -3.1958e-01,\n",
            "           -5.7862e-01, -6.1950e-01],\n",
            "          [ 1.3373e+00, -4.0064e-01,  6.1497e-01,  ..., -3.2100e-02,\n",
            "           -2.5865e-01,  2.4648e-01],\n",
            "          ...,\n",
            "          [-9.5223e-01,  2.4384e-01,  3.0282e-01,  ...,  4.9176e-01,\n",
            "           -2.7774e-01, -2.0479e-01],\n",
            "          [ 1.4131e-01,  6.7407e-02, -4.9415e-01,  ..., -9.7463e-02,\n",
            "            2.2785e-01, -2.1736e-01],\n",
            "          [ 3.0278e-01,  2.8454e-01, -1.4638e-01,  ..., -2.9336e-02,\n",
            "           -5.7448e-02, -1.0451e-01]],\n",
            "\n",
            "         [[-2.3517e-02,  1.1636e-02, -8.0621e-03,  ..., -1.0021e-01,\n",
            "           -4.8923e-02, -2.2826e-02],\n",
            "          [-1.1295e+00,  7.1364e-01,  1.6987e-01,  ...,  1.9357e+00,\n",
            "           -6.7020e-01, -1.4108e-01],\n",
            "          [-2.3131e+00,  1.4548e+00, -7.2814e-01,  ...,  6.1473e-01,\n",
            "            9.2521e-02, -3.2437e-01],\n",
            "          ...,\n",
            "          [ 1.7239e+00, -1.6698e+00,  3.3799e-01,  ...,  3.1895e-01,\n",
            "            2.7013e-01, -5.1493e-01],\n",
            "          [-1.6928e-01, -9.5347e-01, -2.4673e-01,  ..., -6.0270e-03,\n",
            "            3.8347e-01, -5.7482e-01],\n",
            "          [-1.4576e+00, -2.0588e-01, -1.5943e-01,  ...,  3.9482e-01,\n",
            "           -2.1811e-01, -4.4904e-01]],\n",
            "\n",
            "         [[-2.4629e-02, -1.4320e-02, -1.7461e-02,  ...,  1.6419e-01,\n",
            "            5.7673e-01,  9.7806e-02],\n",
            "          [ 7.8448e-01, -1.9667e-01, -4.5592e-01,  ..., -8.2401e-01,\n",
            "           -2.2500e+00, -1.4724e+00],\n",
            "          [-8.4795e-01, -6.9422e-02, -6.1781e-01,  ..., -1.3014e+00,\n",
            "           -2.5811e+00, -1.3461e+00],\n",
            "          ...,\n",
            "          [ 1.9312e+00, -3.5970e-02,  8.4871e-01,  ..., -4.8565e-01,\n",
            "           -2.4429e+00, -6.8826e-01],\n",
            "          [ 7.8675e-01,  8.7332e-01,  4.3991e-01,  ..., -1.2868e+00,\n",
            "           -2.4549e+00,  2.7136e-01],\n",
            "          [-1.1102e-01, -7.4848e-02,  4.6224e-01,  ..., -1.5979e-01,\n",
            "           -2.5464e+00, -4.1130e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 3.7948e-03, -5.8593e-04,  3.9494e-03,  ..., -6.6470e-04,\n",
            "           -1.1058e-03, -4.0000e-04],\n",
            "          [ 5.7143e-02, -2.8510e-02, -9.3428e-02,  ...,  2.7918e-01,\n",
            "           -1.4961e-01, -3.1494e-01],\n",
            "          [-5.4405e-02,  1.6106e-01,  3.7653e-02,  ..., -3.5636e-02,\n",
            "            7.7478e-02,  1.6014e-01],\n",
            "          ...,\n",
            "          [ 4.0856e-01,  1.8108e-01,  7.8260e-02,  ...,  6.8023e-02,\n",
            "           -3.4536e-02,  8.1185e-02],\n",
            "          [-3.2828e-01,  2.9384e-01, -8.3994e-03,  ...,  2.1601e-02,\n",
            "           -8.6286e-02, -2.8498e-01],\n",
            "          [-2.5829e-01,  1.3057e-01,  5.8681e-02,  ..., -8.3848e-02,\n",
            "            1.1176e-01, -2.3217e-02]],\n",
            "\n",
            "         [[ 1.1474e-02,  3.8197e-03, -3.2497e-03,  ..., -2.0240e-03,\n",
            "            2.0265e-03, -2.1936e-04],\n",
            "          [-9.9134e-02,  1.7266e-01, -1.7313e-01,  ...,  1.6600e-01,\n",
            "            1.3949e-04,  2.1770e-01],\n",
            "          [-7.7499e-01,  3.9301e-02, -8.3635e-02,  ...,  5.0368e-02,\n",
            "            4.7602e-01,  2.5072e-01],\n",
            "          ...,\n",
            "          [-3.8275e-01,  1.2289e-01,  1.7797e-01,  ...,  2.6471e-01,\n",
            "            1.0739e-01,  3.7838e-02],\n",
            "          [-2.9644e-02,  1.7600e-01,  1.5932e-01,  ...,  2.3014e-01,\n",
            "            1.7064e-01,  2.1310e-01],\n",
            "          [-5.5179e-02, -1.8347e-01, -4.8479e-02,  ..., -1.9736e-01,\n",
            "            1.5946e-01,  2.2523e-02]],\n",
            "\n",
            "         [[-5.5398e-04, -1.2023e-03,  2.0765e-03,  ...,  8.4870e-04,\n",
            "           -3.5075e-03, -1.6486e-03],\n",
            "          [ 1.9109e-01, -8.8554e-02, -2.5522e-01,  ...,  8.1320e-02,\n",
            "            1.2065e-01, -6.4364e-02],\n",
            "          [-2.7562e-01, -1.5356e-01, -3.2742e-01,  ...,  1.1594e-01,\n",
            "            8.3694e-02,  2.3653e-01],\n",
            "          ...,\n",
            "          [ 5.8163e-02, -1.2758e-01, -8.8128e-02,  ..., -1.3025e-02,\n",
            "            5.7345e-02, -1.2144e-02],\n",
            "          [ 6.3458e-03, -8.4574e-02, -1.9858e-01,  ..., -1.0805e-01,\n",
            "           -6.6013e-02,  4.8130e-02],\n",
            "          [-1.5947e-01, -9.4290e-02,  1.1194e-01,  ...,  6.9066e-02,\n",
            "            7.1622e-02, -9.6680e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0463e-03,  1.3868e-02, -2.1922e-03,  ...,  5.1034e-03,\n",
            "           -4.4236e-03, -5.1040e-05],\n",
            "          [-5.9990e-02,  3.7350e-01,  9.2150e-02,  ..., -3.5939e-01,\n",
            "            4.9731e-03, -8.6912e-02],\n",
            "          [-3.9881e-02,  1.1517e-01,  1.5754e-01,  ..., -4.2556e-02,\n",
            "           -1.5337e-01,  6.1579e-02],\n",
            "          ...,\n",
            "          [-1.6798e-01,  1.9165e-01,  1.0914e-01,  ...,  1.9914e-01,\n",
            "            2.2739e-01,  1.2900e-01],\n",
            "          [-1.0336e-01, -7.8607e-02,  1.6992e-01,  ..., -4.0843e-01,\n",
            "           -3.1941e-01,  4.3582e-02],\n",
            "          [-2.1217e-01,  4.2193e-02,  7.1364e-03,  ..., -9.3355e-02,\n",
            "           -1.5793e-01,  1.8599e-01]],\n",
            "\n",
            "         [[ 2.1106e-03, -6.4423e-03,  1.8588e-03,  ..., -4.1733e-03,\n",
            "           -3.7815e-03, -5.2128e-03],\n",
            "          [ 4.1288e-02,  2.8080e-01, -3.2554e-02,  ...,  6.7576e-02,\n",
            "            6.5287e-02, -4.6041e-02],\n",
            "          [ 1.2323e-01,  1.8150e-01,  1.7123e-01,  ...,  1.6265e-01,\n",
            "            8.1633e-02, -2.6133e-01],\n",
            "          ...,\n",
            "          [-2.2655e-01,  5.0174e-02,  7.8552e-02,  ...,  2.5914e-04,\n",
            "            1.6805e-01,  1.0528e-01],\n",
            "          [-7.5173e-02,  5.3535e-01,  1.6705e-01,  ..., -1.4334e-01,\n",
            "            1.1035e-01, -2.3307e-01],\n",
            "          [ 5.7814e-03, -4.6051e-01, -1.0086e-01,  ..., -9.3643e-02,\n",
            "            2.0412e-01,  1.3689e-01]],\n",
            "\n",
            "         [[-2.2497e-03, -1.4471e-03, -1.0252e-03,  ...,  1.7904e-02,\n",
            "            1.3910e-03, -3.6785e-03],\n",
            "          [-1.8820e-02, -1.8643e-01,  1.5637e-02,  ..., -3.1862e-01,\n",
            "           -1.9134e-01, -7.1209e-02],\n",
            "          [-1.0118e-01, -6.5148e-02, -1.4708e-01,  ..., -1.1027e-01,\n",
            "           -1.9724e-01, -2.2394e-01],\n",
            "          ...,\n",
            "          [-1.2229e-01, -2.6068e-01,  6.2691e-02,  ...,  5.6863e-02,\n",
            "           -8.0564e-02, -1.3329e-01],\n",
            "          [-1.6062e-03, -2.1922e-01,  1.8071e-01,  ..., -1.8098e-01,\n",
            "            1.2251e-01,  1.0884e-01],\n",
            "          [-1.4675e-01,  1.6417e-01,  1.8250e-01,  ...,  1.7141e-02,\n",
            "           -8.4142e-02, -1.4529e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.3467e-02, -1.9558e-02,  1.2696e-02,  ...,  6.3502e-01,\n",
            "            5.5918e-02, -6.3423e-01],\n",
            "          [-6.5702e-01,  3.6407e-01, -4.3848e-01,  ..., -2.8588e+00,\n",
            "            4.9745e-01,  3.0555e+00],\n",
            "          [-8.2290e-01, -1.2641e+00,  1.0057e+00,  ..., -2.5411e+00,\n",
            "           -2.2286e+00,  2.7362e+00],\n",
            "          ...,\n",
            "          [ 5.3897e-02,  1.4715e+00, -7.3013e-01,  ..., -2.5882e+00,\n",
            "           -3.1753e+00,  2.7601e+00],\n",
            "          [-1.0123e+00,  5.4018e-01, -5.9578e-01,  ..., -2.7402e+00,\n",
            "           -1.4389e+00,  2.8395e+00],\n",
            "          [-7.7921e-02,  3.8298e-02, -3.4829e-01,  ..., -2.5639e+00,\n",
            "           -1.1149e+00,  2.6469e+00]],\n",
            "\n",
            "         [[ 7.0813e-03,  2.1064e-02, -9.6315e-03,  ..., -4.2709e-01,\n",
            "           -3.5756e-01, -2.7985e-01],\n",
            "          [-3.3704e-01, -4.7194e-01, -2.9262e-01,  ...,  1.5830e+00,\n",
            "            1.3329e+00,  1.1312e+00],\n",
            "          [-2.9693e-01,  1.8313e-01, -6.7898e-02,  ...,  2.0062e+00,\n",
            "            1.4452e+00,  1.6674e+00],\n",
            "          ...,\n",
            "          [-2.4279e-01, -3.6487e-01, -1.7657e-02,  ...,  1.6042e+00,\n",
            "            1.9673e+00,  1.5834e+00],\n",
            "          [-1.5412e-01, -1.2498e-01,  1.2461e-01,  ...,  1.3100e+00,\n",
            "            1.1226e+00,  1.5737e+00],\n",
            "          [-8.8266e-02, -9.2413e-03,  6.2827e-02,  ...,  8.0787e-01,\n",
            "            1.0797e+00,  5.9938e-01]],\n",
            "\n",
            "         [[ 1.0661e-02,  1.4195e-02,  1.8130e-02,  ...,  1.3225e+00,\n",
            "           -1.4461e-01,  2.6040e-01],\n",
            "          [-9.8895e-01, -5.2572e-01,  1.4478e-01,  ..., -5.1328e+00,\n",
            "           -5.2093e-01,  1.5631e-01],\n",
            "          [ 8.3042e-01, -7.0587e-01, -9.6404e-01,  ..., -4.8586e+00,\n",
            "            1.3697e+00, -1.3785e+00],\n",
            "          ...,\n",
            "          [-1.5031e+00,  6.7526e-01,  6.1914e-01,  ..., -5.0030e+00,\n",
            "            1.8327e+00, -1.8326e+00],\n",
            "          [-1.0769e+00,  1.2110e-01,  8.3482e-01,  ..., -4.9545e+00,\n",
            "            7.3497e-01, -9.1699e-01],\n",
            "          [-2.0907e-01,  1.1134e-01,  8.8033e-01,  ..., -5.4582e+00,\n",
            "            8.8238e-02, -7.0235e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.5531e-03, -1.4920e-02,  6.8578e-03,  ...,  8.9680e-01,\n",
            "           -8.4168e-02,  6.2646e-02],\n",
            "          [ 5.2796e-01,  4.9790e-01,  3.1227e-01,  ..., -2.8950e+00,\n",
            "            2.3034e-01,  8.9678e-01],\n",
            "          [-2.4852e-01,  5.2019e-01, -2.1746e-01,  ..., -2.6762e+00,\n",
            "           -6.5485e-03,  8.7707e-01],\n",
            "          ...,\n",
            "          [ 3.4585e-01, -3.5126e-01,  2.3354e-02,  ..., -2.5727e+00,\n",
            "            1.2739e+00,  1.0660e+00],\n",
            "          [ 1.4279e-01, -7.9369e-02, -1.8848e-01,  ..., -2.6557e+00,\n",
            "            1.5694e-01,  9.1346e-01],\n",
            "          [-3.7378e-02,  5.5383e-01,  3.1390e-01,  ..., -3.1032e+00,\n",
            "            1.4733e+00,  9.7109e-01]],\n",
            "\n",
            "         [[ 1.6637e-02,  1.6215e-02, -6.1247e-03,  ...,  9.6903e-02,\n",
            "           -1.0101e-01, -2.8477e-02],\n",
            "          [-3.8812e-01, -3.1210e-01,  6.7215e-01,  ..., -1.2553e+00,\n",
            "            6.8122e-01, -4.1316e-01],\n",
            "          [-2.9770e-01,  1.6145e-01, -1.8434e-04,  ..., -1.0128e+00,\n",
            "            1.0527e+00, -2.2919e-01],\n",
            "          ...,\n",
            "          [-2.7639e-01, -3.5241e-01, -5.0747e-02,  ..., -1.3698e+00,\n",
            "            2.0741e-01, -1.4046e+00],\n",
            "          [-9.9653e-02, -3.3351e-01, -8.4570e-03,  ..., -7.0482e-01,\n",
            "            9.1567e-01,  4.9923e-01],\n",
            "          [-2.7944e-01, -3.6443e-01, -6.4718e-02,  ..., -9.3075e-01,\n",
            "            1.6804e+00, -4.9463e-01]],\n",
            "\n",
            "         [[ 8.0904e-03,  1.7065e-02,  2.6404e-02,  ...,  8.2745e-01,\n",
            "           -7.8907e-01, -6.9350e-01],\n",
            "          [-2.9842e-01,  2.9092e-02, -4.2346e-02,  ..., -4.3800e+00,\n",
            "            2.5728e-01,  3.8400e+00],\n",
            "          [ 6.4321e-01,  4.3504e-01, -6.9660e-02,  ..., -1.3305e+00,\n",
            "            2.8832e+00,  1.8831e+00],\n",
            "          ...,\n",
            "          [-5.9018e-01, -8.7264e-02, -3.1416e-01,  ..., -4.2957e+00,\n",
            "            3.3620e+00,  3.1513e+00],\n",
            "          [-4.2589e-01,  5.3273e-02,  9.1163e-02,  ..., -5.1306e+00,\n",
            "            5.5509e+00,  1.4335e+00],\n",
            "          [-1.6067e-02,  1.6335e-02,  1.3959e-01,  ..., -4.5447e+00,\n",
            "            3.3125e+00,  4.7296e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 2.2254e-03,  8.1092e-04,  1.5628e-03,  ..., -3.0067e-03,\n",
            "           -3.1567e-03,  2.1146e-03],\n",
            "          [-1.7315e-01,  4.2066e-01,  1.6983e-01,  ...,  8.4994e-03,\n",
            "            7.4960e-02,  2.4533e-01],\n",
            "          [ 1.0234e-01,  1.6567e-01, -2.5322e-01,  ..., -8.6782e-02,\n",
            "            2.3760e-01, -1.8835e-01],\n",
            "          ...,\n",
            "          [ 3.9636e-01,  3.9742e-02, -5.5709e-03,  ...,  2.0560e-01,\n",
            "           -9.1253e-02, -3.7539e-01],\n",
            "          [ 3.0089e-01,  1.2286e-01, -3.1813e-01,  ..., -3.2923e-01,\n",
            "            1.1493e-01, -2.6592e-01],\n",
            "          [ 1.8100e-02,  3.5109e-02,  1.1804e-01,  ...,  2.5141e-01,\n",
            "           -2.0408e-01, -3.8974e-02]],\n",
            "\n",
            "         [[ 4.2550e-03, -1.9774e-03, -2.3665e-04,  ..., -9.1869e-04,\n",
            "            3.1291e-03, -2.6933e-03],\n",
            "          [-2.2964e-01,  4.9591e-02, -3.1734e-01,  ..., -4.3447e-02,\n",
            "            6.0649e-03, -1.0846e-01],\n",
            "          [-2.0736e-01, -5.1205e-02, -1.5442e-01,  ..., -8.9986e-02,\n",
            "           -2.0383e-01, -2.0987e-01],\n",
            "          ...,\n",
            "          [-2.7160e-01, -5.0642e-02,  7.8181e-02,  ...,  9.9338e-02,\n",
            "           -1.9247e-01, -1.6856e-01],\n",
            "          [-5.9061e-02, -1.0839e-01,  8.0069e-02,  ...,  1.6103e-01,\n",
            "           -1.2719e-01,  3.5980e-01],\n",
            "          [ 8.7243e-02,  2.2872e-01,  2.4102e-01,  ..., -7.2511e-02,\n",
            "           -1.1727e-01, -5.2028e-01]],\n",
            "\n",
            "         [[-1.2726e-03,  3.2550e-03, -8.4816e-03,  ...,  1.7844e-03,\n",
            "           -1.1540e-03,  3.3008e-03],\n",
            "          [-1.1174e-01, -2.5081e-02,  1.5794e-01,  ..., -1.8200e-01,\n",
            "            1.1558e-01,  1.4642e-01],\n",
            "          [ 3.9028e-02,  1.9566e-01, -7.7175e-02,  ..., -6.8922e-01,\n",
            "           -5.6884e-01, -1.8714e-01],\n",
            "          ...,\n",
            "          [ 1.4031e-01, -2.0932e-01,  6.1972e-01,  ..., -3.6823e-01,\n",
            "           -1.7986e-01,  8.3413e-03],\n",
            "          [-9.8745e-02,  2.7632e-01,  1.9492e-01,  ..., -2.6474e-01,\n",
            "           -3.0279e-01, -2.4330e-01],\n",
            "          [ 3.3061e-01,  2.5355e-02,  2.5159e-01,  ..., -4.4531e-01,\n",
            "           -2.7122e-02,  3.5580e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.1898e-03,  3.4121e-05,  6.2064e-04,  ...,  2.8390e-03,\n",
            "            6.4842e-04, -1.1182e-03],\n",
            "          [-3.5089e-01, -5.8228e-01, -2.6787e-01,  ..., -2.4847e-01,\n",
            "           -1.5514e-02, -2.0700e-01],\n",
            "          [-7.3064e-02, -5.8171e-01, -2.0325e-01,  ..., -1.2443e-01,\n",
            "           -1.3375e-01, -2.4032e-01],\n",
            "          ...,\n",
            "          [-1.7260e-01, -3.8298e-01,  2.7884e-01,  ...,  1.4935e-01,\n",
            "            3.2175e-01,  3.4607e-02],\n",
            "          [-1.6194e-01, -1.0819e-01, -3.3041e-01,  ...,  1.3605e-01,\n",
            "            9.5998e-02, -6.2081e-02],\n",
            "          [-1.3169e-01, -2.9779e-01,  3.2790e-01,  ..., -3.4390e-01,\n",
            "            2.0176e-02, -1.1069e-01]],\n",
            "\n",
            "         [[ 1.7833e-03, -1.3180e-03,  7.0127e-03,  ..., -3.6151e-03,\n",
            "            6.1501e-03, -8.4167e-03],\n",
            "          [ 1.7220e-01, -2.2318e-01, -7.1129e-01,  ..., -1.1668e-01,\n",
            "            5.8469e-02,  2.9376e-01],\n",
            "          [-6.0661e-02, -6.5740e-02,  1.8918e-02,  ...,  2.8620e-01,\n",
            "           -2.1005e-01, -2.3277e-02],\n",
            "          ...,\n",
            "          [ 7.9723e-04, -1.9770e-02, -5.0944e-01,  ...,  2.0470e-01,\n",
            "           -1.6700e-01,  9.0165e-02],\n",
            "          [ 2.8586e-01,  2.6974e-01,  3.6212e-01,  ...,  4.3601e-01,\n",
            "           -6.9200e-02, -2.8087e-01],\n",
            "          [ 9.2414e-03,  6.2551e-01, -6.3810e-02,  ..., -7.9419e-01,\n",
            "            3.9463e-01,  4.8666e-01]],\n",
            "\n",
            "         [[ 9.5654e-04,  4.6095e-03,  3.1098e-03,  ...,  3.1033e-03,\n",
            "           -4.8545e-03,  1.6219e-03],\n",
            "          [-1.0682e-01,  3.1030e-01, -1.0114e-01,  ..., -4.5818e-02,\n",
            "            1.1511e-01,  7.1378e-02],\n",
            "          [-4.8448e-02,  6.3127e-02,  1.8702e-01,  ..., -9.9897e-02,\n",
            "            4.6931e-02,  9.2666e-02],\n",
            "          ...,\n",
            "          [-7.0568e-02,  1.0183e-01,  2.1126e-01,  ..., -9.3313e-02,\n",
            "            9.3689e-02,  5.9239e-02],\n",
            "          [-2.6967e-02, -1.6149e-01,  2.9935e-02,  ..., -9.9596e-02,\n",
            "            1.4474e-01, -1.1464e-02],\n",
            "          [-1.4513e-01,  6.3340e-02, -5.5821e-02,  ..., -2.0070e-02,\n",
            "           -6.5182e-02, -5.6864e-03]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.4076e-02,  2.1938e-02, -5.7716e-03,  ...,  6.7540e-02,\n",
            "            6.0283e-02,  2.2073e-01],\n",
            "          [ 2.5116e+00,  6.4260e-01, -8.8977e-01,  ..., -6.2092e-01,\n",
            "           -4.2623e-01, -1.4609e+00],\n",
            "          [ 8.3121e-01,  1.0241e+00, -5.5178e-01,  ..., -9.2366e-01,\n",
            "           -8.2642e-01, -1.4119e+00],\n",
            "          ...,\n",
            "          [ 1.1076e+00, -8.2989e-01,  5.6396e-01,  ..., -6.7971e-01,\n",
            "           -1.4090e+00, -8.9179e-01],\n",
            "          [ 1.4343e+00, -2.8512e-01,  3.0318e-01,  ..., -2.4628e-01,\n",
            "           -2.4281e-01, -4.9839e-01],\n",
            "          [ 1.3404e+00, -1.3035e-01, -7.0034e-01,  ..., -1.6664e+00,\n",
            "           -3.4569e-01, -8.8232e-01]],\n",
            "\n",
            "         [[ 2.5422e-03, -5.1737e-04,  1.3189e-03,  ...,  7.8229e-01,\n",
            "            1.7948e-01,  4.7945e-01],\n",
            "          [ 4.7045e-01,  5.8621e-01,  3.8437e-01,  ..., -2.4704e+00,\n",
            "            1.7990e-01, -1.6991e+00],\n",
            "          [ 4.8114e-01, -7.8008e-03,  1.5300e-01,  ..., -1.7079e+00,\n",
            "           -8.6213e-01, -4.3648e-02],\n",
            "          ...,\n",
            "          [ 2.7263e-03,  3.0127e-01,  4.5607e-03,  ..., -9.2691e-01,\n",
            "           -1.0620e+00, -1.2851e+00],\n",
            "          [ 3.4530e-02,  6.2933e-01, -6.2392e-02,  ..., -1.1114e+00,\n",
            "           -2.0788e+00,  1.9677e-01],\n",
            "          [-1.0159e-01, -3.8936e-02, -2.1868e-02,  ..., -1.6583e+00,\n",
            "           -2.4622e+00, -1.1006e+00]],\n",
            "\n",
            "         [[-6.1496e-03, -3.8637e-02,  2.4638e-02,  ...,  2.3101e-02,\n",
            "           -9.9935e-02,  1.3199e-01],\n",
            "          [-8.3201e-02, -1.4876e-01,  2.6623e-01,  ...,  1.7221e-01,\n",
            "            1.2168e+00, -6.7360e-01],\n",
            "          [ 4.3427e-01,  1.9869e-01,  1.8264e-01,  ...,  9.0013e-01,\n",
            "            1.1536e+00,  6.0206e-01],\n",
            "          ...,\n",
            "          [-5.0148e-01,  1.9563e-01, -5.6392e-02,  ...,  1.5629e-01,\n",
            "           -4.7504e-02,  5.4955e-01],\n",
            "          [-9.4997e-02,  4.2543e-02,  9.8325e-02,  ...,  4.5793e-01,\n",
            "            1.3448e-01,  4.2818e-01],\n",
            "          [ 6.7808e-03, -4.6421e-01, -2.5898e-01,  ..., -3.3681e-01,\n",
            "           -4.1102e-01,  5.2216e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.4928e-03, -1.3432e-02,  2.1750e-03,  ..., -1.4812e-01,\n",
            "           -2.4757e-01, -1.7766e-01],\n",
            "          [ 6.4226e-01,  1.3802e-01,  4.4692e-01,  ...,  2.2099e-01,\n",
            "            1.2425e+00, -2.1612e-01],\n",
            "          [ 9.8415e-01,  4.2787e-01,  4.1787e-01,  ...,  1.2020e+00,\n",
            "            1.4495e+00,  1.5801e+00],\n",
            "          ...,\n",
            "          [-1.7760e-01, -5.0014e-01, -5.4849e-02,  ...,  8.1489e-01,\n",
            "            1.3016e+00,  7.3814e-01],\n",
            "          [ 2.2087e-01,  1.7759e-01, -5.5377e-01,  ...,  2.9195e-01,\n",
            "            1.6579e+00,  8.8636e-01],\n",
            "          [-1.0798e-01,  8.3784e-01, -3.4596e-01,  ...,  1.1664e+00,\n",
            "            1.7006e+00,  1.9808e-01]],\n",
            "\n",
            "         [[-2.5278e-02, -1.8712e-02, -1.2997e-03,  ..., -2.9312e-01,\n",
            "           -4.5217e-01, -3.7426e-01],\n",
            "          [ 1.0666e+00,  1.8963e-01, -1.9915e-01,  ...,  1.0001e+00,\n",
            "            1.5012e+00,  1.0754e+00],\n",
            "          [ 7.9284e-01, -7.1385e-02,  5.4051e-01,  ...,  1.8166e+00,\n",
            "            4.2250e-01, -1.9524e-01],\n",
            "          ...,\n",
            "          [-8.3962e-01, -1.6122e-01, -5.4759e-01,  ...,  9.6689e-01,\n",
            "            4.5267e-01,  4.6200e-01],\n",
            "          [ 2.7250e-01,  5.6720e-01, -1.9676e-02,  ...,  2.0469e+00,\n",
            "            6.9353e-01, -6.6677e-01],\n",
            "          [ 3.1842e-01, -1.0085e-01, -4.2030e-01,  ...,  1.0928e+00,\n",
            "            7.0274e-01,  1.5334e+00]],\n",
            "\n",
            "         [[ 1.5468e-02,  9.9472e-03,  7.3426e-03,  ..., -2.0616e-01,\n",
            "           -4.8480e-01, -1.5521e-01],\n",
            "          [-1.0722e+00,  1.0711e+00, -8.9306e-02,  ...,  5.9998e-01,\n",
            "            1.8673e+00, -1.1309e+00],\n",
            "          [-3.1193e-01,  5.6948e-02,  5.5653e-02,  ..., -6.3022e-01,\n",
            "            2.1023e+00, -4.3429e-01],\n",
            "          ...,\n",
            "          [ 2.0994e-01, -5.0747e-04, -1.3554e-01,  ..., -3.2657e-01,\n",
            "            1.7038e+00,  1.0123e-01],\n",
            "          [-5.1808e-01,  2.3147e-01, -7.6500e-02,  ..., -2.7355e-01,\n",
            "            1.3691e+00, -4.6841e-02],\n",
            "          [ 1.6859e-02,  7.2271e-01,  3.7692e-01,  ...,  9.2126e-01,\n",
            "            6.3832e-01,  5.6556e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 8.9141e-04,  2.7740e-03,  6.6752e-03,  ..., -3.8094e-03,\n",
            "           -4.1217e-03, -4.1950e-03],\n",
            "          [-3.6969e-02, -3.6432e-01, -2.5392e-01,  ...,  2.3780e-01,\n",
            "           -5.1187e-02, -3.1443e-01],\n",
            "          [ 1.9944e-01,  2.1611e-01,  6.0905e-03,  ...,  1.8889e-01,\n",
            "            2.6762e-01, -1.0711e-01],\n",
            "          ...,\n",
            "          [-1.9229e-01,  4.0381e-01,  2.4439e-01,  ..., -2.6850e-01,\n",
            "            6.5298e-02, -1.9536e-01],\n",
            "          [-4.0461e-02,  2.2994e-01, -5.2683e-02,  ..., -5.3476e-02,\n",
            "            2.1830e-01, -1.2683e-01],\n",
            "          [-1.8809e-01, -1.8906e-01, -3.1498e-01,  ..., -2.4195e-01,\n",
            "            1.1673e-01,  7.8368e-03]],\n",
            "\n",
            "         [[ 6.4970e-03, -7.0885e-03,  5.9180e-03,  ..., -6.5229e-03,\n",
            "            6.5027e-03,  1.0238e-02],\n",
            "          [ 1.4288e-02, -2.7143e-01, -4.0870e-02,  ...,  1.0909e-01,\n",
            "           -1.6970e-01, -2.4187e-01],\n",
            "          [-7.7169e-02,  4.4613e-01, -3.4482e-02,  ..., -6.0498e-02,\n",
            "           -1.7517e-01,  3.9166e-02],\n",
            "          ...,\n",
            "          [-3.8230e-01,  4.7924e-02,  1.2480e-02,  ...,  7.4983e-02,\n",
            "           -6.0965e-01,  1.5040e-01],\n",
            "          [-2.1552e-01, -1.1459e-01, -4.6993e-01,  ...,  2.8574e-02,\n",
            "           -3.5954e-01, -4.1825e-02],\n",
            "          [ 2.0817e-01,  1.1507e-01, -6.5505e-02,  ..., -4.1951e-01,\n",
            "            3.4763e-02,  8.9192e-02]],\n",
            "\n",
            "         [[ 5.6856e-03,  6.1410e-03, -7.1104e-03,  ...,  1.5291e-02,\n",
            "            8.8982e-03,  1.0350e-02],\n",
            "          [-3.4068e-01, -3.3158e-02, -7.6317e-02,  ..., -6.2180e-01,\n",
            "           -2.6197e-01, -1.1056e+00],\n",
            "          [ 5.2307e-02, -1.1307e-02, -3.9251e-02,  ..., -6.3288e-02,\n",
            "            3.1656e-01, -7.5135e-01],\n",
            "          ...,\n",
            "          [-4.0942e-02, -1.8888e-01, -1.9701e-01,  ...,  4.6092e-02,\n",
            "            1.8839e-01, -1.2166e-01],\n",
            "          [ 2.9889e-01, -2.5508e-01,  7.2951e-02,  ..., -1.8896e-01,\n",
            "            2.4845e-03, -4.0664e-01],\n",
            "          [-1.2358e-01, -1.0172e-01, -5.0226e-02,  ...,  5.5276e-02,\n",
            "            8.9660e-02,  9.6616e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1515e-04, -2.4955e-04,  2.3932e-03,  ...,  3.3140e-03,\n",
            "           -3.4691e-03, -1.6356e-03],\n",
            "          [-2.1202e-01, -8.9451e-02, -4.1639e-02,  ...,  1.1563e-01,\n",
            "            3.0612e-01,  4.7128e-01],\n",
            "          [-9.8380e-02,  3.0256e-01,  2.5219e-01,  ...,  6.3898e-02,\n",
            "           -2.1462e-01,  2.8960e-01],\n",
            "          ...,\n",
            "          [ 2.4517e-01, -2.9658e-02, -4.2864e-01,  ...,  1.2691e-01,\n",
            "            5.9812e-02,  1.7605e-01],\n",
            "          [-2.5114e-01,  3.2020e-01,  3.6912e-01,  ..., -2.8118e-02,\n",
            "            4.4330e-03,  3.2537e-01],\n",
            "          [ 1.2337e-01,  9.1388e-02, -2.6953e-01,  ..., -7.2677e-02,\n",
            "            2.9071e-02, -1.2936e-01]],\n",
            "\n",
            "         [[-2.0057e-03,  3.8717e-03, -2.9557e-03,  ..., -9.4287e-03,\n",
            "           -1.2136e-03,  1.2125e-03],\n",
            "          [-1.2071e-02,  1.7361e-01, -1.6849e-01,  ..., -8.8774e-02,\n",
            "           -8.8714e-02,  1.0125e-01],\n",
            "          [-3.0958e-01,  1.1191e-01,  4.7249e-03,  ...,  6.1409e-01,\n",
            "           -1.3260e-01,  2.8415e-01],\n",
            "          ...,\n",
            "          [-1.7937e-01, -2.4914e-01,  1.0986e-01,  ...,  3.3191e-02,\n",
            "            1.7283e-03, -2.1134e-01],\n",
            "          [-1.1494e-01,  1.1458e-01,  4.2203e-02,  ...,  1.5032e-01,\n",
            "           -2.8577e-01,  8.1633e-03],\n",
            "          [ 7.1026e-02, -9.8375e-02,  1.9092e-01,  ...,  6.9301e-02,\n",
            "           -1.8939e-02,  1.1893e-01]],\n",
            "\n",
            "         [[ 7.6777e-03, -2.0246e-02, -3.8583e-03,  ...,  3.0491e-03,\n",
            "           -5.3123e-03, -3.5091e-03],\n",
            "          [-6.8272e-02, -2.9498e-01, -1.9250e-01,  ...,  2.3689e-01,\n",
            "            2.1034e-01,  1.2104e-01],\n",
            "          [ 4.9690e-02, -6.2281e-03, -1.2992e-01,  ...,  3.1751e-01,\n",
            "           -2.4068e-01, -3.3138e-02],\n",
            "          ...,\n",
            "          [ 2.2537e-01,  1.8958e-02, -1.3987e-01,  ...,  2.5805e-01,\n",
            "           -2.2900e-01, -3.4129e-02],\n",
            "          [ 3.0383e-01, -2.6258e-01,  2.3596e-01,  ..., -6.7313e-02,\n",
            "           -1.5291e-01, -3.3710e-01],\n",
            "          [ 6.3614e-02, -9.8172e-02, -1.1657e-01,  ...,  1.2613e-01,\n",
            "            4.7715e-02, -9.5547e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-8.1072e-04,  9.7849e-03,  1.0134e-03,  ..., -1.1650e-03,\n",
            "           -2.2360e-01, -1.6458e-02],\n",
            "          [-5.3204e-02, -3.5550e-01,  2.1326e-01,  ..., -5.8818e-01,\n",
            "            1.8078e-01, -4.6343e-01],\n",
            "          [-6.7985e-01, -1.0202e+00,  5.8453e-01,  ...,  2.5351e-01,\n",
            "            1.0139e+00, -2.4062e-01],\n",
            "          ...,\n",
            "          [ 1.3537e-01,  7.8138e-01, -9.3599e-01,  ...,  7.2607e-01,\n",
            "            7.3970e-01, -4.9703e-01],\n",
            "          [ 3.7924e-01,  3.2444e-01,  6.0585e-03,  ..., -1.3393e+00,\n",
            "           -3.7978e-01,  5.4907e-01],\n",
            "          [-2.9369e-01, -5.9880e-02, -7.1857e-01,  ...,  6.3866e-02,\n",
            "            1.2427e+00,  1.5292e+00]],\n",
            "\n",
            "         [[ 2.3682e-03,  3.4308e-02, -2.6234e-02,  ...,  3.3895e-02,\n",
            "            2.3597e-03, -8.2476e-04],\n",
            "          [ 6.0691e-01,  3.4153e-01,  4.1038e-01,  ...,  1.7505e-01,\n",
            "           -2.2370e-01, -1.0297e+00],\n",
            "          [ 6.0922e-01,  1.8747e-03, -3.0187e-01,  ...,  3.8432e-01,\n",
            "            1.2313e-01,  8.8763e-01],\n",
            "          ...,\n",
            "          [ 1.5756e-02, -4.0860e-01,  1.2749e-01,  ..., -4.4933e-01,\n",
            "           -1.3888e+00,  1.3219e+00],\n",
            "          [ 7.9149e-01,  5.1358e-01,  2.0099e-01,  ...,  2.0044e-01,\n",
            "            6.5084e-01,  1.2819e-02],\n",
            "          [ 9.2076e-01, -1.5256e-01, -2.1014e-01,  ...,  2.9845e-01,\n",
            "           -5.2595e-01,  4.7757e-01]],\n",
            "\n",
            "         [[-1.0470e-02,  3.8635e-02, -1.3558e-02,  ..., -4.9675e-01,\n",
            "            3.4355e-01, -8.4899e-01],\n",
            "          [ 2.4614e-01, -5.7155e-01, -3.5821e-01,  ...,  9.0341e-02,\n",
            "           -5.2822e-01,  3.2384e+00],\n",
            "          [ 1.8723e+00, -1.8350e+00, -1.0535e+00,  ...,  1.2015e+00,\n",
            "           -1.3721e+00,  2.8775e+00],\n",
            "          ...,\n",
            "          [-6.3750e-01,  1.3205e+00,  5.3477e-01,  ...,  1.0402e+00,\n",
            "           -1.7478e+00,  2.8043e+00],\n",
            "          [ 1.4999e-01, -3.0031e-01,  1.3071e+00,  ...,  1.4273e+00,\n",
            "           -1.7477e+00,  3.1746e+00],\n",
            "          [ 1.2345e+00, -1.4725e+00,  5.5298e-01,  ...,  2.0586e+00,\n",
            "           -1.9143e+00,  2.6556e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.2002e-03, -4.1334e-03, -7.6975e-03,  ..., -4.8270e-01,\n",
            "           -4.4574e-01,  5.1268e-02],\n",
            "          [-3.5735e-02,  6.5577e-01,  1.7312e-01,  ...,  1.9898e+00,\n",
            "            2.9664e+00, -2.4782e-01],\n",
            "          [-3.3622e-01, -1.2542e-02,  3.6546e-02,  ...,  3.6399e+00,\n",
            "            1.4575e+00, -1.6369e-01],\n",
            "          ...,\n",
            "          [-1.2052e-01, -2.5362e-01,  1.1563e-01,  ...,  2.8514e+00,\n",
            "            2.4384e+00, -5.5928e-01],\n",
            "          [ 2.6068e-02, -1.4964e-02,  3.6234e-02,  ...,  2.8301e+00,\n",
            "            2.2139e+00, -1.0402e+00],\n",
            "          [-1.9560e-01,  7.6457e-01, -3.8025e-01,  ...,  1.8650e+00,\n",
            "            1.5883e+00, -7.9097e-01]],\n",
            "\n",
            "         [[-1.1429e-02, -6.9195e-03, -6.4464e-03,  ..., -2.0344e-01,\n",
            "           -1.0407e-01, -8.1791e-02],\n",
            "          [ 9.8703e-02, -3.5527e-01,  5.7157e-01,  ...,  5.8732e-01,\n",
            "            1.1443e+00,  9.0042e-01],\n",
            "          [ 1.2051e-01, -4.9453e-01,  5.0860e-01,  ...,  1.0563e+00,\n",
            "            9.2213e-01, -6.0780e-02],\n",
            "          ...,\n",
            "          [-2.5250e-01,  3.2575e-01, -4.9585e-01,  ...,  6.1454e-01,\n",
            "            9.4237e-01,  2.5270e-01],\n",
            "          [-1.6396e-01, -4.3500e-01,  1.2146e-01,  ...,  3.4404e-01,\n",
            "            1.0024e+00,  3.8322e-01],\n",
            "          [ 2.1846e-01, -4.8198e-02,  6.9684e-02,  ...,  2.3650e+00,\n",
            "            1.8117e-01, -8.9373e-02]],\n",
            "\n",
            "         [[ 1.4687e-02,  5.1006e-03, -1.5159e-02,  ...,  8.7739e-01,\n",
            "           -1.0834e-01,  8.5772e-01],\n",
            "          [-1.8211e-01, -4.3094e-01, -7.2187e-01,  ..., -1.7214e+00,\n",
            "            7.9728e-01, -3.3480e+00],\n",
            "          [-8.8953e-02, -1.5638e+00, -2.0620e-01,  ..., -4.9045e-01,\n",
            "            4.0021e-01, -2.9050e+00],\n",
            "          ...,\n",
            "          [ 5.7468e-01,  8.2909e-01,  5.4130e-02,  ..., -1.1733e+00,\n",
            "            1.6597e+00, -2.4231e+00],\n",
            "          [-1.7128e-01,  5.0780e-03, -2.9512e-01,  ..., -2.2730e+00,\n",
            "            1.3606e+00, -2.0846e+00],\n",
            "          [ 4.2978e-02, -3.3444e-01,  3.1692e-01,  ..., -1.5073e+00,\n",
            "            9.4291e-01, -2.0551e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 5.7080e-03, -1.3533e-02, -3.2038e-03,  ...,  5.2712e-03,\n",
            "            3.3676e-03, -2.0978e-02],\n",
            "          [ 1.8140e-01, -1.9224e-01,  1.9191e-01,  ..., -4.9339e-01,\n",
            "           -4.5691e-01,  5.0219e-01],\n",
            "          [ 2.4652e-01,  3.4704e-01,  1.6011e-01,  ..., -6.2877e-01,\n",
            "            3.8067e-01, -2.2308e-01],\n",
            "          ...,\n",
            "          [-1.0869e-03,  2.7834e-01,  2.4195e-01,  ..., -5.2414e-02,\n",
            "            3.0192e-02,  1.0774e-01],\n",
            "          [ 3.9127e-02,  1.5560e-01, -5.5808e-01,  ...,  2.6307e-01,\n",
            "            2.1789e-02,  3.3493e-01],\n",
            "          [-2.4725e-02,  1.5305e-01,  3.0594e-01,  ...,  1.9589e-01,\n",
            "           -3.6729e-02,  1.5015e-01]],\n",
            "\n",
            "         [[ 2.2337e-03,  4.9083e-04, -4.2854e-03,  ..., -1.6119e-02,\n",
            "           -1.5730e-03, -9.1793e-03],\n",
            "          [ 9.2569e-02, -2.8969e-01,  1.6115e-02,  ...,  2.8465e-02,\n",
            "           -7.5979e-02,  5.1212e-01],\n",
            "          [-2.9864e-01, -2.1692e-01, -7.1289e-02,  ..., -1.8477e-01,\n",
            "            8.3122e-01,  3.1265e-01],\n",
            "          ...,\n",
            "          [-2.7000e-01, -1.3438e-01,  4.2453e-01,  ..., -5.2840e-01,\n",
            "            5.3911e-01,  2.0291e-01],\n",
            "          [-5.0457e-01,  3.6061e-01, -3.8541e-02,  ...,  1.5783e-01,\n",
            "            1.1590e-01,  5.3167e-01],\n",
            "          [ 5.1166e-01,  1.1347e-01, -4.3154e-01,  ..., -8.3350e-01,\n",
            "           -1.8328e-02,  3.6997e-01]],\n",
            "\n",
            "         [[-4.4570e-03,  4.8672e-03,  1.5937e-02,  ..., -8.4043e-03,\n",
            "            2.6623e-02,  1.7138e-03],\n",
            "          [ 1.2834e-01, -1.1393e-01, -2.1543e-01,  ...,  3.4262e-02,\n",
            "           -1.4127e-02, -3.2822e-01],\n",
            "          [-1.2897e-01, -1.0226e-01, -3.4026e-01,  ..., -5.8276e-01,\n",
            "            2.7280e-01,  1.3900e-01],\n",
            "          ...,\n",
            "          [ 6.2725e-02,  3.0068e-01, -6.7429e-02,  ..., -8.2714e-01,\n",
            "            6.9617e-02, -1.4262e-02],\n",
            "          [-1.5953e-01,  1.7767e-01, -1.1112e-02,  ..., -1.5570e-01,\n",
            "           -1.5613e-01, -3.7146e-01],\n",
            "          [-5.4027e-01, -3.7728e-01,  2.6024e-01,  ..., -5.5248e-02,\n",
            "           -1.2812e-01,  6.8979e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.4323e-03,  2.0059e-03, -1.1719e-04,  ...,  3.4577e-03,\n",
            "           -1.4194e-03, -3.0115e-03],\n",
            "          [-1.3805e-01, -6.0129e-01, -5.4675e-01,  ...,  1.7345e-01,\n",
            "            4.6018e-02, -1.3954e-01],\n",
            "          [ 1.0938e-01, -6.0598e-02, -9.5233e-01,  ..., -1.5849e-01,\n",
            "            1.7494e-01, -8.1308e-02],\n",
            "          ...,\n",
            "          [-1.4765e-01, -2.8967e-01, -3.0539e-01,  ..., -5.7440e-02,\n",
            "            2.2019e-01,  1.2892e-01],\n",
            "          [ 2.6444e-01,  4.3180e-02, -2.1599e-01,  ..., -2.0218e-01,\n",
            "           -1.3562e-03,  1.6223e-02],\n",
            "          [ 8.9689e-02,  8.7499e-02, -1.0612e-01,  ..., -2.8523e-01,\n",
            "            4.5863e-01, -3.1032e-01]],\n",
            "\n",
            "         [[-2.6229e-03,  1.3290e-03, -1.5243e-03,  ...,  9.8696e-04,\n",
            "           -5.4534e-03,  2.2786e-03],\n",
            "          [-2.1273e-01, -1.1330e-01,  9.3760e-02,  ...,  1.8195e-01,\n",
            "           -2.3614e-01, -3.3189e-01],\n",
            "          [-3.2708e-02, -2.9626e-01,  1.2093e-01,  ...,  3.9318e-01,\n",
            "           -3.7876e-02, -2.5756e-01],\n",
            "          ...,\n",
            "          [-1.5297e-02, -1.7876e-01, -2.2725e-02,  ...,  1.9246e-01,\n",
            "           -1.1769e-01, -2.0128e-01],\n",
            "          [ 2.4590e-01,  1.4775e-01,  6.1589e-02,  ...,  5.3618e-01,\n",
            "           -5.2723e-01, -2.9335e-01],\n",
            "          [ 3.2833e-02, -2.6208e-01,  2.9955e-01,  ...,  2.5300e-01,\n",
            "           -4.1590e-01, -5.1051e-01]],\n",
            "\n",
            "         [[-3.4460e-03,  1.3444e-02, -1.0362e-03,  ..., -1.4816e-03,\n",
            "            2.9568e-04, -2.0696e-03],\n",
            "          [ 2.6299e-01,  4.2509e-01, -1.3411e-01,  ..., -3.9996e-02,\n",
            "           -8.1246e-02, -2.6203e-01],\n",
            "          [-4.2957e-01,  7.5263e-02, -2.2209e-01,  ..., -1.1425e-01,\n",
            "            4.9150e-02,  1.4334e-01],\n",
            "          ...,\n",
            "          [ 1.2015e-01, -7.6427e-03, -1.1184e-01,  ..., -1.2395e-01,\n",
            "           -4.8527e-01,  1.5008e-02],\n",
            "          [ 9.7746e-02,  1.7870e-01,  3.8719e-01,  ..., -2.5467e-01,\n",
            "           -5.0123e-02,  6.2316e-02],\n",
            "          [ 1.0608e-01,  1.1317e-01, -3.5526e-01,  ..., -2.6440e-01,\n",
            "            1.9868e-01,  1.3938e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.9458e-03, -1.2133e-03, -3.6245e-03,  ...,  3.6563e-02,\n",
            "           -3.2896e-02,  6.1629e-01],\n",
            "          [ 1.3051e+00, -1.3499e+00,  9.6944e-02,  ...,  1.1082e+00,\n",
            "            6.5863e-03, -2.2281e+00],\n",
            "          [ 2.3405e-01, -6.6437e-01,  1.2533e-01,  ...,  1.7531e+00,\n",
            "           -4.7076e-01, -2.3116e+00],\n",
            "          ...,\n",
            "          [ 4.3359e-01, -5.9997e-01,  4.9853e-01,  ...,  4.2629e-01,\n",
            "           -8.6896e-01, -2.1243e+00],\n",
            "          [ 7.9510e-01, -8.8364e-01, -1.3728e-01,  ...,  1.3322e+00,\n",
            "           -1.4184e+00, -2.4948e+00],\n",
            "          [-2.9476e-01, -2.3231e+00,  3.8494e-01,  ...,  6.1001e-01,\n",
            "           -2.1619e+00, -2.3583e+00]],\n",
            "\n",
            "         [[-2.4838e-02,  6.4339e-03, -1.9311e-03,  ...,  1.2536e+00,\n",
            "           -2.1367e-01, -4.5170e-01],\n",
            "          [ 1.3995e+00,  2.6843e-01,  7.7617e-01,  ..., -2.4861e+00,\n",
            "            1.3589e-01,  1.2293e+00],\n",
            "          [ 1.7236e+00,  1.1752e+00,  1.4127e+00,  ..., -2.4623e+00,\n",
            "           -9.6153e-02,  1.7747e+00],\n",
            "          ...,\n",
            "          [-8.3569e-01, -8.4740e-01, -7.2502e-01,  ..., -2.0132e+00,\n",
            "            1.4109e+00,  6.4470e-01],\n",
            "          [ 3.0693e-01,  2.1456e-01,  3.1623e-01,  ..., -2.8884e+00,\n",
            "           -7.4488e-01,  5.9660e-01],\n",
            "          [ 5.0669e-01,  2.3288e-01,  3.7837e-01,  ..., -2.1485e+00,\n",
            "           -1.1718e+00,  5.8756e-01]],\n",
            "\n",
            "         [[ 2.0861e-02,  1.0963e-04,  2.1603e-02,  ...,  4.9574e-02,\n",
            "            1.8044e-02, -4.3301e-02],\n",
            "          [-7.4216e-01,  3.9996e-01, -1.6548e-01,  ...,  2.2677e-01,\n",
            "            2.4718e-01,  1.6113e+00],\n",
            "          [-4.1177e-01,  6.4952e-01, -2.4480e-01,  ...,  1.0595e+00,\n",
            "           -3.7890e-02,  1.9998e+00],\n",
            "          ...,\n",
            "          [-3.7926e-01, -4.7613e-01,  4.7613e-01,  ..., -9.2610e-01,\n",
            "            7.1565e-01,  6.5155e-01],\n",
            "          [-3.9115e-01,  1.2696e-01,  1.4344e-03,  ...,  7.7873e-01,\n",
            "           -5.4613e-01,  1.4102e+00],\n",
            "          [-9.4356e-01,  3.3372e-01, -1.7014e-01,  ...,  1.4466e+00,\n",
            "            1.9553e-01,  1.9500e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.1009e-02,  3.5573e-02,  4.7207e-03,  ...,  1.2994e-01,\n",
            "            1.3838e-01, -1.2962e-01],\n",
            "          [ 2.2016e-01,  3.3577e-01,  1.4423e+00,  ...,  1.8598e-01,\n",
            "           -3.4119e-01, -6.6943e-01],\n",
            "          [ 1.1967e+00,  1.0581e+00,  9.8617e-01,  ...,  1.4791e-01,\n",
            "           -9.0606e-01, -1.0619e+00],\n",
            "          ...,\n",
            "          [-5.6063e-01, -9.4608e-01, -1.2835e+00,  ..., -6.4253e-01,\n",
            "           -1.2484e+00, -8.0815e-01],\n",
            "          [-3.1635e-01,  3.5290e-01, -1.6515e-01,  ...,  5.4010e-02,\n",
            "           -3.0235e-02,  4.0850e-01],\n",
            "          [ 3.1907e-01, -6.4757e-02,  4.0089e-01,  ...,  5.8370e-01,\n",
            "            9.1501e-01,  1.2786e-01]],\n",
            "\n",
            "         [[-1.6830e-02, -2.5056e-02,  1.8452e-02,  ..., -1.8008e-01,\n",
            "            3.1760e-01,  5.6982e-01],\n",
            "          [-1.8513e-01, -2.5207e-01, -3.0895e-01,  ...,  5.3499e-01,\n",
            "           -1.0060e+00, -5.7922e-01],\n",
            "          [ 1.2568e-02, -4.3973e-01, -2.4143e-01,  ...,  6.7643e-01,\n",
            "           -1.3284e+00, -1.5404e+00],\n",
            "          ...,\n",
            "          [ 1.5214e-01,  1.1697e-01,  3.7932e-01,  ..., -9.8673e-01,\n",
            "           -3.5572e-01, -2.3620e-01],\n",
            "          [ 8.7195e-02,  3.6052e-01,  1.7499e-01,  ...,  1.6128e+00,\n",
            "           -2.1394e+00, -1.8499e+00],\n",
            "          [-1.4572e-01, -7.7302e-02,  5.0253e-02,  ..., -3.5315e-01,\n",
            "           -5.8910e-03,  6.0872e-01]],\n",
            "\n",
            "         [[ 1.1281e-02, -2.5821e-03,  3.9655e-02,  ..., -2.3398e-01,\n",
            "           -4.9289e-02,  1.3939e-01],\n",
            "          [ 9.7585e-01,  1.2671e-01, -9.8279e-01,  ...,  1.1960e+00,\n",
            "            2.1979e+00,  6.4227e-01],\n",
            "          [ 1.1450e+00, -5.9919e-02, -2.4681e-01,  ..., -3.6448e-01,\n",
            "            1.5676e+00,  5.5404e-01],\n",
            "          ...,\n",
            "          [-9.2614e-01,  1.2720e+00, -8.7965e-01,  ...,  1.5958e+00,\n",
            "            2.4842e+00,  3.4948e-02],\n",
            "          [-2.0824e-01,  9.7432e-01, -6.6099e-01,  ..., -1.4793e-01,\n",
            "            1.6918e+00, -4.7458e-01],\n",
            "          [-5.3222e-02, -6.3081e-02, -6.2902e-01,  ...,  1.0168e-01,\n",
            "            2.5748e+00, -5.5423e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-7.9432e-03, -1.0420e-02,  6.3880e-03,  ...,  1.4006e-02,\n",
            "           -4.6779e-04, -1.2620e-02],\n",
            "          [ 5.8340e-02, -4.2294e-01,  1.1296e-01,  ..., -5.5915e-01,\n",
            "            4.9851e-01, -2.9863e-01],\n",
            "          [-6.0847e-02,  1.8231e-02,  4.1229e-02,  ...,  3.7850e-01,\n",
            "            4.2114e-01,  1.8965e-01],\n",
            "          ...,\n",
            "          [-2.0866e-01,  2.1198e-01, -2.6371e-02,  ...,  8.4913e-01,\n",
            "            9.4665e-01,  1.9688e-01],\n",
            "          [-8.1516e-01, -4.8747e-01,  7.8221e-01,  ..., -4.1621e-01,\n",
            "            3.5923e-01, -5.4229e-02],\n",
            "          [-7.2788e-01,  1.7321e-01,  9.5954e-01,  ..., -2.0514e-01,\n",
            "            3.8627e-01, -5.4813e-02]],\n",
            "\n",
            "         [[ 4.2649e-03, -8.3109e-04, -4.7957e-03,  ...,  3.5470e-03,\n",
            "            1.5769e-02,  3.6537e-03],\n",
            "          [ 2.1446e-01,  1.1665e-01,  2.2548e-02,  ..., -2.7453e-01,\n",
            "            4.3647e-01, -1.7562e-01],\n",
            "          [-3.6864e-02, -9.2996e-02, -5.3451e-02,  ..., -5.4089e-02,\n",
            "            2.5680e-01, -2.4150e-01],\n",
            "          ...,\n",
            "          [ 3.6167e-01,  3.6079e-01,  3.1736e-01,  ..., -5.1220e-01,\n",
            "           -1.1487e-01, -4.1396e-02],\n",
            "          [ 1.9181e-01,  2.2227e-01, -8.7205e-02,  ..., -3.0182e-01,\n",
            "            4.5044e-01,  1.4123e-02],\n",
            "          [ 4.1388e-01, -2.3706e-01,  9.2534e-02,  ..., -3.0112e-01,\n",
            "           -1.4533e-03, -6.3206e-02]],\n",
            "\n",
            "         [[-1.9586e-02, -1.7569e-02,  1.2883e-02,  ...,  1.1076e-01,\n",
            "            1.1656e-02, -7.0091e-03],\n",
            "          [ 7.7044e-02, -1.3597e-01, -3.3016e-01,  ..., -9.5397e-01,\n",
            "            1.6710e-02,  4.4258e-01],\n",
            "          [ 1.9958e-01, -1.0537e+00, -2.1620e-01,  ..., -3.3066e-01,\n",
            "            6.8259e-01,  1.7356e-01],\n",
            "          ...,\n",
            "          [-9.1563e-02,  3.5771e-01, -1.4901e-01,  ...,  2.3589e-01,\n",
            "            2.2954e-01,  1.3772e-02],\n",
            "          [ 6.3703e-01, -2.4568e-01, -1.0122e-01,  ...,  5.0621e-01,\n",
            "            5.0319e-01,  1.4631e-01],\n",
            "          [-5.3881e-01, -1.2697e-01, -6.8118e-02,  ..., -5.0242e-01,\n",
            "            7.9271e-01,  5.4428e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.2875e-03, -1.1720e-02, -2.3990e-03,  ...,  1.1309e-03,\n",
            "            4.1860e-05,  6.5920e-03],\n",
            "          [ 2.1686e-01, -7.0942e-03, -8.2036e-03,  ...,  2.5979e-01,\n",
            "           -2.3983e-02, -1.0103e-01],\n",
            "          [ 2.4961e-01,  4.1739e-01,  1.4759e-01,  ...,  6.9660e-01,\n",
            "           -4.2864e-02,  4.5303e-01],\n",
            "          ...,\n",
            "          [ 7.8034e-01,  4.3535e-01, -2.0896e-01,  ...,  5.4339e-01,\n",
            "            1.5021e-02,  7.9160e-02],\n",
            "          [ 3.2483e-01,  5.4835e-01,  1.9321e-01,  ...,  6.0639e-01,\n",
            "            1.1671e-01, -4.4103e-02],\n",
            "          [ 6.7288e-01,  5.5056e-01,  4.1999e-01,  ...,  9.4574e-01,\n",
            "           -1.8856e-01,  1.6927e-01]],\n",
            "\n",
            "         [[ 1.4914e-02, -2.3045e-03,  1.7913e-03,  ..., -7.2021e-03,\n",
            "           -3.9354e-03, -1.9598e-03],\n",
            "          [ 4.6991e-02,  1.2342e-01, -1.1067e-02,  ..., -2.6829e-01,\n",
            "            2.9739e-01,  3.8116e-01],\n",
            "          [-4.0008e-01,  4.6457e-03,  2.1435e-02,  ...,  1.1341e-01,\n",
            "           -1.2497e-01, -1.0571e-01],\n",
            "          ...,\n",
            "          [-5.5022e-01, -5.4348e-01,  4.6804e-01,  ..., -3.3673e-01,\n",
            "           -2.7704e-01,  6.5003e-02],\n",
            "          [-2.0440e-01, -3.5471e-02, -2.5970e-01,  ...,  3.4467e-02,\n",
            "           -5.9432e-02,  1.4962e-01],\n",
            "          [-3.2233e-02, -3.0880e-01,  8.7599e-02,  ..., -4.9818e-01,\n",
            "           -4.0556e-02,  3.0016e-01]],\n",
            "\n",
            "         [[-2.4938e-03,  5.0879e-03, -6.1542e-03,  ..., -4.5894e-03,\n",
            "           -1.1344e-02, -6.0087e-03],\n",
            "          [ 2.1746e-01, -4.3747e-01,  3.9889e-01,  ...,  8.7723e-02,\n",
            "           -4.3478e-01, -2.5183e-01],\n",
            "          [-1.6383e-01, -1.2113e-01,  3.3007e-01,  ...,  1.8372e-01,\n",
            "           -3.2078e-01,  3.5430e-01],\n",
            "          ...,\n",
            "          [-1.1864e-01, -1.0419e-01,  2.7596e-01,  ...,  1.5565e-01,\n",
            "           -3.4996e-02,  1.3617e-01],\n",
            "          [-1.8253e-02, -7.3717e-01, -6.6537e-02,  ...,  3.3088e-01,\n",
            "           -2.4971e-01, -2.1922e-01],\n",
            "          [ 4.1157e-01, -8.6178e-01, -1.3869e-01,  ...,  4.1353e-01,\n",
            "           -7.1875e-01,  4.0638e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.3712e-02,  6.3962e-03,  3.9831e-02,  ...,  9.8501e-02,\n",
            "            1.3681e-01,  2.4320e-02],\n",
            "          [-3.2806e-01, -5.1669e-01, -1.8840e-01,  ...,  4.2779e-01,\n",
            "            5.1440e-01, -5.4912e-01],\n",
            "          [-7.0785e-01,  2.5501e-01, -8.5172e-01,  ..., -8.6122e-01,\n",
            "            6.3634e-01, -1.4418e+00],\n",
            "          ...,\n",
            "          [-4.4615e-01, -5.0124e-01,  9.8042e-01,  ...,  1.3306e+00,\n",
            "            6.1776e-01, -7.5643e-01],\n",
            "          [-1.1854e+00, -6.0389e-01,  5.3635e-01,  ..., -3.7748e-01,\n",
            "            3.0238e-01, -1.1736e+00],\n",
            "          [-5.5782e-01, -2.7869e-03,  3.0035e-01,  ...,  4.3559e-01,\n",
            "            7.1344e-01, -1.4731e-01]],\n",
            "\n",
            "         [[-1.4992e-02,  7.1673e-03,  8.0713e-03,  ..., -1.4421e-01,\n",
            "           -7.6210e-03, -7.1884e-03],\n",
            "          [-7.4624e-02,  8.3865e-01,  4.1988e-01,  ...,  8.7697e-01,\n",
            "           -7.4244e-04, -4.3991e-01],\n",
            "          [-8.1059e-01,  1.9845e-01, -2.5707e-01,  ...,  6.1327e-01,\n",
            "           -4.1493e-01, -1.6688e+00],\n",
            "          ...,\n",
            "          [-5.8149e-01, -2.1547e-01,  2.7680e-03,  ...,  8.8795e-01,\n",
            "            7.3432e-01, -2.8546e-01],\n",
            "          [-8.0939e-01, -4.2882e-04, -7.7748e-03,  ...,  5.7543e-02,\n",
            "            5.0056e-01, -1.4754e+00],\n",
            "          [ 4.5695e-01,  5.3823e-01,  1.5959e-01,  ...,  9.6956e-01,\n",
            "            4.2014e-01, -9.8998e-01]],\n",
            "\n",
            "         [[ 3.5760e-02, -2.4411e-02, -1.1605e-02,  ..., -1.3445e-01,\n",
            "           -2.8687e-02, -2.2505e-02],\n",
            "          [-1.2896e-01,  3.5382e-01,  5.5275e-01,  ...,  8.1440e-01,\n",
            "            1.3436e+00, -3.3940e-02],\n",
            "          [ 5.4448e-01,  6.0716e-01,  1.2305e+00,  ...,  2.3293e+00,\n",
            "            1.9300e+00,  8.6557e-01],\n",
            "          ...,\n",
            "          [ 1.0199e-01, -3.8744e-01, -6.8658e-01,  ...,  3.9451e-01,\n",
            "            1.1219e+00, -2.7035e-01],\n",
            "          [ 1.4175e-01,  2.4227e-01, -6.5535e-01,  ...,  8.5939e-01,\n",
            "            2.8746e-01,  5.2888e-01],\n",
            "          [ 3.6971e-01,  6.9542e-01,  1.4494e-01,  ...,  1.4969e+00,\n",
            "            5.3568e-01,  7.8110e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2747e-02,  2.4551e-02, -1.5358e-02,  ..., -1.7856e-01,\n",
            "           -1.2757e-01, -6.9817e-02],\n",
            "          [ 1.1608e+00,  8.1511e-01, -2.2588e-02,  ..., -6.0829e-01,\n",
            "            4.0219e-01, -2.5074e-01],\n",
            "          [ 1.2479e+00,  1.6368e+00, -6.7040e-01,  ..., -2.1317e-01,\n",
            "            5.0279e-01, -5.9410e-01],\n",
            "          ...,\n",
            "          [ 6.5676e-01, -7.5216e-01,  8.4957e-01,  ..., -1.4279e+00,\n",
            "            9.4107e-01, -1.2481e+00],\n",
            "          [ 1.2983e+00,  5.9634e-01,  1.1883e+00,  ..., -7.1886e-01,\n",
            "            1.4492e+00, -8.2718e-01],\n",
            "          [ 1.0214e+00,  5.0240e-01, -3.7566e-01,  ..., -6.3321e-01,\n",
            "           -2.3450e-01, -1.3654e+00]],\n",
            "\n",
            "         [[-9.2542e-03,  1.1158e-02,  2.4049e-02,  ..., -8.9729e-01,\n",
            "           -6.6058e-02,  1.5015e-01],\n",
            "          [ 5.1840e-01,  1.5483e-01,  1.2412e-01,  ...,  4.1621e+00,\n",
            "            5.3677e-01, -2.6588e-01],\n",
            "          [-4.0187e-01,  5.8632e-01, -1.2497e+00,  ...,  3.6363e+00,\n",
            "            4.3811e-02, -9.1644e-01],\n",
            "          ...,\n",
            "          [ 1.0976e+00, -6.4578e-01,  1.6321e+00,  ...,  4.0624e+00,\n",
            "           -1.0342e-01, -5.1542e-01],\n",
            "          [ 2.9006e-02, -3.2108e-01,  1.3725e+00,  ...,  3.9522e+00,\n",
            "            1.0125e+00,  4.7248e-01],\n",
            "          [ 7.5734e-01, -3.2697e-01, -7.3623e-03,  ...,  3.5069e+00,\n",
            "            7.3068e-01, -6.9816e-01]],\n",
            "\n",
            "         [[-4.9085e-03, -1.4344e-02, -2.1611e-02,  ..., -1.6672e+00,\n",
            "            7.7040e-02,  8.5547e-02],\n",
            "          [-1.2552e+00,  3.3884e-01,  1.5366e-01,  ...,  4.7998e+00,\n",
            "           -8.9031e-01,  1.6578e+00],\n",
            "          [-9.7517e-01,  1.5920e+00, -8.4841e-01,  ...,  4.2483e+00,\n",
            "            2.3408e-01,  5.5749e-01],\n",
            "          ...,\n",
            "          [ 6.8077e-01, -1.0121e+00,  1.4836e-01,  ...,  3.4725e+00,\n",
            "           -7.2803e-01,  1.9265e+00],\n",
            "          [-7.9827e-01,  3.4296e-01,  5.0484e-01,  ...,  2.9630e+00,\n",
            "           -1.6689e+00,  1.1007e+00],\n",
            "          [-4.8898e-01, -2.8271e-02,  1.0487e-01,  ...,  2.4464e+00,\n",
            "           -1.6625e+00,  8.2948e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-3.0694e-02, -5.5024e-03,  1.2294e-02,  ...,  6.5230e-04,\n",
            "           -9.0190e-03,  4.1996e-02],\n",
            "          [-1.8959e-01,  1.1142e-01,  5.7306e-01,  ...,  4.1185e-01,\n",
            "            3.1767e-01,  1.3265e-01],\n",
            "          [ 5.4004e-01,  1.8213e-02, -4.7435e-02,  ...,  1.5304e-01,\n",
            "           -2.6676e-02,  5.2429e-01],\n",
            "          ...,\n",
            "          [ 5.2186e-01, -2.3668e-01, -1.2036e-01,  ..., -9.5692e-02,\n",
            "            5.5186e-01,  1.1351e+00],\n",
            "          [ 6.7589e-02, -1.1970e-01,  2.7445e-01,  ...,  1.9616e-01,\n",
            "            2.9958e-01,  7.0315e-01],\n",
            "          [-1.6835e-01,  5.2267e-01,  7.1367e-01,  ..., -1.7979e-01,\n",
            "            6.9494e-01,  1.1747e+00]],\n",
            "\n",
            "         [[ 8.4548e-03,  6.1483e-03,  2.3281e-02,  ..., -1.6085e-02,\n",
            "            3.1180e-02,  2.4137e-02],\n",
            "          [ 5.0632e-02, -3.2577e-01, -3.1818e-01,  ...,  3.8913e-01,\n",
            "            3.5255e-01, -8.6117e-02],\n",
            "          [-4.6087e-01, -7.0087e-02, -5.5451e-01,  ...,  4.1360e-01,\n",
            "            6.6975e-01, -2.6829e-01],\n",
            "          ...,\n",
            "          [-4.3960e-01, -3.0042e-01, -2.6738e-01,  ...,  1.8224e-01,\n",
            "            1.4664e-01,  9.6680e-02],\n",
            "          [-5.2183e-01, -2.1989e-01,  7.5273e-02,  ..., -5.7470e-02,\n",
            "            9.8047e-01,  2.0623e-01],\n",
            "          [ 3.6683e-01, -4.3412e-01,  7.3577e-02,  ..., -4.2339e-01,\n",
            "            1.4500e+00, -2.0164e-01]],\n",
            "\n",
            "         [[ 6.2946e-03,  1.2655e-02, -2.6194e-03,  ...,  1.7846e-02,\n",
            "           -6.8556e-03,  8.6266e-03],\n",
            "          [-2.1322e-01, -1.0606e-02, -3.8541e-02,  ..., -1.0040e-01,\n",
            "           -9.4222e-02,  4.8975e-01],\n",
            "          [-1.9090e-01, -3.2906e-01, -2.1635e-01,  ...,  2.5385e-02,\n",
            "           -2.5805e-01,  3.8346e-02],\n",
            "          ...,\n",
            "          [-2.3451e-01,  3.6560e-01,  3.5591e-01,  ...,  1.9996e-01,\n",
            "           -1.2329e-01,  8.0472e-01],\n",
            "          [ 1.9207e-01, -1.9550e-01,  5.3453e-03,  ..., -4.4093e-01,\n",
            "           -1.2570e-01,  1.9689e-01],\n",
            "          [ 3.1966e-01, -1.4650e-01, -3.8269e-01,  ..., -5.7306e-01,\n",
            "           -6.7676e-01, -3.1282e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.9787e-03,  1.6046e-03,  1.7420e-04,  ...,  2.8502e-03,\n",
            "            4.3261e-04, -2.7075e-03],\n",
            "          [-3.4022e-01,  8.5286e-02,  5.2589e-01,  ...,  4.9244e-01,\n",
            "           -1.6567e-02,  5.2518e-01],\n",
            "          [-5.1623e-01,  7.9285e-02,  3.7049e-01,  ...,  4.2789e-01,\n",
            "           -2.6073e-01,  3.2081e-01],\n",
            "          ...,\n",
            "          [-2.6302e-01, -3.7325e-01,  4.7765e-01,  ...,  2.6821e-01,\n",
            "            5.7328e-02,  1.6013e-02],\n",
            "          [-9.7605e-01, -2.0596e-01,  5.4610e-01,  ...,  1.0391e+00,\n",
            "            2.3220e-01,  6.3503e-01],\n",
            "          [-3.2407e-01,  1.3010e-01,  5.0366e-01,  ...,  3.2481e-01,\n",
            "            6.9003e-02,  4.0590e-01]],\n",
            "\n",
            "         [[-1.9443e-03,  1.0471e-03,  4.1304e-03,  ...,  3.7329e-03,\n",
            "           -6.1531e-03, -1.2087e-03],\n",
            "          [-4.4771e-02, -3.6702e-01,  3.4112e-02,  ...,  5.6361e-01,\n",
            "           -1.6558e-01,  8.6195e-03],\n",
            "          [ 3.4776e-01,  1.1149e-01,  9.7292e-01,  ...,  4.0878e-01,\n",
            "           -3.9745e-01, -1.9457e-01],\n",
            "          ...,\n",
            "          [ 5.0880e-02,  3.0158e-01,  1.2424e+00,  ...,  4.3334e-01,\n",
            "            3.9933e-02,  6.4123e-01],\n",
            "          [ 3.0706e-01,  1.7015e-01,  1.2050e+00,  ...,  3.3027e-01,\n",
            "            3.4801e-01,  5.2547e-01],\n",
            "          [ 3.4061e-01, -9.2765e-02,  1.9634e-02,  ...,  1.0619e-02,\n",
            "           -1.5143e-01,  7.1151e-02]],\n",
            "\n",
            "         [[ 8.2526e-03, -2.0742e-03,  1.1454e-03,  ...,  1.1768e-03,\n",
            "            3.7740e-03, -1.9941e-04],\n",
            "          [-4.7157e-01,  5.2252e-01,  8.2479e-02,  ...,  6.6984e-01,\n",
            "           -2.7807e-01,  1.8084e-01],\n",
            "          [-2.9533e-01, -8.2919e-02,  9.3111e-02,  ..., -8.1922e-03,\n",
            "           -2.3801e-01, -2.4437e-01],\n",
            "          ...,\n",
            "          [-4.1243e-01,  9.1384e-03,  4.0189e-01,  ...,  3.8964e-01,\n",
            "            1.5778e-01, -2.8364e-01],\n",
            "          [-1.4278e-01,  1.6328e-01,  2.0558e-01,  ...,  4.6849e-01,\n",
            "            1.5365e-01,  6.5671e-02],\n",
            "          [-6.6110e-01,  4.2183e-01,  2.3020e-02,  ...,  4.0293e-01,\n",
            "           -6.9042e-02,  2.4275e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.0898e-02,  1.0892e-02,  1.9254e-02,  ..., -1.8029e-01,\n",
            "            3.6622e-01,  5.3543e-01],\n",
            "          [-7.6268e-01,  2.9204e-01, -2.8274e-01,  ...,  7.0150e-01,\n",
            "            5.7053e-01, -2.5052e-02],\n",
            "          [-1.4772e+00,  3.4707e-01, -6.5584e-01,  ...,  1.6263e+00,\n",
            "           -8.0567e-01, -4.5892e-01],\n",
            "          ...,\n",
            "          [ 1.0803e+00, -1.7602e-01,  8.5565e-01,  ...,  1.3175e+00,\n",
            "           -6.5460e-01, -2.8217e-01],\n",
            "          [-6.6423e-02,  5.8030e-01,  5.6513e-01,  ...,  2.6606e-01,\n",
            "           -5.8026e-01, -6.4790e-01],\n",
            "          [-1.1263e+00,  6.0347e-01,  2.0341e-01,  ...,  2.5843e-01,\n",
            "            2.9942e-01,  1.9492e-01]],\n",
            "\n",
            "         [[ 2.1381e-02, -4.7694e-03,  2.5621e-02,  ..., -1.4699e-01,\n",
            "           -7.8745e-02,  5.4420e-01],\n",
            "          [ 8.4688e-01, -1.8825e-01, -1.0269e+00,  ...,  5.7180e-01,\n",
            "           -3.3937e-01, -5.9344e-01],\n",
            "          [-1.2730e+00,  1.3202e+00, -1.8888e-01,  ..., -8.5771e-01,\n",
            "           -3.1538e-01, -8.5634e-03],\n",
            "          ...,\n",
            "          [ 1.2203e+00, -9.8379e-01, -1.3392e-01,  ...,  1.6246e-01,\n",
            "            1.0325e+00,  9.4856e-01],\n",
            "          [ 8.6404e-01, -4.0378e-02, -1.3176e+00,  ...,  9.3692e-01,\n",
            "           -7.1800e-01,  5.0808e-01],\n",
            "          [ 2.1264e-01, -2.1794e-01, -3.2967e-01,  ...,  6.7972e-01,\n",
            "            6.4931e-01,  5.1861e-01]],\n",
            "\n",
            "         [[-1.2429e-03, -5.5645e-03, -1.1209e-02,  ..., -1.3494e-01,\n",
            "            3.8777e-01,  2.3685e-01],\n",
            "          [ 1.4614e+00, -3.1689e-01,  1.6273e-01,  ...,  5.1834e-01,\n",
            "            3.2208e-01,  9.0476e-01],\n",
            "          [ 1.4045e+00, -2.0973e+00, -5.0311e-01,  ...,  4.2024e-01,\n",
            "            1.2230e+00,  3.2292e-01],\n",
            "          ...,\n",
            "          [ 1.1055e-01,  8.0730e-01, -3.6028e-01,  ...,  1.2632e+00,\n",
            "            3.8318e-01,  9.3646e-01],\n",
            "          [ 1.1479e+00,  1.4831e-01,  4.3278e-01,  ...,  1.7646e-01,\n",
            "           -1.5384e-01,  9.4806e-01],\n",
            "          [ 8.3290e-01, -9.4015e-01,  3.4197e-01,  ..., -5.0515e-03,\n",
            "           -8.3885e-01,  1.4147e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4551e-02,  2.2414e-02, -8.7858e-03,  ...,  1.5388e-01,\n",
            "           -2.7886e-01, -1.0466e-01],\n",
            "          [-1.6127e+00, -1.2147e-01, -6.0288e-01,  ..., -8.4119e-02,\n",
            "            6.0270e-01, -2.2638e-01],\n",
            "          [-6.3798e-02,  4.0871e-01, -3.0279e-02,  ...,  1.0649e+00,\n",
            "            1.1158e+00, -5.4053e-01],\n",
            "          ...,\n",
            "          [-8.1271e-01, -4.4145e-01,  5.7888e-03,  ...,  5.6526e-01,\n",
            "            1.5982e+00,  4.2843e-01],\n",
            "          [-1.8107e+00, -8.7142e-01,  8.9642e-02,  ..., -2.7839e-01,\n",
            "            9.0642e-01, -4.7501e-01],\n",
            "          [-4.3449e-01,  1.4850e-01, -5.8816e-01,  ...,  9.9397e-01,\n",
            "            9.3607e-01,  5.8330e-01]],\n",
            "\n",
            "         [[-9.4779e-04,  3.5858e-02,  3.3796e-03,  ...,  2.4631e-02,\n",
            "           -1.2239e-01, -3.8193e-01],\n",
            "          [-9.6787e-01, -3.5284e-01, -1.1933e+00,  ...,  3.9181e-01,\n",
            "           -4.0309e-01,  3.5320e-01],\n",
            "          [-8.2248e-01,  8.5921e-01, -6.2346e-01,  ...,  7.5930e-01,\n",
            "           -7.8522e-01,  2.0609e+00],\n",
            "          ...,\n",
            "          [-6.7825e-01, -9.1025e-03,  4.5671e-01,  ...,  6.3552e-01,\n",
            "            1.3350e+00,  1.4987e+00],\n",
            "          [-5.8841e-01, -3.9798e-01,  5.7042e-02,  ..., -3.1151e-01,\n",
            "           -7.5774e-02,  1.9435e+00],\n",
            "          [-8.6904e-03, -5.4460e-01, -2.4882e-01,  ...,  1.1915e+00,\n",
            "            7.8818e-01,  1.7758e+00]],\n",
            "\n",
            "         [[-2.5136e-02,  1.3225e-02,  1.6684e-02,  ...,  1.6344e+00,\n",
            "           -1.4194e-01, -7.6480e-03],\n",
            "          [-1.0994e-02,  2.4254e-01,  7.2161e-02,  ..., -3.2289e+00,\n",
            "           -3.0188e-02,  2.0562e-01],\n",
            "          [-6.4739e-01,  5.0210e-03, -1.5145e-01,  ..., -3.5824e+00,\n",
            "           -4.5405e-01, -9.4931e-01],\n",
            "          ...,\n",
            "          [ 1.1547e+00,  3.9163e-02,  1.8129e-01,  ..., -4.4536e+00,\n",
            "           -1.2100e+00, -4.7062e-01],\n",
            "          [ 6.1157e-01,  1.3161e-01,  6.1517e-01,  ..., -2.8725e+00,\n",
            "            2.1724e-01,  4.2247e-01],\n",
            "          [ 1.5726e-01,  8.2150e-01, -3.0270e-01,  ..., -3.1723e+00,\n",
            "            1.3158e-01,  1.7638e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-3.1788e-03, -6.7357e-03,  1.6187e-03,  ..., -3.2207e-03,\n",
            "            2.9202e-03,  1.4117e-02],\n",
            "          [ 7.1546e-02,  7.1313e-02,  7.3013e-01,  ...,  4.9729e-02,\n",
            "            3.0730e-01,  2.7857e-01],\n",
            "          [ 3.2377e-01,  8.6780e-02,  2.2583e-01,  ...,  2.5271e-01,\n",
            "            2.0725e-01,  1.2416e-01],\n",
            "          ...,\n",
            "          [-1.8635e-01,  6.2882e-02, -8.0958e-02,  ...,  6.5703e-01,\n",
            "            3.0919e-01, -4.6380e-01],\n",
            "          [-1.8284e-01,  3.6297e-02, -5.8424e-01,  ..., -1.0650e-01,\n",
            "            2.6384e-01, -3.6077e-01],\n",
            "          [ 1.0745e-01,  7.0132e-01, -4.5491e-01,  ...,  5.2690e-01,\n",
            "            5.1856e-01, -1.2831e+00]],\n",
            "\n",
            "         [[-1.3665e-02,  2.1990e-02,  9.2299e-03,  ..., -1.1950e-01,\n",
            "            1.0599e-02, -2.2113e-02],\n",
            "          [-1.1743e-02, -2.9349e-01, -2.7126e-02,  ...,  2.1597e-01,\n",
            "            3.5306e-01, -1.9895e-02],\n",
            "          [-7.6361e-03, -4.6281e-01, -1.5272e-01,  ..., -2.6467e-01,\n",
            "            3.4846e-02,  2.9677e-01],\n",
            "          ...,\n",
            "          [ 1.9487e-01, -3.9198e-01, -1.4958e-01,  ...,  4.3874e-02,\n",
            "            1.1840e-01, -7.4293e-02],\n",
            "          [-3.8136e-01, -4.8478e-01, -4.1745e-01,  ..., -1.3043e-01,\n",
            "           -3.4871e-02, -1.1281e-01],\n",
            "          [-3.6397e-01, -2.2667e-01, -1.9478e-01,  ...,  3.9157e-01,\n",
            "            1.0253e-01, -9.7101e-02]],\n",
            "\n",
            "         [[-1.3912e-03, -1.1298e-02,  9.9624e-03,  ...,  1.5390e-02,\n",
            "            1.7103e-02, -7.5028e-03],\n",
            "          [ 5.7790e-02, -7.9216e-02, -4.2364e-02,  ..., -4.3059e-01,\n",
            "           -7.2177e-02,  2.0523e-01],\n",
            "          [ 2.6620e-01,  7.2548e-02,  5.3361e-02,  ..., -3.1081e-01,\n",
            "           -2.8803e-02,  4.9351e-01],\n",
            "          ...,\n",
            "          [ 1.6806e-01,  2.7799e-01,  3.4416e-01,  ..., -2.0573e-01,\n",
            "            4.5413e-01,  3.6271e-01],\n",
            "          [-1.9951e-01,  7.2613e-02,  4.1470e-01,  ..., -3.6206e-01,\n",
            "            2.1488e-01, -1.2542e-01],\n",
            "          [-1.7147e-01,  2.1847e-02, -2.8524e-02,  ..., -1.8065e-01,\n",
            "            3.6292e-01,  9.6031e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.9147e-03, -2.0449e-02,  2.1829e-02,  ...,  1.4971e-02,\n",
            "            7.6416e-04,  7.3926e-03],\n",
            "          [-3.6465e-01, -4.3707e-01, -3.4082e-01,  ..., -2.2537e-01,\n",
            "           -3.8187e-01, -1.9464e-01],\n",
            "          [-1.1396e-01, -7.1246e-01,  1.4684e-01,  ..., -4.1707e-03,\n",
            "           -1.1954e-01,  4.2448e-01],\n",
            "          ...,\n",
            "          [ 8.5835e-02, -3.2482e-01, -8.6008e-02,  ...,  9.6484e-02,\n",
            "           -1.6743e-01,  2.4313e-01],\n",
            "          [ 3.9130e-02, -3.2543e-01, -3.9496e-01,  ..., -1.3164e-01,\n",
            "           -1.2712e-01,  1.5666e-01],\n",
            "          [-5.7639e-01, -8.2661e-01, -2.1103e-01,  ...,  5.3645e-02,\n",
            "           -2.3425e-01,  3.4803e-01]],\n",
            "\n",
            "         [[-7.1194e-03,  5.7774e-03,  3.4095e-03,  ...,  5.6463e-03,\n",
            "           -1.3333e-03,  5.6234e-04],\n",
            "          [-8.1151e-02,  1.2529e-01,  1.9377e-01,  ..., -2.8186e-01,\n",
            "            3.3723e-01,  7.6943e-02],\n",
            "          [-3.8147e-01,  8.2071e-02,  1.3065e-01,  ..., -1.8394e-01,\n",
            "           -5.2506e-02,  2.1697e-01],\n",
            "          ...,\n",
            "          [-9.1174e-02, -5.4266e-02,  2.0364e-01,  ..., -2.7862e-01,\n",
            "            4.1111e-02,  4.1357e-02],\n",
            "          [ 1.7101e-02, -1.9370e-01, -1.7244e-01,  ..., -1.8442e-01,\n",
            "            4.5011e-01, -3.8381e-02],\n",
            "          [ 1.0531e-02,  2.9972e-01,  3.9014e-01,  ..., -2.4235e-01,\n",
            "            1.9785e-01, -3.6773e-01]],\n",
            "\n",
            "         [[-5.9812e-03,  6.4339e-04, -6.1203e-03,  ..., -7.3922e-03,\n",
            "            4.1783e-05,  7.9801e-03],\n",
            "          [ 2.9863e-01,  7.3649e-03, -1.7474e-01,  ...,  3.1286e-02,\n",
            "            4.1813e-02,  4.5549e-02],\n",
            "          [-6.3075e-01, -2.3681e-01,  9.7057e-02,  ..., -1.0524e-02,\n",
            "            1.1866e-01, -2.9602e-01],\n",
            "          ...,\n",
            "          [-3.6849e-01, -2.9258e-02, -3.0713e-01,  ...,  6.9104e-02,\n",
            "           -2.2964e-01, -2.8448e-01],\n",
            "          [ 2.2570e-01,  1.7828e-02, -5.9373e-01,  ..., -1.6727e-01,\n",
            "           -2.3015e-01, -2.5481e-01],\n",
            "          [-7.1660e-02,  1.7915e-01, -2.3684e-01,  ...,  3.6473e-02,\n",
            "            1.0635e-01, -1.9925e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.7628e-03, -7.5501e-03,  7.1633e-03,  ..., -1.2616e-01,\n",
            "            5.4014e-01,  1.6547e+00],\n",
            "          [ 3.1079e-01,  2.8035e-01,  4.6475e-01,  ...,  2.1991e+00,\n",
            "           -2.6911e+00, -5.1230e+00],\n",
            "          [-1.3448e+00,  6.7334e-01,  5.2120e-01,  ...,  2.2380e+00,\n",
            "           -1.3312e+00, -5.2875e+00],\n",
            "          ...,\n",
            "          [ 1.6262e+00, -1.3318e+00, -4.1598e-01,  ...,  1.5038e+00,\n",
            "           -2.3526e+00, -6.7736e+00],\n",
            "          [ 1.1986e+00, -1.0839e+00, -7.7263e-01,  ...,  5.8511e-01,\n",
            "           -1.4778e+00, -5.9043e+00],\n",
            "          [ 1.8926e-01,  2.7262e-02,  5.4191e-02,  ...,  1.9066e+00,\n",
            "           -1.2549e+00, -5.1931e+00]],\n",
            "\n",
            "         [[-2.8591e-03,  7.8643e-03, -2.7720e-02,  ..., -3.2680e-01,\n",
            "            1.5193e-01,  3.3660e-01],\n",
            "          [ 1.2980e+00,  7.7203e-01,  6.9200e-02,  ..., -5.6799e-01,\n",
            "            1.0201e-01, -1.8024e+00],\n",
            "          [-9.2221e-01,  1.3934e+00,  4.3232e-01,  ...,  1.1665e+00,\n",
            "           -2.0229e+00, -5.5317e-01],\n",
            "          ...,\n",
            "          [ 1.9003e+00, -8.7448e-01, -9.5309e-01,  ...,  7.6969e-01,\n",
            "           -4.6714e-01,  2.7480e-02],\n",
            "          [ 1.5066e+00, -1.2290e-01, -4.4599e-01,  ...,  1.1687e+00,\n",
            "           -2.9232e-01, -4.9849e-02],\n",
            "          [ 4.7905e-01,  6.3195e-01, -1.3295e-01,  ...,  2.4804e-01,\n",
            "           -1.3631e+00, -1.2545e+00]],\n",
            "\n",
            "         [[ 6.0288e-03,  1.9848e-02, -1.1167e-02,  ..., -9.0246e-01,\n",
            "            3.7283e-01,  3.9344e-01],\n",
            "          [ 6.7261e-01,  5.9276e-01, -6.5288e-01,  ...,  2.3963e+00,\n",
            "            1.1751e+00, -9.3174e-01],\n",
            "          [-1.8459e+00,  3.6711e-01, -4.3873e-01,  ...,  1.3727e+00,\n",
            "           -5.5448e-01,  3.1370e-01],\n",
            "          ...,\n",
            "          [ 1.8965e+00, -2.6689e-01,  3.6438e-01,  ...,  1.4757e+00,\n",
            "           -2.4729e-01, -1.8203e+00],\n",
            "          [ 1.4654e+00, -7.0404e-02,  1.9022e-01,  ...,  1.5093e+00,\n",
            "           -2.2944e+00, -4.5850e-01],\n",
            "          [ 3.4467e-01, -5.7362e-01,  4.9863e-01,  ...,  3.6952e-02,\n",
            "           -2.7921e+00, -4.0147e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8094e-03,  1.1835e-02, -2.2055e-03,  ..., -4.3424e-02,\n",
            "            1.4579e-01, -1.2316e+00],\n",
            "          [-1.0288e+00,  1.5363e-02, -9.3967e-01,  ..., -1.9829e-01,\n",
            "            1.6011e+00,  2.2709e+00],\n",
            "          [ 6.7184e-03, -4.5968e-01, -3.8789e-01,  ...,  1.3377e+00,\n",
            "            6.2179e-02,  2.5480e+00],\n",
            "          ...,\n",
            "          [-4.0442e-01,  3.7560e-01,  3.0883e-01,  ...,  8.6692e-01,\n",
            "            6.4360e-01,  3.3736e+00],\n",
            "          [-6.9624e-01,  1.3907e+00,  2.6153e-01,  ...,  1.5907e+00,\n",
            "           -7.4273e-01,  3.5980e+00],\n",
            "          [-6.3481e-01, -3.6627e-01, -1.6046e-01,  ...,  3.1716e-01,\n",
            "            8.3817e-01,  2.5955e+00]],\n",
            "\n",
            "         [[-2.1686e-02, -6.1970e-04,  5.9319e-03,  ...,  3.7146e-01,\n",
            "            4.2755e-01, -2.0388e-01],\n",
            "          [ 1.0974e+00, -4.3957e-01, -1.2640e-01,  ...,  4.6039e-02,\n",
            "            7.1707e-01,  4.7400e-02],\n",
            "          [ 3.6752e-01,  2.0284e-01,  1.6429e-01,  ..., -4.6160e-01,\n",
            "           -9.9209e-02, -1.7210e-01],\n",
            "          ...,\n",
            "          [ 1.0089e+00,  5.3157e-01,  2.2874e-02,  ...,  1.9150e+00,\n",
            "            3.2069e+00, -1.0549e+00],\n",
            "          [ 1.2721e+00, -9.1325e-02, -4.8687e-01,  ...,  1.0612e+00,\n",
            "            1.3402e+00, -1.0702e+00],\n",
            "          [ 2.0682e-01,  1.0617e+00, -8.8225e-01,  ...,  5.1737e-01,\n",
            "            9.9490e-01, -9.8415e-01]],\n",
            "\n",
            "         [[-1.0309e-02, -2.0791e-02, -7.7958e-03,  ..., -7.9739e-01,\n",
            "           -4.9237e-01,  1.0989e-01],\n",
            "          [-2.8500e+00, -1.1597e+00,  6.4874e-01,  ...,  9.5498e-01,\n",
            "            1.0219e+00, -1.0309e+00],\n",
            "          [-3.3116e+00,  1.2701e+00,  5.0156e-01,  ...,  1.6767e+00,\n",
            "           -4.6742e-02, -7.0853e-01],\n",
            "          ...,\n",
            "          [-1.1832e+00, -1.7388e+00, -1.8970e-01,  ...,  7.7495e-01,\n",
            "           -6.5739e-01,  7.0342e-02],\n",
            "          [-4.0319e+00, -2.2001e+00,  7.0219e-01,  ...,  1.5765e-01,\n",
            "            6.7171e-01, -3.6279e-01],\n",
            "          [-1.8922e+00, -1.3237e-01,  9.6670e-01,  ...,  1.0624e+00,\n",
            "            3.1599e-01, -1.3885e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 5.7760e-03, -2.6718e-03,  1.4348e-03,  ...,  7.2839e-03,\n",
            "           -7.1221e-03,  6.9679e-03],\n",
            "          [-3.8934e-01,  4.3936e-01,  2.7154e-01,  ...,  2.0031e-01,\n",
            "           -1.6616e-02,  4.4014e-01],\n",
            "          [-5.7052e-01, -1.6423e-01, -8.0121e-02,  ...,  4.5106e-01,\n",
            "           -3.1904e-01,  3.0541e-01],\n",
            "          ...,\n",
            "          [-1.7012e-01, -1.7487e-01, -2.4353e-01,  ...,  5.8064e-01,\n",
            "           -3.9089e-01,  4.4352e-01],\n",
            "          [ 6.3156e-02,  3.4417e-01,  3.2260e-02,  ...,  6.2693e-01,\n",
            "           -2.2009e-01,  5.9764e-02],\n",
            "          [ 1.4479e-01,  1.8819e-01,  1.8856e-01,  ..., -1.7106e-01,\n",
            "            3.9689e-01,  6.6531e-01]],\n",
            "\n",
            "         [[ 2.7583e-02,  8.9270e-02,  2.0226e-02,  ...,  5.9042e-02,\n",
            "           -9.9826e-02, -3.4621e-02],\n",
            "          [-1.8364e-01, -1.5005e-02,  1.4249e-01,  ...,  3.2704e-01,\n",
            "           -1.0527e-01,  1.6884e-01],\n",
            "          [-1.2437e-01,  5.1419e-01, -1.4676e-01,  ...,  2.6011e-01,\n",
            "            3.7843e-02,  2.3731e-01],\n",
            "          ...,\n",
            "          [-1.4982e-01,  8.4059e-01, -2.1945e-01,  ...,  2.5906e-01,\n",
            "            8.2622e-02, -8.7390e-02],\n",
            "          [-8.2583e-02,  6.9318e-01,  3.1819e-01,  ...,  2.4309e-01,\n",
            "           -2.7883e-01,  4.7625e-01],\n",
            "          [-2.1896e-01, -4.3126e-01,  1.5479e-01,  ..., -4.8258e-01,\n",
            "            9.4117e-02, -3.9825e-02]],\n",
            "\n",
            "         [[-7.1221e-03,  8.8689e-03, -2.1105e-02,  ...,  2.5256e-03,\n",
            "           -4.4492e-03,  1.3653e-03],\n",
            "          [-9.1039e-02, -3.7319e-01,  9.1135e-02,  ..., -2.4075e-01,\n",
            "           -6.3180e-02,  2.3328e-01],\n",
            "          [ 1.4440e-01, -2.6785e-01,  1.4880e-01,  ..., -7.6934e-02,\n",
            "           -2.8225e-01,  1.9557e-02],\n",
            "          ...,\n",
            "          [ 2.8248e-01, -3.6003e-01, -1.7437e-01,  ..., -3.0270e-01,\n",
            "            2.4528e-01,  8.4931e-02],\n",
            "          [-7.0173e-02, -6.7864e-01,  1.6602e-01,  ..., -1.2721e-02,\n",
            "           -2.0778e-01, -2.5518e-02],\n",
            "          [-1.1955e-01, -3.7199e-01, -1.3887e-01,  ..., -3.3279e-01,\n",
            "           -5.2399e-01,  1.9755e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7068e-02,  2.3157e-02,  6.7672e-03,  ...,  1.7603e-02,\n",
            "           -2.4459e-03,  3.0162e-02],\n",
            "          [ 1.3831e-01, -2.1351e-01, -7.1432e-02,  ..., -9.0275e-03,\n",
            "           -6.4895e-01, -1.8425e-01],\n",
            "          [ 6.7746e-02, -4.0234e-02, -7.6658e-01,  ..., -2.0782e-02,\n",
            "           -3.7551e-01, -4.0634e-01],\n",
            "          ...,\n",
            "          [ 1.6724e-02,  4.1103e-01, -5.4270e-01,  ...,  2.3153e-02,\n",
            "           -1.2672e-01,  1.3101e-02],\n",
            "          [-1.2333e-01,  1.6566e-01, -3.5793e-02,  ...,  6.9914e-04,\n",
            "            3.2531e-01, -2.5283e-01],\n",
            "          [-3.3660e-01,  6.2521e-01, -4.1550e-01,  ...,  9.5696e-02,\n",
            "            1.4307e-01, -6.0658e-01]],\n",
            "\n",
            "         [[-1.0127e-02,  2.1136e-02,  2.6735e-03,  ..., -2.1111e-02,\n",
            "           -1.9116e-03,  1.8420e-03],\n",
            "          [-1.4601e-01, -1.5082e-02,  5.4615e-02,  ..., -4.8155e-02,\n",
            "            1.4257e-01, -1.8195e-01],\n",
            "          [ 2.1931e-01, -7.2252e-02, -1.9374e-02,  ...,  1.3619e-01,\n",
            "            1.4534e-01,  6.5752e-02],\n",
            "          ...,\n",
            "          [-2.7632e-01, -1.4304e-01,  1.0186e-01,  ..., -1.2478e-01,\n",
            "           -7.2540e-02,  6.3476e-03],\n",
            "          [ 1.5088e-01, -2.3543e-02, -1.4521e-01,  ..., -2.0691e-01,\n",
            "            3.5026e-02,  2.7795e-02],\n",
            "          [ 1.9392e-01,  1.9189e-01, -2.3497e-01,  ..., -1.8058e-01,\n",
            "            2.1653e-01,  2.2090e-01]],\n",
            "\n",
            "         [[ 5.1799e-03,  4.0532e-03, -1.2339e-02,  ...,  7.0735e-05,\n",
            "           -1.0514e-02,  8.4519e-03],\n",
            "          [ 3.0469e-01,  1.5003e-01,  1.1522e-03,  ...,  2.5428e-01,\n",
            "            1.1669e-01, -1.0303e-01],\n",
            "          [ 2.0087e-01, -9.3380e-03, -2.0737e-02,  ...,  1.0655e-01,\n",
            "           -3.8550e-02,  9.8316e-02],\n",
            "          ...,\n",
            "          [ 3.1291e-01, -4.2091e-01, -3.7560e-01,  ...,  1.2975e-01,\n",
            "           -2.4758e-02, -2.5718e-01],\n",
            "          [ 4.0780e-01, -6.5883e-01, -2.9367e-01,  ...,  5.0270e-01,\n",
            "            5.0704e-01, -5.2198e-01],\n",
            "          [ 5.5641e-01, -1.1416e-01, -2.7574e-01,  ...,  4.4112e-01,\n",
            "            3.1990e-01, -4.4545e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-8.3598e-03, -5.7379e-03,  4.4545e-03,  ...,  1.1904e+00,\n",
            "            3.8591e-02,  3.9809e-01],\n",
            "          [ 4.0372e-01, -7.0508e-02, -7.4807e-01,  ..., -1.7621e+00,\n",
            "           -1.0914e+00, -5.0752e-01],\n",
            "          [ 1.1469e-01, -1.1145e+00, -7.3567e-01,  ..., -1.2209e+00,\n",
            "           -6.1732e-01,  3.5338e-01],\n",
            "          ...,\n",
            "          [ 9.6336e-01,  1.1072e+00,  8.2362e-01,  ..., -1.8329e+00,\n",
            "           -1.8419e-01, -2.6248e-01],\n",
            "          [ 2.7036e-01,  1.2781e+00,  4.8247e-01,  ..., -1.9795e+00,\n",
            "           -1.3902e+00, -8.3002e-01],\n",
            "          [-3.7730e-01,  2.3951e-01,  1.2725e-01,  ..., -9.3321e-01,\n",
            "           -1.1118e+00,  2.2881e-01]],\n",
            "\n",
            "         [[-6.8641e-03, -3.1575e-02, -2.8397e-03,  ..., -2.1477e-01,\n",
            "           -1.6266e-01, -1.5868e-01],\n",
            "          [-2.7053e+00,  4.6232e-01, -5.6877e-01,  ...,  5.1976e-01,\n",
            "            8.0020e-01,  1.0584e-01],\n",
            "          [-4.2480e+00,  3.6583e-01, -1.3462e+00,  ...,  3.5030e-02,\n",
            "            3.5066e-01,  4.3716e-01],\n",
            "          ...,\n",
            "          [ 1.4377e+00, -3.3846e-01,  1.6678e+00,  ...,  1.0696e+00,\n",
            "            3.7143e-01,  3.3467e-02],\n",
            "          [-1.8714e+00,  3.5586e-01,  7.8224e-01,  ...,  2.3407e-01,\n",
            "            6.1986e-01,  2.8803e-01],\n",
            "          [-3.1767e+00,  7.5114e-01,  2.7484e-01,  ...,  1.3334e-01,\n",
            "            8.3803e-01,  6.9361e-02]],\n",
            "\n",
            "         [[-1.1296e-02, -1.0093e-02, -7.9696e-03,  ...,  2.9901e-01,\n",
            "            9.2048e-02, -6.1139e-01],\n",
            "          [ 8.2252e-01,  3.8965e-01, -9.1790e-02,  ..., -3.6438e-01,\n",
            "            7.2324e-02,  1.9365e-01],\n",
            "          [ 1.1249e+00, -8.8292e-01,  1.0485e+00,  ..., -8.5474e-01,\n",
            "            1.8925e-01,  9.2095e-01],\n",
            "          ...,\n",
            "          [-9.4099e-02,  1.1840e+00,  2.6226e-01,  ..., -4.1562e-01,\n",
            "            3.2413e-01, -1.0770e-01],\n",
            "          [ 1.3058e+00,  1.0778e+00, -1.7056e-01,  ..., -2.1019e+00,\n",
            "            7.1261e-01,  1.7219e+00],\n",
            "          [ 9.9811e-01,  4.9758e-01,  8.9407e-02,  ..., -7.5677e-01,\n",
            "            9.9156e-01,  9.0916e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.3543e-03,  8.3387e-03,  3.4790e-02,  ...,  2.4902e-01,\n",
            "            2.2795e+00,  7.9670e-02],\n",
            "          [-2.7205e-01, -2.2141e-01,  3.3332e-01,  ..., -8.9316e-01,\n",
            "           -3.1992e+00, -5.5256e-01],\n",
            "          [-1.3723e-01, -3.4586e-01, -1.0536e-01,  ..., -1.2620e+00,\n",
            "           -4.5364e+00,  1.0813e+00],\n",
            "          ...,\n",
            "          [-1.8332e-01, -3.5753e-01,  2.3219e-01,  ..., -1.4068e+00,\n",
            "           -5.3240e+00, -2.8213e+00],\n",
            "          [ 1.2335e-01, -3.7061e-01, -5.2474e-01,  ..., -7.7920e-01,\n",
            "           -4.8235e+00, -2.1960e-01],\n",
            "          [-8.5607e-01,  2.3342e-01, -5.7938e-01,  ..., -1.2446e+00,\n",
            "           -3.8094e+00,  3.2048e-01]],\n",
            "\n",
            "         [[ 3.5834e-02,  1.5922e-03,  7.5546e-03,  ..., -9.6693e-01,\n",
            "            1.1015e+00, -9.1504e-01],\n",
            "          [ 1.2061e+00, -2.4469e-01, -2.2721e-01,  ...,  1.4885e+00,\n",
            "            1.1000e+00, -1.3626e+00],\n",
            "          [ 1.4257e-01, -5.6719e-01,  9.8515e-01,  ...,  1.3522e+00,\n",
            "           -2.5325e-01, -4.4546e-01],\n",
            "          ...,\n",
            "          [ 1.3442e+00,  8.6674e-01, -2.7515e-01,  ...,  1.2352e+00,\n",
            "           -2.2691e+00, -9.1730e-01],\n",
            "          [ 1.7194e+00,  1.3297e-02, -4.1623e-01,  ...,  2.2610e+00,\n",
            "           -1.2789e+00, -1.0914e+00],\n",
            "          [ 7.9686e-01,  7.0094e-02, -8.3030e-01,  ...,  7.9592e-01,\n",
            "            3.9773e-01, -2.1408e+00]],\n",
            "\n",
            "         [[ 3.7681e-02, -4.8553e-02, -1.8283e-02,  ..., -8.8707e-02,\n",
            "           -2.0481e-01, -4.2319e-02],\n",
            "          [-8.3099e-01,  7.6322e-02,  4.1783e-01,  ...,  2.6600e-01,\n",
            "            9.3571e-01,  7.1713e-01],\n",
            "          [ 7.3859e-01,  3.2102e-01,  1.0369e+00,  ...,  4.7292e-01,\n",
            "            7.8672e-02, -4.1115e-02],\n",
            "          ...,\n",
            "          [-1.4401e+00, -2.6084e-01, -6.7850e-01,  ...,  7.9207e-02,\n",
            "           -7.8048e-02,  2.3995e-01],\n",
            "          [-1.2858e+00, -4.0556e-01,  1.8025e-01,  ..., -2.8170e-01,\n",
            "           -4.4843e-01,  1.2021e-01],\n",
            "          [-2.0089e-01,  4.7159e-01,  1.2083e-01,  ..., -8.2940e-01,\n",
            "            1.1633e+00,  3.6406e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.9636e-02,  1.7947e-02, -1.0026e-02,  ...,  2.6339e-03,\n",
            "            2.8136e-02, -5.1966e-03],\n",
            "          [ 9.8712e-01,  5.0922e-02,  2.7708e-01,  ...,  2.7088e-01,\n",
            "            2.1937e-01, -4.5104e-02],\n",
            "          [ 2.9997e-01, -1.0564e-02,  9.9933e-01,  ...,  4.1546e-01,\n",
            "           -2.5030e-01, -7.7346e-01],\n",
            "          ...,\n",
            "          [ 6.1274e-01,  1.5567e-01,  3.9191e-01,  ..., -8.3972e-02,\n",
            "           -2.2328e-01, -6.8995e-01],\n",
            "          [-1.2229e-03,  6.3611e-01,  1.2623e-01,  ...,  9.8613e-02,\n",
            "           -3.1711e-01, -6.9915e-02],\n",
            "          [ 6.5352e-01,  6.8199e-02,  5.4989e-01,  ...,  3.7439e-01,\n",
            "            2.8728e-01, -3.4320e-01]],\n",
            "\n",
            "         [[-8.3534e-02, -9.1421e-02,  7.8908e-02,  ...,  1.1520e-02,\n",
            "           -8.8332e-03, -6.3080e-02],\n",
            "          [ 8.1209e-01,  3.1682e-01, -4.6726e-01,  ..., -2.7977e-01,\n",
            "            1.6962e-01,  1.6192e-01],\n",
            "          [ 9.5862e-01,  2.7579e-01, -6.4266e-01,  ..., -7.6323e-02,\n",
            "           -4.2246e-02, -1.6837e-02],\n",
            "          ...,\n",
            "          [ 7.0488e-01,  6.1627e-01, -5.0530e-01,  ...,  1.1760e-01,\n",
            "            1.2607e-01,  2.5684e-01],\n",
            "          [ 5.6436e-01,  8.6360e-02, -1.3950e-01,  ...,  3.4483e-02,\n",
            "            1.3891e-01,  2.3690e-01],\n",
            "          [ 6.3609e-01, -2.3415e-01, -1.6829e-01,  ...,  2.9598e-01,\n",
            "            5.7192e-01,  2.0283e-02]],\n",
            "\n",
            "         [[ 7.2435e-02,  6.1836e-02, -4.4342e-02,  ...,  1.7372e-02,\n",
            "            3.1067e-02,  3.5014e-02],\n",
            "          [-2.2072e-01, -4.9970e-01,  8.2652e-01,  ...,  3.6454e-01,\n",
            "           -7.3576e-03,  1.3881e-01],\n",
            "          [-2.9836e-01, -7.3519e-01,  6.9133e-01,  ...,  2.8395e-01,\n",
            "            4.3877e-01,  4.4872e-01],\n",
            "          ...,\n",
            "          [-3.9566e-01, -2.5362e-01,  9.5439e-03,  ...,  1.2540e-02,\n",
            "            6.5333e-01, -4.5772e-01],\n",
            "          [-1.4657e-01,  8.0487e-02,  1.4115e-01,  ..., -1.2753e-01,\n",
            "            6.0114e-01,  3.4710e-03],\n",
            "          [-4.0259e-01, -6.9826e-01,  7.9419e-01,  ..., -1.9524e-02,\n",
            "            8.1705e-01,  1.7518e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.0871e-03, -1.0886e-02,  7.9951e-03,  ..., -1.4815e-02,\n",
            "            1.4655e-02, -1.1553e-02],\n",
            "          [-2.0292e-02, -4.1251e-01,  4.2477e-01,  ..., -2.6508e-01,\n",
            "           -3.2924e-03, -2.8070e-01],\n",
            "          [-6.7340e-02, -8.4942e-02,  1.7437e-01,  ..., -4.8761e-01,\n",
            "            3.0191e-01, -2.7056e-01],\n",
            "          ...,\n",
            "          [-2.2166e-01, -2.4373e-01, -1.5653e-02,  ..., -3.1456e-01,\n",
            "            2.2443e-01, -2.2888e-01],\n",
            "          [-3.5859e-01,  2.4210e-02, -3.3403e-02,  ..., -1.1959e-01,\n",
            "            2.4878e-02, -2.3732e-01],\n",
            "          [ 4.1085e-02, -5.8620e-01,  2.2726e-02,  ..., -6.9354e-01,\n",
            "            2.5394e-01, -2.3094e-01]],\n",
            "\n",
            "         [[-4.0621e-03,  1.9285e-02, -1.3192e-02,  ...,  7.6164e-03,\n",
            "           -7.4825e-03,  1.0157e-02],\n",
            "          [ 1.2319e-01,  2.4116e-01,  1.1737e-01,  ..., -2.2623e-01,\n",
            "           -5.2471e-01, -1.2805e-01],\n",
            "          [ 1.8702e-01, -3.4842e-03, -1.3457e-01,  ..., -1.4832e-01,\n",
            "           -4.5763e-01, -7.6000e-02],\n",
            "          ...,\n",
            "          [ 2.1789e-01,  6.9530e-02,  2.8145e-01,  ...,  8.3103e-02,\n",
            "           -3.4901e-01, -1.4380e-01],\n",
            "          [ 4.9252e-01,  2.6284e-01,  1.5200e-01,  ...,  3.5651e-02,\n",
            "           -2.7443e-01,  2.0253e-01],\n",
            "          [ 4.3361e-01,  4.2965e-01, -1.2734e-01,  ...,  5.3196e-02,\n",
            "            9.8642e-02,  1.2313e-02]],\n",
            "\n",
            "         [[-9.2347e-03,  5.5161e-03, -1.1815e-02,  ...,  7.3588e-03,\n",
            "           -6.4840e-03,  4.8366e-04],\n",
            "          [ 4.2647e-01, -1.2312e-01,  9.6083e-03,  ...,  2.3655e-01,\n",
            "           -1.5562e-02, -1.0863e-01],\n",
            "          [ 2.2822e-01, -6.7062e-01, -5.1333e-02,  ...,  1.6773e-01,\n",
            "            8.1306e-03, -2.7736e-02],\n",
            "          ...,\n",
            "          [ 3.7091e-01, -6.3415e-01, -9.4847e-02,  ...,  6.0628e-02,\n",
            "           -6.3050e-01,  3.8420e-02],\n",
            "          [ 3.4751e-01, -3.1259e-01, -7.6444e-02,  ...,  6.4125e-02,\n",
            "           -1.3953e-01, -3.2667e-01],\n",
            "          [-4.9620e-01, -7.5111e-01, -1.8708e-01,  ..., -5.0860e-02,\n",
            "            3.6291e-01, -1.9559e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-7.9712e-03, -1.6624e-02,  1.2149e-02,  ...,  8.5481e-02,\n",
            "           -1.5770e-01,  2.1053e-02],\n",
            "          [-8.3022e-01,  4.7346e-02,  2.8911e-01,  ...,  1.8979e-02,\n",
            "           -1.0981e+00,  1.7895e-01],\n",
            "          [-3.2448e-01, -7.4828e-01,  2.5499e-01,  ..., -8.8089e-02,\n",
            "           -2.4220e-01,  5.3045e-01],\n",
            "          ...,\n",
            "          [-4.9936e-01,  1.1066e-01, -2.0494e-01,  ..., -3.2277e-01,\n",
            "           -6.4554e-01, -2.4380e-01],\n",
            "          [-1.4124e+00,  4.7055e-01, -2.0531e-01,  ..., -8.3164e-02,\n",
            "           -1.3179e-01,  1.6284e-02],\n",
            "          [-3.8510e-01,  1.0441e-01, -3.4195e-01,  ..., -2.0749e-01,\n",
            "           -1.1213e-02, -4.7392e-01]],\n",
            "\n",
            "         [[ 2.7386e-02, -6.9148e-03,  3.2650e-02,  ...,  1.3748e-01,\n",
            "            1.7803e-01,  2.0295e-01],\n",
            "          [ 9.2786e-01,  1.1344e-01, -5.5798e-01,  ..., -3.0247e+00,\n",
            "           -8.3602e-01,  3.6933e-01],\n",
            "          [ 2.0286e+00, -4.9718e-01, -1.2544e-01,  ..., -2.4885e+00,\n",
            "            1.7561e-01,  2.2461e+00],\n",
            "          ...,\n",
            "          [-6.6489e-01,  9.7296e-01,  5.1142e-01,  ..., -3.5739e+00,\n",
            "            4.4508e-01,  1.0615e+00],\n",
            "          [ 7.5896e-01,  5.9736e-01, -1.5534e-01,  ..., -2.8924e+00,\n",
            "           -7.6031e-01, -2.7095e-01],\n",
            "          [ 9.9444e-01,  8.4986e-02, -8.4136e-01,  ..., -2.3649e+00,\n",
            "           -1.7101e+00,  1.0926e+00]],\n",
            "\n",
            "         [[-2.5307e-02,  2.9934e-03,  2.6521e-02,  ..., -1.0404e+00,\n",
            "           -7.2195e-02, -2.1401e+00],\n",
            "          [-1.9872e-01,  4.1478e-01,  4.6107e-01,  ...,  3.9469e+00,\n",
            "           -2.5203e+00,  6.3566e+00],\n",
            "          [-5.4503e-01, -4.3599e-01,  8.5512e-01,  ...,  4.4900e+00,\n",
            "           -1.1644e+00,  5.1121e+00],\n",
            "          ...,\n",
            "          [ 4.0843e-01,  5.9983e-01, -5.8408e-01,  ...,  3.4630e+00,\n",
            "            2.3727e-01,  4.9447e+00],\n",
            "          [-2.2618e-01,  7.0194e-01, -6.7485e-01,  ...,  4.3101e+00,\n",
            "           -2.9447e+00,  4.2736e+00],\n",
            "          [-2.9153e-01,  7.3030e-02, -7.2142e-01,  ...,  3.1329e+00,\n",
            "           -7.0211e-01,  5.9548e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5448e-02,  1.8229e-02, -1.5472e-02,  ...,  2.1875e-01,\n",
            "            4.0629e-02,  7.9309e-02],\n",
            "          [-6.2777e-02, -6.5757e-01, -6.0900e-01,  ..., -2.8150e-01,\n",
            "           -9.9315e-01, -1.8003e+00],\n",
            "          [-1.3918e+00,  9.6350e-02, -3.3592e-01,  ..., -1.7142e-01,\n",
            "           -7.5625e-01, -1.3581e+00],\n",
            "          ...,\n",
            "          [ 1.8237e+00, -4.6671e-01, -2.0351e-02,  ..., -3.0269e-01,\n",
            "           -1.2908e+00, -1.2903e+00],\n",
            "          [ 9.7502e-01, -4.0478e-01, -6.8302e-02,  ..., -5.8441e-01,\n",
            "           -4.6263e-01, -5.6821e-01],\n",
            "          [-7.9828e-03,  2.8340e-01, -2.8494e-02,  ..., -9.7703e-01,\n",
            "           -1.4534e+00, -1.3942e+00]],\n",
            "\n",
            "         [[ 7.8828e-03, -1.1835e-02,  9.8274e-04,  ...,  2.1803e-01,\n",
            "            2.5968e+00,  9.2977e-02],\n",
            "          [ 4.4379e-01, -3.9871e-01,  7.4219e-01,  ..., -2.3410e+00,\n",
            "           -5.5317e+00,  1.8524e-01],\n",
            "          [ 2.5286e+00,  3.3336e-01,  1.3848e+00,  ..., -3.5984e+00,\n",
            "           -5.8187e+00,  2.8903e-01],\n",
            "          ...,\n",
            "          [-2.4704e+00, -2.0717e-01, -1.0838e+00,  ..., -3.1372e+00,\n",
            "           -6.3971e+00, -5.0863e-01],\n",
            "          [-3.0231e-01, -8.8644e-01,  3.2532e-01,  ..., -4.2210e+00,\n",
            "           -5.7085e+00, -1.6002e+00],\n",
            "          [ 3.9588e-01,  3.9431e-01, -9.2715e-02,  ..., -1.7183e+00,\n",
            "           -6.0796e+00, -2.5549e-01]],\n",
            "\n",
            "         [[ 7.9917e-03, -3.1423e-03, -8.2487e-03,  ..., -2.3891e-01,\n",
            "           -1.8670e-01,  2.6459e-01],\n",
            "          [ 1.0579e+00, -6.3660e-02, -1.1220e+00,  ..., -1.2367e+00,\n",
            "           -7.7838e-01,  4.8971e-01],\n",
            "          [ 1.4959e+00, -8.2711e-01,  1.4256e-01,  ..., -4.9137e-01,\n",
            "           -3.5642e-02,  3.3057e-01],\n",
            "          ...,\n",
            "          [-1.1712e+00,  8.0084e-01,  2.2086e-01,  ..., -1.0474e+00,\n",
            "            4.1871e-01, -5.7400e-01],\n",
            "          [ 8.8846e-01, -2.0122e-01, -1.8173e-01,  ..., -2.0080e-01,\n",
            "            9.7837e-01, -4.9696e-03],\n",
            "          [ 8.8074e-01,  5.2897e-01, -1.4955e-01,  ..., -1.2836e-01,\n",
            "           -1.6911e-01,  1.0815e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 7.2462e-03,  1.8214e-02,  7.6000e-04,  ...,  2.3458e-03,\n",
            "           -8.4346e-03, -2.0956e-02],\n",
            "          [ 2.3952e-01,  2.5782e-01,  2.7900e-01,  ..., -1.8075e-01,\n",
            "            1.3042e-01,  1.6314e-03],\n",
            "          [ 3.4021e-01,  3.0802e-01,  1.8893e-01,  ..., -9.4847e-02,\n",
            "           -8.9587e-02,  2.6230e-01],\n",
            "          ...,\n",
            "          [ 3.0135e-01, -2.1219e-01,  3.9024e-01,  ..., -5.6843e-01,\n",
            "           -3.5540e-01,  4.2298e-01],\n",
            "          [ 8.4475e-02, -2.8627e-01,  2.3186e-02,  ..., -4.4496e-01,\n",
            "           -1.3301e-01, -1.9015e-01],\n",
            "          [ 2.3499e-02,  2.4275e-01, -1.5636e-01,  ..., -1.7619e-01,\n",
            "           -4.9961e-01, -1.8257e-03]],\n",
            "\n",
            "         [[-2.5334e-02, -3.2987e-04, -9.6608e-03,  ..., -7.4988e-03,\n",
            "            1.4712e-02,  3.2667e-02],\n",
            "          [-1.5600e-01, -1.3574e-01, -2.5945e-01,  ..., -7.4613e-02,\n",
            "           -2.3417e-01,  4.5265e-03],\n",
            "          [-2.7918e-01, -3.7829e-01, -3.0981e-01,  ...,  1.3120e-01,\n",
            "           -3.4264e-02, -4.7326e-02],\n",
            "          ...,\n",
            "          [ 2.1176e-01,  7.2982e-02, -2.4784e-01,  ...,  1.5425e-01,\n",
            "           -2.4107e-01,  4.8783e-01],\n",
            "          [-2.5194e-03,  8.5557e-02, -7.0029e-02,  ...,  2.6254e-01,\n",
            "            3.6274e-01,  1.8233e-01],\n",
            "          [ 1.6963e-01,  3.7185e-01,  8.3661e-02,  ..., -1.5500e-01,\n",
            "            6.6046e-02,  1.5300e-01]],\n",
            "\n",
            "         [[-8.7178e-03,  8.9104e-03, -3.9395e-03,  ..., -3.9597e-03,\n",
            "            3.0609e-03,  4.5136e-03],\n",
            "          [-4.8955e-03,  3.1986e-01,  5.9953e-02,  ..., -6.0456e-01,\n",
            "            5.7708e-01, -5.7726e-01],\n",
            "          [-8.5321e-02,  2.6397e-01, -7.4082e-01,  ..., -2.9058e-01,\n",
            "            3.0273e-01,  1.1427e-01],\n",
            "          ...,\n",
            "          [-6.3977e-01,  8.5818e-01,  3.8032e-01,  ..., -4.6890e-01,\n",
            "           -2.3706e-01,  5.3899e-01],\n",
            "          [-1.1476e+00, -4.9717e-02, -1.8109e-01,  ..., -2.4956e-01,\n",
            "            1.7072e-02,  2.2909e-01],\n",
            "          [-7.3384e-01, -5.0397e-01, -8.8830e-02,  ..., -2.3126e-01,\n",
            "           -8.0944e-02, -4.4232e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.4552e-03, -2.7122e-02,  2.7730e-03,  ...,  4.3726e-03,\n",
            "            3.8729e-03, -2.6313e-02],\n",
            "          [-3.4847e-01,  1.4291e-01, -9.9360e-01,  ..., -8.7205e-02,\n",
            "           -4.4909e-01,  1.7497e-01],\n",
            "          [-5.7742e-01,  2.6211e-01, -9.4602e-01,  ...,  3.3632e-01,\n",
            "           -7.5143e-01,  2.1130e-01],\n",
            "          ...,\n",
            "          [-5.5824e-01,  1.2600e-01, -6.7670e-01,  ...,  2.2109e-01,\n",
            "           -5.6471e-01,  8.3721e-02],\n",
            "          [-6.1103e-01, -1.6143e-01, -2.8093e-01,  ..., -2.9907e-01,\n",
            "           -2.3761e-03,  7.7950e-01],\n",
            "          [-4.7765e-01,  2.7121e-01, -2.5681e-01,  ..., -1.1615e-01,\n",
            "           -4.0808e-01,  1.7617e-01]],\n",
            "\n",
            "         [[ 1.4710e-04, -9.1710e-03,  6.7997e-03,  ..., -1.7310e-02,\n",
            "            7.4612e-03, -3.0721e-03],\n",
            "          [ 5.5821e-01,  7.1077e-02,  8.8540e-02,  ...,  2.2589e-02,\n",
            "            1.5234e-01, -1.4326e-01],\n",
            "          [ 2.0324e-01,  7.7112e-02,  3.0667e-01,  ...,  7.3198e-03,\n",
            "            3.7037e-01, -1.2871e-01],\n",
            "          ...,\n",
            "          [-1.0761e-02,  4.4841e-03,  3.2144e-01,  ..., -7.8571e-02,\n",
            "            3.6383e-01,  2.3061e-01],\n",
            "          [ 5.2462e-02, -4.7597e-01,  2.1470e-01,  ..., -2.3591e-01,\n",
            "            1.8277e-01,  3.2791e-01],\n",
            "          [ 3.5629e-01, -2.2695e-01,  3.5568e-01,  ..., -2.6635e-01,\n",
            "            5.2188e-01, -2.6029e-01]],\n",
            "\n",
            "         [[-2.7699e-02, -2.0117e-02,  1.1297e-02,  ...,  1.4553e-02,\n",
            "            1.8240e-02,  9.1900e-04],\n",
            "          [ 2.6204e-01, -3.6296e-01, -2.4504e-01,  ..., -2.3077e-01,\n",
            "           -1.7681e-01, -2.8648e-01],\n",
            "          [ 4.8586e-01, -9.8447e-02, -3.7279e-01,  ..., -4.3125e-01,\n",
            "            1.2946e-01, -5.4560e-01],\n",
            "          ...,\n",
            "          [ 7.3601e-01, -1.1269e-02,  3.0607e-02,  ...,  1.3289e-01,\n",
            "           -4.8658e-02, -3.3828e-01],\n",
            "          [ 6.4724e-01, -2.7333e-01,  1.2130e-01,  ..., -3.6876e-01,\n",
            "            3.4067e-03,  9.8736e-02],\n",
            "          [ 5.8275e-01,  1.5663e-01, -2.1311e-01,  ...,  3.2892e-02,\n",
            "           -1.6547e-01, -9.8825e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.9206e-03,  4.3508e-02, -3.0250e-03,  ...,  5.4492e-02,\n",
            "            7.4753e-02, -1.8526e-01],\n",
            "          [ 4.2824e-01, -2.3828e-01, -1.0957e-01,  ...,  3.9125e-01,\n",
            "           -6.2610e-01, -4.0975e-02],\n",
            "          [-1.6098e+00,  3.6109e-01,  2.1874e-01,  ...,  1.0543e+00,\n",
            "            3.7575e-01,  1.0145e+00],\n",
            "          ...,\n",
            "          [ 2.3782e+00, -1.3624e-01,  6.2661e-02,  ...,  1.1460e+00,\n",
            "           -5.2986e-01,  2.7894e-01],\n",
            "          [ 1.0833e+00,  3.6850e-01,  9.8154e-01,  ...,  4.0082e-01,\n",
            "            2.3083e-01, -8.0051e-02],\n",
            "          [ 9.4575e-02, -5.5404e-01,  7.1287e-01,  ...,  5.5995e-01,\n",
            "            5.0557e-01, -1.8789e-01]],\n",
            "\n",
            "         [[-7.7206e-03,  2.5559e-03,  2.6217e-02,  ...,  7.2459e-01,\n",
            "           -3.8211e-01,  6.3303e-01],\n",
            "          [-8.7474e-01, -1.6336e-01,  6.0411e-01,  ...,  6.0355e-01,\n",
            "           -5.6326e-01, -8.5185e-01],\n",
            "          [-2.6103e+00,  3.3187e-01, -6.1350e-01,  ...,  7.3370e-01,\n",
            "           -2.4360e-01, -1.3749e+00],\n",
            "          ...,\n",
            "          [ 2.0475e+00, -4.9793e-02, -2.6480e-01,  ..., -1.5471e-01,\n",
            "           -6.6964e-01, -1.9129e+00],\n",
            "          [ 1.2656e-01, -8.6657e-03,  1.0226e-01,  ..., -1.1522e+00,\n",
            "            1.9042e-01, -1.8706e+00],\n",
            "          [-6.6870e-01, -2.0337e-01,  5.6835e-01,  ..., -4.5274e-01,\n",
            "            3.4608e-01, -8.1128e-01]],\n",
            "\n",
            "         [[ 1.4524e-02,  4.8841e-03,  4.6334e-03,  ...,  6.1180e-02,\n",
            "            1.0555e+00, -6.2027e-01],\n",
            "          [-6.4945e-01, -3.6746e-01, -3.1495e-02,  ..., -8.7816e-02,\n",
            "           -4.5824e+00,  1.1373e+00],\n",
            "          [-2.9187e-01, -3.4363e-01, -1.3789e-01,  ...,  1.3111e+00,\n",
            "           -2.7790e+00, -1.2641e-01],\n",
            "          ...,\n",
            "          [ 2.7926e-01,  2.4537e-01,  3.1400e-01,  ...,  2.6985e+00,\n",
            "           -3.3557e+00,  1.8902e+00],\n",
            "          [-2.7676e-01,  2.2845e-01,  1.3773e-01,  ..., -5.7642e-01,\n",
            "           -5.2089e+00,  1.1443e-01],\n",
            "          [-1.7446e-01, -2.5561e-01,  2.4040e-01,  ...,  1.5734e+00,\n",
            "           -2.5590e+00,  1.3269e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.6454e-04,  1.6087e-02, -7.9203e-03,  ...,  5.3014e-01,\n",
            "            1.2314e+00,  5.2038e-01],\n",
            "          [-1.2646e-01,  2.3199e-01,  5.3187e-01,  ...,  2.0493e-01,\n",
            "           -1.2201e+00,  5.8348e-01],\n",
            "          [ 2.2898e+00, -5.5567e-01,  5.0671e-01,  ...,  8.7816e-01,\n",
            "           -1.8948e+00, -7.7079e-01],\n",
            "          ...,\n",
            "          [-1.8811e+00,  9.5962e-02, -1.0291e+00,  ...,  6.2418e-01,\n",
            "           -1.5927e+00, -7.7782e-01],\n",
            "          [-5.4349e-01,  6.3633e-01, -8.4284e-01,  ...,  3.3182e-01,\n",
            "           -2.1047e+00, -3.6425e-01],\n",
            "          [ 5.1598e-01, -9.0520e-01,  3.0001e-01,  ...,  2.9273e-01,\n",
            "           -2.5882e+00,  3.6600e-01]],\n",
            "\n",
            "         [[ 9.9248e-03, -4.2506e-02,  3.7478e-02,  ...,  2.9756e-01,\n",
            "            2.3552e-01, -2.0946e+00],\n",
            "          [ 6.5736e-01,  1.2935e+00,  1.8055e-01,  ...,  6.6375e-02,\n",
            "            2.0475e-01,  2.9397e+00],\n",
            "          [-9.3688e-01,  4.5912e-01,  4.0619e-01,  ..., -1.4132e+00,\n",
            "           -1.7311e+00,  2.1591e+00],\n",
            "          ...,\n",
            "          [ 1.9255e+00,  2.6424e-02, -5.1556e-01,  ..., -2.4268e+00,\n",
            "           -1.9273e-01,  2.4232e+00],\n",
            "          [ 1.3727e+00,  6.7600e-01, -1.2895e-01,  ..., -1.2264e+00,\n",
            "           -1.2149e+00,  2.9452e+00],\n",
            "          [ 4.4192e-01,  1.4973e+00,  1.4171e-01,  ..., -2.5026e+00,\n",
            "           -1.6915e+00,  2.6595e+00]],\n",
            "\n",
            "         [[-2.3982e-02,  7.5983e-03, -1.4419e-02,  ...,  3.2934e-01,\n",
            "           -2.0494e-01, -8.7621e-01],\n",
            "          [ 2.4090e+00, -1.5858e-01, -2.0066e-02,  ..., -8.7819e-01,\n",
            "            7.6496e-02,  2.4105e+00],\n",
            "          [ 2.9497e+00, -2.1958e-01, -2.6174e-01,  ...,  4.4033e-01,\n",
            "            1.2003e+00,  3.0687e+00],\n",
            "          ...,\n",
            "          [-6.2632e-01, -5.0834e-01, -5.6188e-02,  ..., -9.4369e-01,\n",
            "            1.1597e+00,  2.4076e+00],\n",
            "          [ 1.8785e+00, -7.5015e-01, -7.0974e-04,  ...,  9.5525e-02,\n",
            "            1.5731e+00,  3.1766e+00],\n",
            "          [ 2.0049e+00, -5.1904e-01, -5.8605e-01,  ...,  1.1861e+00,\n",
            "           -3.2133e-01,  2.9469e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-1.1366e-02,  2.4691e-02,  9.0909e-03,  ...,  8.5482e-03,\n",
            "            1.0136e-02,  3.0711e-02],\n",
            "          [-1.0147e-01,  3.1870e-01,  3.4862e-02,  ..., -4.0480e-01,\n",
            "           -2.7714e-01, -5.8336e-01],\n",
            "          [ 9.7094e-02,  9.2588e-02,  6.8009e-02,  ..., -1.4584e-01,\n",
            "           -3.3491e-01, -3.5070e-01],\n",
            "          ...,\n",
            "          [-1.6956e-01,  5.3192e-01, -1.1333e-01,  ..., -2.0768e-01,\n",
            "           -5.4364e-01, -4.2296e-01],\n",
            "          [-1.8123e-02,  3.8084e-01, -4.2526e-02,  ..., -1.0904e-01,\n",
            "           -2.8744e-02, -1.3998e-01],\n",
            "          [-3.8455e-01,  8.2099e-02,  1.5656e-01,  ..., -6.4152e-01,\n",
            "           -1.0520e-01, -2.3929e-01]],\n",
            "\n",
            "         [[ 2.4157e-02,  3.1741e-02, -1.8387e-03,  ...,  1.5442e-01,\n",
            "            1.7166e-02,  1.1107e-02],\n",
            "          [-2.2606e-01, -3.3447e-01,  7.1311e-02,  ..., -8.5669e-02,\n",
            "            7.7471e-03,  3.1496e-01],\n",
            "          [-9.0790e-02, -1.7717e-01,  5.0923e-01,  ..., -2.2144e-01,\n",
            "           -3.3000e-01,  5.3181e-01],\n",
            "          ...,\n",
            "          [ 1.9201e-02, -4.8699e-01,  1.3926e-01,  ..., -9.1145e-01,\n",
            "           -7.2880e-01, -2.5177e-03],\n",
            "          [ 7.3448e-02, -1.4742e-01,  2.4712e-01,  ..., -3.9215e-01,\n",
            "           -1.6368e-01,  9.7701e-02],\n",
            "          [-2.0118e-02,  5.2454e-01,  7.5413e-02,  ..., -4.1814e-01,\n",
            "            4.0136e-01,  8.0996e-01]],\n",
            "\n",
            "         [[ 4.6670e-03,  5.3326e-03, -1.5313e-02,  ..., -7.6988e-03,\n",
            "            5.8347e-03, -5.1419e-04],\n",
            "          [-7.1147e-01,  9.2343e-01,  4.2361e-01,  ...,  4.9744e-01,\n",
            "           -6.0660e-01, -6.4129e-01],\n",
            "          [-1.1544e-01,  5.7165e-01, -3.9556e-01,  ...,  2.9007e-01,\n",
            "           -7.9573e-03, -2.3762e-01],\n",
            "          ...,\n",
            "          [-1.4047e-01, -2.5419e-01, -6.4668e-01,  ..., -2.2080e-01,\n",
            "            1.0973e-01,  2.2872e-02],\n",
            "          [-1.8157e-01,  7.1507e-01, -8.3137e-01,  ..., -6.0540e-01,\n",
            "           -2.6614e-01, -7.7377e-01],\n",
            "          [ 1.1229e+00,  8.7060e-01, -3.0900e-01,  ...,  1.4117e-01,\n",
            "            7.8765e-02, -1.5892e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.9736e-05,  7.4718e-04, -2.6825e-03,  ...,  5.6975e-03,\n",
            "            1.2294e-02,  7.4902e-03],\n",
            "          [-5.9341e-02,  1.6398e-01, -2.3056e-01,  ...,  1.7307e-01,\n",
            "            2.7170e-01,  1.4913e-01],\n",
            "          [-3.4098e-01, -3.5863e-01,  2.9176e-01,  ...,  2.2144e-01,\n",
            "           -1.0052e-01, -2.4965e-01],\n",
            "          ...,\n",
            "          [-4.3288e-01,  8.2409e-02, -6.8726e-02,  ...,  4.0259e-02,\n",
            "            2.1688e-01,  2.1084e-01],\n",
            "          [-5.0617e-01,  2.5949e-01,  1.0250e-01,  ...,  2.7462e-02,\n",
            "            5.3554e-01,  3.4847e-01],\n",
            "          [-3.8535e-01,  2.0286e-01, -2.8518e-01,  ..., -4.0802e-02,\n",
            "            6.1078e-01, -4.1687e-02]],\n",
            "\n",
            "         [[-2.8269e-03, -9.3114e-03,  6.6801e-03,  ..., -5.7844e-03,\n",
            "            3.9057e-02, -2.5234e-02],\n",
            "          [ 8.3222e-02, -7.8048e-02, -4.8170e-02,  ...,  5.2697e-01,\n",
            "            2.0880e-01, -2.0181e-01],\n",
            "          [ 4.7791e-01, -2.8646e-01, -4.3999e-02,  ...,  9.3243e-03,\n",
            "            4.7707e-01, -4.5776e-01],\n",
            "          ...,\n",
            "          [ 1.2179e+00, -4.3954e-01,  3.2411e-01,  ..., -1.1112e-01,\n",
            "            1.8765e-01, -4.0621e-01],\n",
            "          [-4.0291e-02, -2.2248e-01,  4.0548e-01,  ...,  6.7645e-03,\n",
            "           -1.2867e-01, -5.3416e-01],\n",
            "          [ 3.6415e-01, -3.0661e-01,  4.2177e-01,  ...,  3.9263e-02,\n",
            "            3.3675e-01, -1.0154e+00]],\n",
            "\n",
            "         [[ 1.5409e-02, -5.5960e-02,  1.4440e-02,  ..., -2.9179e-02,\n",
            "           -2.1220e-02,  1.0018e-02],\n",
            "          [ 4.4486e-01,  1.5495e-01, -4.2529e-01,  ..., -2.3489e-01,\n",
            "            3.1283e-01,  1.2805e-01],\n",
            "          [ 3.0764e-01,  1.7481e-01, -2.4576e-01,  ...,  1.0454e-01,\n",
            "            4.7897e-01, -9.6128e-02],\n",
            "          ...,\n",
            "          [ 8.3215e-01, -5.9955e-02, -1.6462e-01,  ..., -2.3817e-02,\n",
            "            5.3632e-01,  1.8490e-01],\n",
            "          [ 4.3916e-01,  2.7895e-01, -3.3623e-01,  ..., -2.4511e-01,\n",
            "            4.4579e-01, -3.1077e-02],\n",
            "          [ 9.1363e-01,  4.9162e-01,  1.6814e-01,  ..., -9.2729e-02,\n",
            "           -3.0428e-02,  1.9786e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 8.9728e-03, -1.8144e-02,  1.5558e-03,  ..., -2.8165e-01,\n",
            "            1.9437e-03,  3.6890e-01],\n",
            "          [ 1.3163e+00, -1.0308e+00,  2.7510e-01,  ...,  3.7480e-01,\n",
            "            2.1632e-01, -8.2541e-01],\n",
            "          [-9.6915e-02,  9.8680e-02, -4.2133e-01,  ...,  5.9113e-01,\n",
            "            1.0312e+00, -1.4607e+00],\n",
            "          ...,\n",
            "          [ 1.3391e+00, -1.1806e+00,  6.5396e-02,  ...,  5.2826e-01,\n",
            "            1.3623e+00, -2.0056e+00],\n",
            "          [ 2.0255e+00, -7.6572e-01,  3.9555e-01,  ..., -2.6370e-02,\n",
            "            6.5833e-01, -2.7466e-01],\n",
            "          [ 7.2145e-01, -5.8751e-02,  8.5969e-02,  ..., -2.4630e-01,\n",
            "            1.1644e+00, -1.5548e+00]],\n",
            "\n",
            "         [[ 2.6712e-02,  8.0064e-05, -1.7687e-02,  ..., -6.5377e-02,\n",
            "            6.8501e-02, -6.7049e-02],\n",
            "          [-8.6886e-01,  3.8114e-01,  9.0350e-02,  ..., -6.0475e-01,\n",
            "            6.5573e-01,  1.2757e+00],\n",
            "          [-1.6139e+00, -2.6808e-01, -5.7079e-01,  ...,  1.2809e+00,\n",
            "            4.0590e-01,  8.2342e-01],\n",
            "          ...,\n",
            "          [ 7.7026e-01, -1.6060e-01,  3.2378e-01,  ...,  3.9619e-01,\n",
            "            1.8933e+00,  1.3549e+00],\n",
            "          [-8.1814e-01,  3.9869e-01,  9.8617e-01,  ...,  8.1695e-01,\n",
            "            3.1917e-01,  1.1138e+00],\n",
            "          [-1.1489e+00, -1.6711e-01,  1.0015e-01,  ...,  7.4885e-01,\n",
            "           -6.2542e-01,  8.9456e-01]],\n",
            "\n",
            "         [[-8.9567e-03, -1.3756e-02, -9.5162e-03,  ...,  1.6098e-02,\n",
            "            8.5227e-02, -9.6235e-01],\n",
            "          [-1.0299e+00,  2.2282e-01, -2.2024e-01,  ...,  1.6607e+00,\n",
            "           -7.5070e-02,  1.4525e+00],\n",
            "          [-8.2115e-01, -3.5738e-01, -2.2489e-01,  ...,  8.3426e-01,\n",
            "            1.5997e+00,  1.8240e+00],\n",
            "          ...,\n",
            "          [ 4.0969e-01,  2.5050e-01,  3.2010e-01,  ..., -1.1940e+00,\n",
            "            2.2101e+00,  1.3245e+00],\n",
            "          [-3.0507e-01,  4.9905e-01, -3.1430e-02,  ..., -1.5512e-01,\n",
            "            2.5857e+00,  3.0531e+00],\n",
            "          [-7.6783e-01, -3.4778e-01, -3.5881e-01,  ...,  1.4165e+00,\n",
            "           -7.5327e-01,  3.4083e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.6273e-03, -2.3231e-02,  7.2364e-03,  ..., -2.1853e-01,\n",
            "           -2.6303e-01, -9.3565e-01],\n",
            "          [-5.2263e-01,  2.9522e-01, -8.9495e-02,  ...,  1.3919e-01,\n",
            "           -1.7356e-01,  1.8636e+00],\n",
            "          [-2.7164e+00,  6.4869e-02,  2.0072e-02,  ..., -1.6806e-01,\n",
            "            2.4056e-01,  1.9102e+00],\n",
            "          ...,\n",
            "          [ 1.8749e+00,  4.8274e-01, -4.1031e-01,  ...,  7.4469e-02,\n",
            "           -4.9039e-01,  4.1937e-01],\n",
            "          [-1.9181e-01, -2.6078e-01,  8.6791e-01,  ...,  1.8926e-01,\n",
            "           -3.4254e-01,  1.7671e+00],\n",
            "          [-7.9492e-01, -6.0215e-01,  1.1642e+00,  ...,  4.8292e-01,\n",
            "           -1.3626e+00,  1.2354e+00]],\n",
            "\n",
            "         [[-4.2691e-03,  2.4338e-03, -1.2770e-02,  ...,  3.0832e-01,\n",
            "           -7.4788e-01, -4.2875e-01],\n",
            "          [ 1.5713e-01, -1.8469e-02,  3.3720e-01,  ..., -3.6884e-01,\n",
            "            2.2354e+00,  1.4121e+00],\n",
            "          [ 1.3988e+00,  7.4594e-01, -5.6921e-01,  ...,  3.2422e+00,\n",
            "            3.6703e+00,  2.0448e+00],\n",
            "          ...,\n",
            "          [-9.8939e-01, -5.8129e-01,  8.1735e-01,  ...,  3.3936e+00,\n",
            "            2.6831e+00,  3.1352e+00],\n",
            "          [ 3.3807e-01, -2.3479e-01, -6.8715e-01,  ...,  2.0380e+00,\n",
            "            3.4353e+00,  1.8884e+00],\n",
            "          [-1.1260e-01, -4.3409e-02,  8.8320e-02,  ...,  3.8788e+00,\n",
            "            3.2157e+00,  3.8962e+00]],\n",
            "\n",
            "         [[-3.0774e-03, -1.1889e-02,  4.0719e-03,  ..., -2.1780e-02,\n",
            "            1.2519e-01, -3.5193e-01],\n",
            "          [-5.5235e-01, -5.3558e-01,  1.3228e-01,  ..., -1.5114e+00,\n",
            "            3.8832e-02, -8.0794e-01],\n",
            "          [-1.8630e+00, -3.1644e-01, -9.1794e-02,  ..., -2.0427e+00,\n",
            "            6.0499e-01, -6.8473e-01],\n",
            "          ...,\n",
            "          [ 5.0587e-01,  8.4280e-01,  3.7657e-01,  ..., -1.8076e+00,\n",
            "           -4.1563e-01, -1.3126e+00],\n",
            "          [-1.2349e+00,  7.3562e-01,  5.9065e-01,  ..., -1.7874e+00,\n",
            "           -1.3998e+00,  6.7130e-01],\n",
            "          [-4.5779e-01, -3.4571e-01, -3.7067e-01,  ..., -2.5460e+00,\n",
            "           -4.3147e-01, -7.0690e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-2.9076e-02,  4.8660e-02,  2.3231e-02,  ...,  2.7971e-02,\n",
            "            5.3325e-03, -1.2754e-02],\n",
            "          [-1.6131e-01,  3.7115e-02, -1.5606e-01,  ..., -3.2309e-01,\n",
            "           -4.1365e-01,  4.1685e-01],\n",
            "          [ 1.8083e-01, -5.0071e-01, -2.9184e-01,  ..., -4.0496e-01,\n",
            "           -4.2592e-01,  2.5350e-01],\n",
            "          ...,\n",
            "          [ 2.0700e-01, -1.6999e-01, -3.1977e-01,  ...,  5.7351e-02,\n",
            "           -4.8769e-01,  6.4990e-01],\n",
            "          [-1.9693e-01,  4.2536e-02, -2.2516e-01,  ..., -2.1744e-01,\n",
            "           -4.3731e-02,  3.8947e-01],\n",
            "          [ 3.0242e-01,  2.5994e-01, -3.0940e-01,  ..., -5.1473e-01,\n",
            "           -3.0705e-01, -4.5294e-01]],\n",
            "\n",
            "         [[-9.8861e-03, -1.7542e-02, -1.2947e-02,  ...,  2.2655e-02,\n",
            "            3.6845e-03,  2.7907e-02],\n",
            "          [-1.8600e-01,  3.7770e-01,  5.9465e-01,  ..., -2.7391e-01,\n",
            "           -9.9370e-02,  2.7473e-01],\n",
            "          [-4.0553e-01, -2.3169e-01,  2.5983e-01,  ...,  4.0804e-02,\n",
            "            4.3493e-02,  1.4774e-01],\n",
            "          ...,\n",
            "          [ 1.1280e-01, -2.3829e-01,  8.2961e-02,  ..., -1.2909e-01,\n",
            "            3.1609e-01,  5.1331e-01],\n",
            "          [-3.2935e-01, -4.3872e-02,  5.4483e-01,  ..., -1.3656e-01,\n",
            "            3.9485e-01, -2.2082e-01],\n",
            "          [-2.4233e-01,  2.8967e-01,  1.9679e-01,  ...,  1.0121e-01,\n",
            "           -9.5928e-02,  4.8344e-01]],\n",
            "\n",
            "         [[-1.7134e-02,  2.4760e-04,  3.2277e-03,  ...,  6.2769e-03,\n",
            "            1.6764e-02,  5.6842e-03],\n",
            "          [ 3.5733e-02,  2.1141e-05, -3.4201e-01,  ...,  3.0989e-01,\n",
            "           -2.2985e-01,  1.4500e-01],\n",
            "          [ 2.5499e-01,  2.8238e-02, -1.4964e-01,  ...,  3.0037e-01,\n",
            "           -4.2634e-01,  1.7331e-01],\n",
            "          ...,\n",
            "          [ 2.3485e-01,  2.3247e-01, -2.0583e-01,  ...,  1.1367e-01,\n",
            "           -2.9789e-01,  5.1094e-01],\n",
            "          [ 2.9465e-01,  4.3132e-01, -3.7535e-01,  ...,  4.5121e-02,\n",
            "           -6.4595e-02, -2.9897e-02],\n",
            "          [ 5.8383e-02,  4.2934e-01, -6.1532e-02,  ..., -1.1579e-01,\n",
            "           -6.2209e-01,  2.8688e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.5921e-02,  3.7171e-02,  4.2684e-03,  ..., -6.7215e-05,\n",
            "            2.7319e-02, -5.1843e-02],\n",
            "          [ 5.3173e-02, -2.2773e-01,  3.6753e-01,  ..., -1.2770e-02,\n",
            "           -6.5411e-02, -2.5643e-01],\n",
            "          [ 2.7707e-01, -5.0552e-01, -6.1785e-01,  ..., -5.6705e-01,\n",
            "            3.5503e-01, -3.8038e-01],\n",
            "          ...,\n",
            "          [ 2.6158e-01, -3.3812e-01, -1.0223e-01,  ..., -4.9339e-01,\n",
            "            2.7679e-01, -7.5817e-01],\n",
            "          [ 2.1273e-01, -3.5683e-01, -1.2904e-01,  ...,  6.4587e-02,\n",
            "            4.0239e-01, -4.2796e-01],\n",
            "          [ 4.1599e-01, -3.5254e-01, -1.0218e+00,  ..., -7.4032e-03,\n",
            "            4.3982e-01, -8.6960e-01]],\n",
            "\n",
            "         [[ 2.0049e-02, -5.3164e-03, -2.1952e-02,  ...,  3.9178e-03,\n",
            "            1.2586e-02,  5.3721e-04],\n",
            "          [ 5.6575e-02, -6.3855e-01,  3.4598e-01,  ...,  1.1462e-01,\n",
            "           -1.2250e-01,  3.2148e-01],\n",
            "          [-5.7520e-02, -1.9262e-01,  7.1922e-02,  ..., -3.0811e-01,\n",
            "            4.0385e-02, -1.7135e-01],\n",
            "          ...,\n",
            "          [ 4.2213e-02,  2.9243e-02,  3.0442e-01,  ..., -7.2909e-01,\n",
            "           -2.0473e-01, -4.3267e-01],\n",
            "          [-2.9761e-01, -4.6874e-02, -4.3546e-02,  ..., -2.7982e-01,\n",
            "           -6.9675e-02, -2.1628e-01],\n",
            "          [-4.0412e-01, -1.7162e-01, -1.5077e-01,  ..., -1.9258e-01,\n",
            "           -6.1416e-01, -5.9688e-01]],\n",
            "\n",
            "         [[-4.3662e-02, -1.5268e-02, -1.7151e-03,  ..., -8.9805e-03,\n",
            "            7.6542e-03,  9.8193e-03],\n",
            "          [ 4.4052e-01,  1.7053e-01,  6.5245e-01,  ..., -4.3234e-02,\n",
            "           -1.3788e-01, -2.5513e-01],\n",
            "          [ 4.8203e-01, -3.2311e-01,  8.3111e-01,  ...,  4.5649e-02,\n",
            "           -2.4343e-03,  5.1254e-01],\n",
            "          ...,\n",
            "          [-1.9040e-01,  3.9512e-01,  9.7757e-01,  ...,  5.0499e-02,\n",
            "            1.6838e-01,  2.7264e-01],\n",
            "          [ 1.1839e-02,  1.6968e-01,  4.8225e-01,  ...,  2.0617e-01,\n",
            "           -2.5595e-01, -1.1044e-01],\n",
            "          [ 2.9271e-01,  1.3083e-01,  7.1053e-01,  ..., -3.8353e-01,\n",
            "            3.9529e-01,  5.4554e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.0804e-02,  2.8732e-02, -7.2275e-03,  ..., -2.5219e-01,\n",
            "            1.0866e-01, -1.5309e-01],\n",
            "          [ 2.1209e+00,  2.5545e-01,  3.2814e-01,  ..., -4.0992e-01,\n",
            "           -5.0353e-01,  1.9617e+00],\n",
            "          [-4.8664e-01, -1.1284e-01, -1.1249e+00,  ...,  1.1839e-01,\n",
            "           -1.9965e+00,  3.2445e-01],\n",
            "          ...,\n",
            "          [ 3.4565e+00, -1.6521e-01,  5.5590e-02,  ...,  8.7352e-01,\n",
            "           -2.1914e+00,  2.5152e-01],\n",
            "          [ 3.4944e+00,  4.8182e-02,  1.2056e+00,  ...,  5.9489e-01,\n",
            "           -1.0640e+00,  9.5483e-01],\n",
            "          [ 5.7042e-01,  4.5073e-01,  2.1075e-01,  ...,  2.3330e-01,\n",
            "           -1.8469e+00,  6.7169e-01]],\n",
            "\n",
            "         [[-1.1453e-02,  1.3575e-02,  4.4752e-03,  ..., -2.2092e+00,\n",
            "            3.0931e-01,  5.1248e-01],\n",
            "          [ 1.7423e+00,  6.3191e-01, -2.1717e-01,  ...,  2.2304e+00,\n",
            "            2.0146e+00, -9.2341e-01],\n",
            "          [ 7.9099e-01,  5.6438e-01,  1.1697e-01,  ...,  3.7890e+00,\n",
            "            1.3680e+00, -7.8759e-01],\n",
            "          ...,\n",
            "          [ 4.6298e-01, -1.6931e-01,  1.5447e-01,  ...,  3.5750e+00,\n",
            "            1.6261e+00, -1.1938e+00],\n",
            "          [ 1.3886e+00,  1.4304e-01, -4.3030e-01,  ...,  3.2289e+00,\n",
            "            2.3995e-02, -1.0464e+00],\n",
            "          [ 1.0819e+00, -9.5360e-01,  3.9970e-01,  ...,  3.0487e+00,\n",
            "            6.2005e-01,  1.2427e-02]],\n",
            "\n",
            "         [[-1.3390e-02, -1.3271e-03,  2.3905e-02,  ..., -1.1390e-01,\n",
            "           -3.8690e-02, -2.7705e-01],\n",
            "          [ 5.4290e-01, -2.0728e-01, -6.8885e-01,  ..., -2.0778e+00,\n",
            "           -2.3189e+00,  1.6613e+00],\n",
            "          [-8.4834e-01, -7.9426e-01, -6.5567e-01,  ..., -2.5757e+00,\n",
            "           -4.4161e+00,  1.3445e+00],\n",
            "          ...,\n",
            "          [ 1.6248e+00,  1.1675e+00,  4.2902e-01,  ..., -2.8204e+00,\n",
            "           -2.8612e+00, -1.2414e+00],\n",
            "          [ 1.5577e+00, -6.5309e-02, -2.7342e-02,  ..., -2.9874e+00,\n",
            "           -3.4074e+00,  6.1181e-01],\n",
            "          [-4.9317e-01, -8.3437e-01, -2.0051e-01,  ..., -2.4292e+00,\n",
            "           -3.7548e+00,  2.8157e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0831e-02, -1.8876e-02,  1.7452e-03,  ..., -1.7823e+00,\n",
            "            2.0315e+00, -5.9405e-01],\n",
            "          [-1.2456e+00, -5.6659e-01, -3.1768e-01,  ...,  2.4944e+00,\n",
            "           -3.3299e+00,  5.2135e-01],\n",
            "          [ 2.3231e-01, -4.9877e-01, -1.9004e-01,  ...,  9.8388e-01,\n",
            "           -3.2738e+00,  2.4152e-01],\n",
            "          ...,\n",
            "          [-1.5762e+00,  3.2791e-01, -1.2414e-01,  ...,  1.3472e+00,\n",
            "           -4.1960e+00,  1.4581e+00],\n",
            "          [-1.0989e+00,  8.8594e-02,  7.6062e-01,  ...,  1.2576e+00,\n",
            "           -4.5510e+00, -7.2566e-01],\n",
            "          [ 8.5076e-01, -5.7227e-01,  3.1399e-01,  ...,  2.5179e+00,\n",
            "           -4.0184e+00, -1.0973e+00]],\n",
            "\n",
            "         [[-2.4467e-02,  5.5575e-03, -8.1474e-03,  ..., -1.7411e-01,\n",
            "            1.6422e-01, -8.9316e-01],\n",
            "          [ 1.0493e+00,  4.0086e-01, -1.5460e-01,  ..., -1.5225e+00,\n",
            "            1.0938e+00,  3.1288e+00],\n",
            "          [-1.0120e+00, -1.6411e-01, -3.1361e-01,  ..., -8.0422e-01,\n",
            "            2.1487e-03,  1.7436e+00],\n",
            "          ...,\n",
            "          [ 1.1966e+00,  1.4482e-01,  2.4856e-01,  ..., -9.1376e-01,\n",
            "            8.2074e-01,  1.8936e+00],\n",
            "          [ 1.5707e+00,  3.9725e-02, -7.9245e-01,  ..., -1.7001e+00,\n",
            "            2.9982e-01,  1.9102e+00],\n",
            "          [-1.4511e-01, -3.5857e-01, -5.6053e-01,  ..., -8.4742e-01,\n",
            "            1.2191e+00,  2.0349e+00]],\n",
            "\n",
            "         [[ 7.4845e-03,  3.0448e-02,  1.9222e-02,  ..., -6.1244e-02,\n",
            "           -2.3534e-01,  3.0057e-02],\n",
            "          [ 1.3825e+00, -7.0732e-01, -9.1033e-01,  ..., -9.2017e-01,\n",
            "           -3.8939e-01,  7.2348e-01],\n",
            "          [ 2.1067e+00,  5.3989e-01,  7.3159e-01,  ..., -7.9963e-01,\n",
            "            3.2773e-01, -3.8211e-01],\n",
            "          ...,\n",
            "          [-7.2872e-01, -1.9319e-01, -1.1838e+00,  ..., -9.2820e-01,\n",
            "           -5.2560e-01, -2.4223e-01],\n",
            "          [ 1.2010e+00,  1.0012e-01, -1.0654e+00,  ..., -8.7067e-01,\n",
            "           -4.3188e-01, -2.6679e-01],\n",
            "          [ 2.0408e+00,  1.0730e+00, -6.8860e-01,  ..., -5.4968e-03,\n",
            "            1.6765e-01, -7.8164e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 4.9977e-02, -1.8492e-02,  5.3293e-02,  ..., -6.6943e-03,\n",
            "            8.5208e-03, -5.9303e-02],\n",
            "          [ 1.0214e-01, -5.3653e-01, -1.1765e-02,  ..., -5.2769e-01,\n",
            "           -2.1588e-01,  6.8738e-01],\n",
            "          [ 1.0900e-01, -1.0325e-01, -1.6113e-02,  ..., -2.3162e-01,\n",
            "           -4.0024e-02, -1.9916e-01],\n",
            "          ...,\n",
            "          [ 4.3307e-01,  1.4382e-02, -1.2031e-01,  ..., -1.6306e-01,\n",
            "           -4.3139e-01, -5.5429e-01],\n",
            "          [ 2.5177e-01, -2.4280e-01,  4.5981e-01,  ...,  1.0270e-01,\n",
            "           -1.8137e-01, -6.6380e-02],\n",
            "          [ 2.7493e-01, -2.2599e-01,  4.6197e-01,  ..., -2.6164e-01,\n",
            "           -5.3676e-01, -4.3631e-01]],\n",
            "\n",
            "         [[ 5.8559e-03, -2.1432e-02, -2.2122e-03,  ...,  6.5736e-03,\n",
            "            1.8772e-04, -1.0414e-04],\n",
            "          [-2.2824e-02, -6.2308e-01, -4.5930e-02,  ...,  1.8230e-01,\n",
            "           -6.3343e-01, -1.5594e-01],\n",
            "          [ 4.3942e-01, -9.3679e-01,  1.9657e-01,  ..., -2.8928e-01,\n",
            "            8.2631e-01,  1.0274e-01],\n",
            "          ...,\n",
            "          [-5.7329e-02,  5.1934e-01,  3.5798e-02,  ..., -2.6163e-01,\n",
            "            8.8781e-01,  5.6527e-02],\n",
            "          [ 6.7029e-01,  3.7507e-01, -1.4755e-01,  ..., -5.7711e-01,\n",
            "            7.4706e-01,  3.9114e-01],\n",
            "          [ 1.1665e-01, -3.3091e-02, -1.7781e-01,  ...,  3.2795e-01,\n",
            "            6.6022e-01,  2.4112e-01]],\n",
            "\n",
            "         [[-9.1800e-04, -3.6186e-03,  8.1721e-04,  ...,  9.0537e-03,\n",
            "           -2.1030e-03, -2.5954e-03],\n",
            "          [-3.1938e-02,  1.9407e-01, -1.4515e-01,  ..., -3.5968e-01,\n",
            "           -4.0782e-01, -1.4283e-01],\n",
            "          [-1.3159e-01,  9.2159e-02, -1.9737e-01,  ...,  1.6652e-01,\n",
            "            1.8424e-01, -4.4204e-01],\n",
            "          ...,\n",
            "          [-1.8420e-01, -1.0504e-01,  1.3582e-01,  ...,  2.7004e-02,\n",
            "           -1.1092e-02, -8.0059e-02],\n",
            "          [ 1.9372e-01, -1.8896e-01,  1.4124e-01,  ..., -9.9361e-02,\n",
            "            2.4900e-01, -2.1004e-01],\n",
            "          [-3.9904e-01, -3.7815e-01,  1.2075e-01,  ...,  5.8167e-02,\n",
            "            1.1991e-01, -1.2057e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1293e-02, -1.5450e-02, -2.0548e-02,  ...,  5.3063e-03,\n",
            "           -2.3659e-02,  3.4026e-02],\n",
            "          [ 2.2389e-01, -8.9522e-02,  2.2480e-01,  ...,  1.9808e-01,\n",
            "           -5.2443e-02,  1.6810e-01],\n",
            "          [ 5.4107e-01,  1.1508e-02, -8.1476e-03,  ...,  4.3744e-02,\n",
            "           -3.4296e-01,  1.4475e-01],\n",
            "          ...,\n",
            "          [ 6.4781e-02, -2.2572e-01, -1.1400e-01,  ...,  3.1365e-01,\n",
            "           -9.3924e-02,  2.6554e-01],\n",
            "          [ 3.3371e-01, -3.0690e-01, -4.8952e-01,  ...,  4.5044e-01,\n",
            "           -2.2438e-01,  1.7286e-01],\n",
            "          [ 1.8273e-01, -5.3059e-01, -1.2746e-01,  ..., -6.0464e-02,\n",
            "            5.8217e-02,  5.1484e-01]],\n",
            "\n",
            "         [[ 1.9789e-03,  7.7331e-04,  2.1914e-02,  ..., -3.6119e-02,\n",
            "           -7.1574e-03,  1.1404e-02],\n",
            "          [ 4.7514e-01,  1.4819e-01, -3.0136e-01,  ..., -2.7416e-01,\n",
            "            1.3058e-02,  9.3885e-02],\n",
            "          [-6.4100e-02,  3.4915e-01, -1.0903e+00,  ..., -4.8232e-01,\n",
            "            1.1257e-01, -2.0710e-01],\n",
            "          ...,\n",
            "          [-2.9467e-01, -7.0712e-02, -6.9148e-01,  ..., -4.3480e-02,\n",
            "            6.8902e-01, -3.2857e-01],\n",
            "          [ 1.6399e-01, -4.4741e-01, -3.5014e-01,  ...,  6.6927e-02,\n",
            "            8.0910e-01,  4.6306e-01],\n",
            "          [ 8.1747e-01,  1.8062e-01, -6.1255e-01,  ..., -3.2559e-01,\n",
            "            5.3061e-01, -3.8131e-01]],\n",
            "\n",
            "         [[ 3.5576e-02,  2.4535e-02, -1.2650e-02,  ...,  2.5635e-02,\n",
            "           -1.3707e-02,  1.9820e-02],\n",
            "          [ 6.2153e-01, -2.8307e-01,  2.1841e-01,  ...,  8.0595e-02,\n",
            "            2.4514e-01, -3.2590e-01],\n",
            "          [-7.2257e-01, -2.0188e-01, -5.0250e-01,  ..., -3.1739e-01,\n",
            "            4.1504e-01, -6.4830e-01],\n",
            "          ...,\n",
            "          [-5.4108e-01, -1.2276e-01, -3.3585e-01,  ..., -1.1539e-02,\n",
            "            1.7196e-01, -6.2399e-01],\n",
            "          [ 3.9131e-02,  3.7083e-01, -2.2922e-02,  ...,  5.1491e-01,\n",
            "           -1.6870e-01, -1.7708e-01],\n",
            "          [-2.0828e-01, -3.3644e-01,  2.1152e-01,  ...,  4.4638e-01,\n",
            "            9.9797e-02, -5.0967e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-6.3146e-03,  3.9372e-04, -9.6392e-03,  ..., -6.3405e-02,\n",
            "           -8.9494e-02, -1.0768e-02],\n",
            "          [-1.3551e-01, -2.5616e-01,  3.2361e-01,  ..., -8.0626e-01,\n",
            "           -6.5293e-02,  9.4904e-01],\n",
            "          [-2.3149e-01,  1.9912e-02,  4.6407e-01,  ...,  1.4383e+00,\n",
            "            4.3084e-02, -1.1570e+00],\n",
            "          ...,\n",
            "          [-5.4663e-01,  3.0788e-01, -4.0628e-01,  ...,  4.8850e-01,\n",
            "            1.4214e-01, -1.8429e-01],\n",
            "          [ 1.5647e-01,  2.1085e-03, -3.7341e-01,  ...,  1.0602e+00,\n",
            "            1.0084e+00,  2.6790e-01],\n",
            "          [-1.1897e-02, -3.4245e-01,  4.5340e-01,  ...,  9.7250e-01,\n",
            "           -3.2333e-01, -7.0086e-01]],\n",
            "\n",
            "         [[-1.2259e-02,  6.2028e-03,  1.4393e-02,  ..., -8.2191e-02,\n",
            "           -7.7537e-02, -9.9662e-02],\n",
            "          [ 1.2353e-01,  1.3570e-01,  6.2539e-01,  ...,  4.2382e-01,\n",
            "           -4.8720e-01, -9.3103e-01],\n",
            "          [-1.1347e+00, -1.1314e+00, -7.0422e-01,  ..., -9.0285e-01,\n",
            "            1.7342e-01, -2.1964e+00],\n",
            "          ...,\n",
            "          [ 7.4156e-01,  1.7286e+00,  1.6123e+00,  ...,  1.3200e-01,\n",
            "            4.1072e-01, -1.2824e+00],\n",
            "          [ 1.5905e-01,  6.2736e-01,  5.6221e-01,  ...,  5.9172e-02,\n",
            "            4.0798e-01, -1.0954e+00],\n",
            "          [-1.4650e+00,  1.3448e-02, -5.5696e-01,  ...,  9.0609e-01,\n",
            "            7.6082e-01, -1.3341e+00]],\n",
            "\n",
            "         [[ 1.6781e-02,  3.7096e-02,  3.2025e-02,  ...,  4.8751e-01,\n",
            "           -6.4926e-01, -8.7856e-02],\n",
            "          [ 1.3052e+00,  2.9530e-01, -3.1905e-02,  ...,  2.4445e-01,\n",
            "            1.3081e+00, -5.4950e-01],\n",
            "          [ 2.6124e+00, -5.2874e-01, -3.9663e-01,  ...,  5.0107e-01,\n",
            "            1.0571e+00, -7.6298e-01],\n",
            "          ...,\n",
            "          [-8.0293e-01,  5.7004e-01,  3.7802e-01,  ...,  3.0293e-01,\n",
            "            9.2413e-01, -5.8888e-01],\n",
            "          [ 2.6650e-02,  6.7380e-01,  5.2941e-01,  ...,  3.3994e-01,\n",
            "            9.6490e-01, -4.3388e-01],\n",
            "          [ 8.2115e-01,  5.1637e-01,  7.1408e-01,  ..., -1.4665e+00,\n",
            "            9.8279e-01, -7.0967e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.1565e-02, -2.9953e-02,  5.2567e-03,  ...,  1.2794e-01,\n",
            "           -6.4730e-01,  1.5135e-01],\n",
            "          [ 1.8842e-01,  8.0413e-02,  6.2515e-01,  ...,  3.3650e-01,\n",
            "            1.9911e+00, -1.0016e-01],\n",
            "          [-1.8088e+00,  6.6584e-01,  4.9008e-01,  ...,  1.8883e+00,\n",
            "            3.7988e-01, -6.2293e-01],\n",
            "          ...,\n",
            "          [ 1.2396e+00, -4.8055e-01, -8.9571e-02,  ...,  1.1030e+00,\n",
            "            2.4795e-02, -3.9128e-01],\n",
            "          [ 4.6282e-02,  1.1197e-01, -5.3768e-01,  ..., -7.2183e-02,\n",
            "           -1.9320e-01,  1.7295e-01],\n",
            "          [-1.4957e+00,  7.5073e-01, -4.8435e-01,  ...,  6.4824e-01,\n",
            "            4.8496e-01, -4.9079e-01]],\n",
            "\n",
            "         [[ 2.2659e-02,  2.2468e-02,  2.6757e-02,  ...,  8.3612e-01,\n",
            "           -2.8893e-01, -2.5595e+00],\n",
            "          [-3.1470e-02,  2.6794e-01,  1.6680e-01,  ...,  1.6561e-01,\n",
            "           -1.7672e+00,  8.5078e+00],\n",
            "          [ 1.4905e+00, -2.0794e-01,  2.3298e-01,  ..., -5.1475e-01,\n",
            "           -1.2914e+00,  7.5065e+00],\n",
            "          ...,\n",
            "          [-1.3459e+00, -1.0102e-01, -2.8493e-01,  ..., -4.6852e-01,\n",
            "           -2.7877e+00,  8.2562e+00],\n",
            "          [-1.0778e+00,  9.8200e-01,  3.2434e-01,  ...,  1.3828e-01,\n",
            "           -1.1976e+00,  7.8211e+00],\n",
            "          [ 6.1761e-01,  1.0902e+00, -2.0438e-01,  ..., -3.6538e-01,\n",
            "           -1.6881e+00,  8.1022e+00]],\n",
            "\n",
            "         [[ 9.0158e-04,  2.4181e-02, -2.7769e-02,  ..., -2.7170e-01,\n",
            "            6.4826e-02,  2.7793e-01],\n",
            "          [ 4.7118e-01, -6.5123e-01, -3.3494e-01,  ...,  2.4528e+00,\n",
            "            2.0165e+00,  1.1506e+00],\n",
            "          [ 1.9458e+00, -6.8427e-02,  1.0592e-02,  ...,  1.8802e+00,\n",
            "           -2.7235e-01,  4.4720e-01],\n",
            "          ...,\n",
            "          [-1.3915e+00, -5.5440e-01,  5.8280e-03,  ..., -2.1705e-01,\n",
            "           -7.8429e-01,  2.3373e+00],\n",
            "          [-1.3856e-01, -4.0690e-01, -1.2711e-01,  ..., -2.8287e-01,\n",
            "           -8.8228e-02,  2.7641e+00],\n",
            "          [ 6.1957e-01, -6.0310e-01, -8.4703e-01,  ...,  2.6360e+00,\n",
            "            5.1310e-02,  5.7837e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.4997e-03,  3.8148e-03, -2.3251e-02,  ...,  1.6549e-02,\n",
            "           -5.0879e-03, -2.0404e-02],\n",
            "          [-3.5663e-01,  2.9457e-01, -1.3341e-01,  ...,  3.0556e-02,\n",
            "           -2.8712e-01, -2.9115e-02],\n",
            "          [ 3.0299e-01,  9.3090e-02, -2.1185e-01,  ..., -1.2538e-01,\n",
            "           -1.6424e-01,  5.7484e-02],\n",
            "          ...,\n",
            "          [-1.3606e-01, -9.0967e-02,  3.1067e-01,  ...,  1.3392e-01,\n",
            "           -7.3015e-02,  4.4938e-02],\n",
            "          [ 3.2232e-02,  2.1685e-01,  1.5496e-01,  ..., -2.3489e-02,\n",
            "            1.7289e-01, -2.7016e-01],\n",
            "          [ 1.2758e-01,  1.0971e-01,  2.8020e-01,  ..., -3.4713e-01,\n",
            "            2.0808e-01, -7.6160e-02]],\n",
            "\n",
            "         [[-1.2209e-02, -4.5223e-02, -5.7760e-02,  ...,  4.7734e-02,\n",
            "           -2.1237e-02, -2.4595e-02],\n",
            "          [-2.7500e-01, -7.1720e-01,  4.5272e-01,  ..., -6.1758e-01,\n",
            "            4.7559e-01,  9.9735e-02],\n",
            "          [ 8.0241e-01, -1.0135e+00,  1.7230e-02,  ..., -1.6574e-01,\n",
            "           -1.3236e-01, -2.7848e-01],\n",
            "          ...,\n",
            "          [ 5.6548e-01, -8.8662e-01,  2.5522e-01,  ...,  5.2958e-01,\n",
            "           -1.0983e-01,  1.5052e-01],\n",
            "          [-3.4566e-01, -1.6399e-01,  1.2575e-01,  ...,  3.7308e-02,\n",
            "            2.9591e-01,  3.8527e-01],\n",
            "          [ 1.2309e-01, -7.2738e-01,  5.8289e-01,  ..., -1.8078e-01,\n",
            "            3.9438e-01,  7.3345e-02]],\n",
            "\n",
            "         [[-5.6297e-02,  8.5127e-03, -2.4855e-02,  ...,  1.6767e-02,\n",
            "            2.2313e-02,  1.2067e-02],\n",
            "          [ 2.9520e-01,  1.7057e-01, -1.4673e-01,  ..., -1.8966e-01,\n",
            "            2.8646e-01,  3.0202e-02],\n",
            "          [ 3.0259e-01, -2.0543e-02,  4.2295e-01,  ..., -3.5987e-01,\n",
            "            7.6511e-01,  4.1455e-01],\n",
            "          ...,\n",
            "          [ 7.3599e-01, -2.1408e-01,  7.5543e-02,  ...,  2.6426e-01,\n",
            "            2.7214e-02, -2.6223e-01],\n",
            "          [-1.3332e-01,  3.7524e-02, -8.8198e-02,  ..., -2.6681e-01,\n",
            "           -1.6431e-01,  1.7356e-01],\n",
            "          [ 6.6803e-01,  4.7851e-01, -3.9659e-02,  ..., -3.9802e-01,\n",
            "            3.5817e-01, -4.3626e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.6881e-04,  9.1757e-03,  6.2024e-02,  ...,  1.6131e-02,\n",
            "           -3.7634e-02, -4.0937e-02],\n",
            "          [-7.5507e-01,  1.4935e-01, -8.9203e-01,  ..., -4.4553e-02,\n",
            "            1.4687e-01,  1.9850e-01],\n",
            "          [-2.8406e-01, -2.0134e-01,  4.1913e-01,  ...,  4.0280e-01,\n",
            "           -3.8468e-01, -2.4082e-01],\n",
            "          ...,\n",
            "          [-9.0153e-02, -8.7994e-02,  4.2864e-02,  ...,  5.3291e-01,\n",
            "           -3.6140e-01,  5.7286e-01],\n",
            "          [ 8.3503e-02, -6.6733e-02,  1.8718e-01,  ..., -6.4119e-02,\n",
            "           -1.9473e-01, -1.0893e-01],\n",
            "          [-2.3055e-01, -4.5841e-01,  4.9221e-01,  ..., -8.7144e-02,\n",
            "           -2.3140e-01,  3.0255e-01]],\n",
            "\n",
            "         [[-2.1388e-02, -4.0294e-03,  6.2368e-04,  ..., -1.9970e-03,\n",
            "           -4.9785e-03, -3.2398e-03],\n",
            "          [ 1.1281e-01,  1.4155e-02,  6.6493e-01,  ..., -6.2001e-01,\n",
            "           -2.2495e-01, -9.0529e-01],\n",
            "          [ 1.9550e-01,  2.9667e-01,  4.7144e-01,  ..., -3.4673e-01,\n",
            "            3.5629e-01, -9.0113e-01],\n",
            "          ...,\n",
            "          [ 3.9616e-02, -1.6030e-02,  1.6519e-01,  ..., -5.9542e-01,\n",
            "           -5.4161e-02, -1.1694e+00],\n",
            "          [ 2.7693e-01,  4.6279e-01,  3.4360e-01,  ...,  1.0180e-01,\n",
            "           -6.2602e-02, -8.3422e-01],\n",
            "          [-1.2262e-02,  4.2985e-01,  8.4843e-01,  ...,  3.5380e-01,\n",
            "           -5.2398e-01, -9.4407e-01]],\n",
            "\n",
            "         [[ 6.9540e-03, -1.0126e-02, -2.0224e-04,  ...,  1.4938e-02,\n",
            "            4.0042e-02,  4.2641e-03],\n",
            "          [ 1.8023e-02, -3.4163e-01, -8.0552e-01,  ..., -2.9411e-01,\n",
            "           -2.1077e-02, -2.1318e-01],\n",
            "          [-4.2023e-01, -1.5968e-02, -9.8632e-01,  ...,  1.0663e-02,\n",
            "            1.4568e-01,  3.4872e-02],\n",
            "          ...,\n",
            "          [ 8.0076e-02, -1.4801e-01, -6.6663e-01,  ..., -1.4257e-01,\n",
            "           -2.9352e-02,  8.8024e-02],\n",
            "          [ 2.2927e-01,  1.9314e-02, -5.3620e-01,  ..., -1.7316e-02,\n",
            "           -4.5839e-01,  1.5019e-01],\n",
            "          [ 1.1926e-02, -4.9890e-02, -7.4832e-01,  ...,  1.5537e-02,\n",
            "            8.7114e-02,  2.4172e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 9.3341e-03, -1.3436e-02,  8.6994e-03,  ..., -3.8764e-01,\n",
            "            2.9346e-01, -1.9686e-01],\n",
            "          [-4.7586e-01,  7.4265e-01,  7.3045e-02,  ...,  4.0531e-01,\n",
            "           -1.5644e+00,  8.5329e-01],\n",
            "          [-1.1285e+00,  9.3525e-01,  2.0544e-01,  ...,  1.0170e+00,\n",
            "           -1.4786e+00,  2.9623e-01],\n",
            "          ...,\n",
            "          [ 4.2506e-01, -1.0028e+00, -2.4747e-01,  ...,  2.0826e+00,\n",
            "           -2.9595e+00, -4.3522e-01],\n",
            "          [ 3.7833e-01, -1.1837e-01,  5.2360e-01,  ...,  1.9574e+00,\n",
            "           -8.8116e-01, -6.7321e-01],\n",
            "          [-8.4446e-01,  2.5931e-01, -2.2554e-01,  ...,  1.6506e+00,\n",
            "           -5.2363e-01, -7.0831e-01]],\n",
            "\n",
            "         [[ 7.6091e-03,  7.0046e-03, -1.3976e-02,  ..., -6.4478e-01,\n",
            "           -8.4483e-01, -3.4582e-01],\n",
            "          [ 9.1397e-01,  1.5709e-01,  4.5573e-01,  ..., -1.2548e-01,\n",
            "            1.4452e-01, -1.6904e+00],\n",
            "          [ 1.8823e+00,  1.0704e+00,  5.3053e-01,  ...,  9.0812e-01,\n",
            "            3.3330e+00,  8.8531e-02],\n",
            "          ...,\n",
            "          [-1.1616e+00, -1.1989e+00, -1.0303e+00,  ...,  9.2978e-01,\n",
            "            3.5455e+00, -8.3899e-01],\n",
            "          [ 6.8497e-01, -3.0196e-01, -4.0308e-01,  ..., -1.2689e-01,\n",
            "            1.2184e+00, -3.3439e-01],\n",
            "          [ 9.6918e-01, -7.0112e-02, -3.9022e-01,  ...,  5.6983e-01,\n",
            "            2.5051e+00, -1.9112e+00]],\n",
            "\n",
            "         [[ 7.8575e-03,  2.0852e-02, -8.3178e-03,  ..., -2.8046e-01,\n",
            "            2.2666e-02,  3.7848e-01],\n",
            "          [-2.3853e+00, -2.2388e-01,  7.1091e-01,  ..., -9.1410e-01,\n",
            "            2.8915e+00,  1.6968e-02],\n",
            "          [-2.9149e+00, -2.3840e-01,  4.6363e-01,  ..., -2.6297e+00,\n",
            "            1.7515e+00,  2.0467e+00],\n",
            "          ...,\n",
            "          [ 7.3168e-01,  8.1681e-02, -3.8156e-01,  ...,  3.8654e-01,\n",
            "            1.9500e+00,  2.3903e+00],\n",
            "          [-1.0806e+00, -5.0886e-01, -1.2668e-01,  ..., -3.4794e-01,\n",
            "            1.4716e+00,  5.2247e-03],\n",
            "          [-1.1209e+00, -4.0191e-02, -6.4248e-02,  ..., -2.1028e+00,\n",
            "            2.4241e+00, -1.6681e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.3894e-02,  9.7477e-05, -1.1739e-02,  ..., -4.8421e-01,\n",
            "           -1.6734e-01,  4.5367e-01],\n",
            "          [-5.5803e-01, -3.5415e-01,  9.2075e-01,  ...,  7.5997e-01,\n",
            "           -1.5975e+00, -2.8219e+00],\n",
            "          [-2.3207e+00, -1.7340e-01,  1.4491e+00,  ...,  1.8936e+00,\n",
            "           -1.6192e+00, -1.4097e+00],\n",
            "          ...,\n",
            "          [ 1.6592e+00, -1.3876e-01, -1.6229e+00,  ...,  1.8130e+00,\n",
            "           -9.6633e-01, -3.2327e-01],\n",
            "          [ 4.9728e-01,  1.0089e+00, -7.2479e-01,  ...,  1.1913e-01,\n",
            "           -2.1328e+00, -2.4242e+00],\n",
            "          [-1.0991e+00,  2.2811e-01,  2.7039e-01,  ...,  1.7314e+00,\n",
            "           -1.4986e+00, -1.1218e+00]],\n",
            "\n",
            "         [[ 2.2042e-02,  1.9172e-03,  3.8539e-02,  ...,  7.2448e-01,\n",
            "            3.3011e-01, -2.2866e+00],\n",
            "          [ 1.0984e+00,  2.0914e-01,  8.5687e-02,  ..., -2.0294e-01,\n",
            "           -1.9967e-01,  3.5493e+00],\n",
            "          [ 2.2962e+00,  1.8294e-02, -2.9750e-01,  ...,  9.4346e-01,\n",
            "            3.3519e+00,  4.9420e+00],\n",
            "          ...,\n",
            "          [-1.3706e+00, -7.1612e-02,  2.0609e-01,  ...,  5.1618e-01,\n",
            "            3.4921e+00,  5.5228e+00],\n",
            "          [-1.4226e-01, -1.1649e-01, -1.9974e-02,  ...,  5.0489e-02,\n",
            "            2.9893e+00,  5.5254e+00],\n",
            "          [ 1.3814e+00, -2.8093e-01,  5.2678e-01,  ...,  3.9088e-01,\n",
            "            2.8791e+00,  4.1055e+00]],\n",
            "\n",
            "         [[ 4.8651e-03,  1.3861e-02, -5.5520e-03,  ...,  3.2774e-01,\n",
            "           -6.4709e-02, -2.3391e-01],\n",
            "          [ 2.0725e+00, -1.1669e+00,  1.5224e-01,  ...,  7.1869e-02,\n",
            "            2.8874e+00,  1.2427e+00],\n",
            "          [ 1.6298e+00, -1.0098e+00, -9.0850e-01,  ..., -4.3061e-01,\n",
            "            2.4590e+00,  1.5555e+00],\n",
            "          ...,\n",
            "          [ 2.8809e-01,  1.0200e+00,  4.8616e-01,  ..., -1.2721e+00,\n",
            "            2.5731e+00,  1.5842e+00],\n",
            "          [ 1.8335e+00, -4.4457e-01,  1.3724e+00,  ..., -1.6810e-03,\n",
            "            1.9236e+00,  2.8126e+00],\n",
            "          [ 9.2676e-01, -1.4528e+00,  8.1513e-01,  ..., -3.5477e-01,\n",
            "            2.9651e+00,  1.0849e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-1.9573e-02,  1.4711e-02, -1.6125e-02,  ...,  3.0057e-02,\n",
            "            1.1817e-02, -1.1311e-02],\n",
            "          [ 1.1884e-01, -1.7978e-01, -6.9163e-02,  ...,  1.9550e-01,\n",
            "            9.5971e-01,  1.3682e-01],\n",
            "          [ 1.8096e-01, -3.5507e-01,  5.9329e-01,  ..., -5.5393e-01,\n",
            "            2.4163e-01,  1.1387e+00],\n",
            "          ...,\n",
            "          [ 3.0578e-02,  5.9082e-02, -4.1408e-01,  ..., -9.1817e-01,\n",
            "            3.8007e-01,  7.1692e-01],\n",
            "          [ 1.8935e-01, -5.1654e-01, -4.9227e-01,  ..., -1.8260e-01,\n",
            "            6.7630e-01,  5.3523e-01],\n",
            "          [ 6.1449e-01, -8.8903e-01, -4.0526e-02,  ..., -2.8313e-01,\n",
            "            5.5394e-01,  1.0347e+00]],\n",
            "\n",
            "         [[-2.8684e-02,  2.1702e-03, -2.3559e-04,  ...,  6.5071e-03,\n",
            "           -2.8846e-03,  1.4335e-02],\n",
            "          [ 6.9310e-02, -2.9282e-01,  4.4454e-01,  ..., -9.8663e-01,\n",
            "            6.9372e-01,  7.9650e-01],\n",
            "          [ 9.9153e-01,  1.2583e-01, -1.4920e-01,  ...,  1.8084e-01,\n",
            "           -1.6208e-01,  2.6203e-01],\n",
            "          ...,\n",
            "          [ 1.2984e-01,  3.7826e-01, -6.3152e-01,  ..., -9.9536e-01,\n",
            "            1.0739e-01, -5.8846e-01],\n",
            "          [ 1.0871e+00,  5.5801e-01, -4.3513e-01,  ...,  6.5713e-02,\n",
            "           -2.8785e-01, -2.0665e-01],\n",
            "          [ 6.4771e-01,  2.7617e-01, -1.4479e+00,  ..., -4.0433e-01,\n",
            "           -2.8564e-01, -5.0213e-01]],\n",
            "\n",
            "         [[-4.0859e-03,  5.1299e-03, -1.9340e-02,  ..., -1.7127e-03,\n",
            "           -8.9017e-03, -6.3413e-03],\n",
            "          [ 3.9194e-01,  8.5719e-02,  1.9038e-01,  ...,  1.2059e-01,\n",
            "           -3.9253e-01, -2.2247e-03],\n",
            "          [ 1.0684e-01, -3.5119e-02, -9.5631e-02,  ...,  4.0764e-01,\n",
            "           -3.1928e-01, -1.2774e-01],\n",
            "          ...,\n",
            "          [ 2.1367e-02, -2.9514e-01,  4.2737e-01,  ...,  4.6821e-01,\n",
            "           -2.5608e-02, -3.5025e-01],\n",
            "          [ 6.3877e-02, -9.0482e-02,  6.0532e-01,  ...,  2.2330e-01,\n",
            "           -1.9117e-01, -3.4715e-01],\n",
            "          [ 1.0697e-01,  5.0801e-02, -2.4099e-01,  ...,  1.5631e-01,\n",
            "           -3.1212e-01, -5.6882e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4619e-02, -8.4841e-03, -8.7034e-03,  ..., -2.6854e-02,\n",
            "           -1.4177e-02, -1.3900e-02],\n",
            "          [-1.3442e-01,  4.3096e-02,  6.3009e-01,  ..., -8.0247e-01,\n",
            "            1.8532e-01, -7.0905e-01],\n",
            "          [ 5.0725e-01, -1.9638e-01, -3.4707e-01,  ..., -2.1412e-01,\n",
            "            6.2509e-01,  7.2506e-01],\n",
            "          ...,\n",
            "          [ 4.6284e-01,  4.0390e-01,  1.5324e-01,  ..., -1.5224e-01,\n",
            "            1.3772e-01, -2.4815e-01],\n",
            "          [ 4.4823e-01,  2.2304e-01,  4.4122e-02,  ..., -1.5338e-01,\n",
            "            1.6689e-01, -4.8109e-01],\n",
            "          [ 1.9578e-01, -7.3255e-02,  1.3910e-01,  ..., -2.1528e-01,\n",
            "           -6.9439e-01,  1.4943e-01]],\n",
            "\n",
            "         [[ 5.2117e-03, -2.2308e-02, -1.0047e-02,  ..., -8.7659e-03,\n",
            "           -2.5993e-02,  1.3043e-02],\n",
            "          [-1.1159e-01, -1.9590e-01,  9.9693e-01,  ..., -1.0150e-01,\n",
            "           -1.2084e-01, -3.2782e-01],\n",
            "          [-6.8515e-02,  8.9954e-02,  7.7489e-01,  ..., -1.7977e-01,\n",
            "           -6.8366e-01,  7.1188e-01],\n",
            "          ...,\n",
            "          [ 5.8048e-02,  6.9000e-02,  2.5591e-01,  ..., -4.4134e-01,\n",
            "           -5.8860e-01,  1.3853e-01],\n",
            "          [-5.0297e-01, -6.6490e-01, -1.4654e-01,  ..., -1.8679e-01,\n",
            "           -4.2698e-01, -4.4089e-01],\n",
            "          [-2.3330e-02, -3.5233e-02, -2.6284e-01,  ...,  2.2189e-01,\n",
            "           -9.1905e-01,  3.3531e-01]],\n",
            "\n",
            "         [[-1.6068e-02,  2.7271e-02, -1.1664e-01,  ..., -9.3470e-03,\n",
            "           -1.9941e-02, -7.8672e-03],\n",
            "          [ 3.9606e-01,  6.9563e-02,  2.0600e-01,  ..., -3.6221e-01,\n",
            "            2.3425e-01, -5.4077e-02],\n",
            "          [ 2.2840e-01,  1.5914e-01, -1.0177e-02,  ..., -8.3488e-01,\n",
            "           -2.2194e-01, -4.6303e-01],\n",
            "          ...,\n",
            "          [-7.2994e-02,  1.1829e-01, -2.6238e-01,  ..., -8.7363e-01,\n",
            "            3.2420e-02,  1.9254e-01],\n",
            "          [ 2.2094e-01,  3.3354e-01, -1.4087e-01,  ..., -6.2299e-01,\n",
            "            1.4142e-01, -2.5623e-01],\n",
            "          [-3.4472e-01,  6.2281e-02, -1.5465e-01,  ..., -3.8282e-01,\n",
            "           -2.4680e-01,  2.6629e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.8813e-02,  1.6208e-02, -1.6166e-02,  ...,  6.3069e-01,\n",
            "           -5.0050e-01, -6.7928e-01],\n",
            "          [-2.1990e-02, -6.4720e-02,  1.1186e-01,  ..., -2.9053e+00,\n",
            "           -6.1805e-01,  7.6810e-01],\n",
            "          [ 1.1466e-01, -7.4550e-02, -3.0865e-01,  ..., -2.8720e+00,\n",
            "           -1.0637e+00,  1.1142e+00],\n",
            "          ...,\n",
            "          [-3.7075e-01, -5.4132e-01,  9.7863e-03,  ..., -3.9609e+00,\n",
            "           -5.5254e-01,  1.8515e+00],\n",
            "          [-2.8516e-01,  8.2249e-02,  1.7409e-01,  ..., -3.4598e+00,\n",
            "           -3.2507e-01,  1.3768e+00],\n",
            "          [ 5.2182e-01, -1.3607e-01, -8.6167e-02,  ..., -3.3658e+00,\n",
            "            2.3387e-01,  1.0577e+00]],\n",
            "\n",
            "         [[-1.5996e-02, -8.4586e-03, -2.1182e-02,  ...,  2.6422e-02,\n",
            "           -1.6038e-01, -1.0833e-01],\n",
            "          [ 7.6732e-01,  8.2962e-01, -8.7250e-01,  ...,  4.0904e-01,\n",
            "           -7.6911e-01,  8.7436e-01],\n",
            "          [-1.4082e-01,  2.3301e-02,  3.1243e-01,  ..., -5.8287e-01,\n",
            "            1.3032e-01,  1.0246e+00],\n",
            "          ...,\n",
            "          [ 5.9066e-01, -3.0616e-01,  1.9651e-01,  ..., -1.1984e+00,\n",
            "           -1.2837e+00,  8.8243e-01],\n",
            "          [ 1.7879e+00,  2.3586e-01,  2.9571e-01,  ...,  1.8797e-01,\n",
            "           -1.1312e+00,  6.6734e-01],\n",
            "          [ 1.0412e+00, -4.9825e-02,  8.1845e-01,  ..., -6.7781e-01,\n",
            "           -1.6661e-01,  2.4460e+00]],\n",
            "\n",
            "         [[ 2.8105e-02,  1.8760e-02, -3.2340e-03,  ..., -5.7654e-01,\n",
            "            1.2424e-01,  3.4273e-01],\n",
            "          [ 3.5356e-01, -3.7856e-01,  1.3641e-01,  ...,  2.0952e+00,\n",
            "            5.1207e-01, -9.4711e-01],\n",
            "          [ 2.9740e+00, -7.2928e-02,  7.4931e-01,  ..., -4.0939e-01,\n",
            "           -1.3923e+00, -1.1522e+00],\n",
            "          ...,\n",
            "          [-1.7007e+00,  2.3666e-01, -6.5592e-01,  ..., -9.4473e-01,\n",
            "           -4.8503e-01, -1.5043e+00],\n",
            "          [-7.9149e-02, -1.5128e-01, -2.2740e-01,  ..., -4.8469e-02,\n",
            "           -7.4917e-01, -1.6293e+00],\n",
            "          [ 1.3308e+00, -1.2193e-01, -5.3922e-01,  ...,  4.7553e-01,\n",
            "           -5.9998e-01, -1.4474e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.1456e-03,  1.7283e-02,  2.0115e-03,  ...,  5.6581e-02,\n",
            "           -9.9465e-02, -1.5120e-01],\n",
            "          [ 1.1402e+00, -4.4571e-01, -3.4993e-01,  ..., -5.6791e-01,\n",
            "           -2.4287e-01,  6.6995e-01],\n",
            "          [-2.4962e-01, -1.1387e+00, -1.9252e+00,  ..., -5.3989e-01,\n",
            "           -3.3795e-01,  2.1625e+00],\n",
            "          ...,\n",
            "          [ 7.4361e-01,  8.2433e-01,  1.3581e+00,  ..., -5.7729e-01,\n",
            "           -6.9142e-01,  1.9620e+00],\n",
            "          [ 1.8800e+00, -2.5401e-01,  9.9205e-01,  ..., -5.2105e-01,\n",
            "           -6.2860e-01,  2.5924e+00],\n",
            "          [ 2.7132e-01, -1.0547e+00,  1.0215e-01,  ..., -1.0313e-01,\n",
            "           -2.5934e-01,  1.9076e+00]],\n",
            "\n",
            "         [[ 1.5910e-02, -4.2941e-03,  8.0370e-04,  ...,  2.1630e-01,\n",
            "           -1.3231e-01, -4.3036e-02],\n",
            "          [-1.7291e-01,  7.1276e-01, -7.5083e-01,  ..., -8.2794e-01,\n",
            "            1.0201e+00, -1.5954e+00],\n",
            "          [-2.1224e+00,  8.0178e-01, -2.2404e-01,  ..., -2.5294e-01,\n",
            "            2.1179e+00, -5.4001e-01],\n",
            "          ...,\n",
            "          [ 1.7767e+00, -1.0683e+00, -5.9252e-02,  ..., -1.1663e-01,\n",
            "            1.4559e+00,  7.8146e-01],\n",
            "          [ 5.2567e-01, -6.2552e-01, -2.0985e-01,  ..., -3.3492e-01,\n",
            "           -3.8042e-01, -1.1910e+00],\n",
            "          [-8.4021e-01, -3.9293e-01,  1.6263e-01,  ..., -9.2978e-01,\n",
            "            2.7242e-01, -1.0203e+00]],\n",
            "\n",
            "         [[ 2.2970e-02,  8.2434e-03, -7.5789e-03,  ...,  1.8964e-01,\n",
            "           -1.3937e+00,  4.7100e-01],\n",
            "          [ 8.6189e-01,  4.1180e-01, -1.0088e-01,  ..., -4.0473e-01,\n",
            "            1.7874e+00, -8.1938e-01],\n",
            "          [ 1.1485e+00, -2.7344e-01,  3.2451e-01,  ..., -9.8419e-01,\n",
            "            2.3980e+00,  6.6706e-01],\n",
            "          ...,\n",
            "          [-2.3236e-02, -7.5137e-02,  1.0392e-01,  ..., -5.2699e-03,\n",
            "            3.9700e+00, -4.6247e-01],\n",
            "          [ 1.9152e+00,  5.7751e-01, -7.7150e-01,  ..., -7.4781e-01,\n",
            "            2.6512e+00, -1.7184e+00],\n",
            "          [ 1.3499e+00,  4.7719e-01, -1.5001e-01,  ...,  3.4227e-01,\n",
            "            1.9629e+00,  9.9803e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-4.6958e-03,  3.3492e-01,  7.6157e-03,  ...,  3.3877e-02,\n",
            "            5.2181e-03, -1.6922e-02],\n",
            "          [-1.1836e+00, -1.3108e+00, -7.4464e-01,  ...,  3.5013e-02,\n",
            "           -1.0033e-01,  8.8043e-01],\n",
            "          [-1.2671e+00, -2.6038e-01, -4.4796e-01,  ..., -3.8301e-01,\n",
            "            3.1641e-01, -3.8885e-01],\n",
            "          ...,\n",
            "          [-1.6471e+00, -1.0353e+00, -1.1935e+00,  ..., -4.6445e-01,\n",
            "            9.6324e-01, -9.7402e-02],\n",
            "          [-2.4076e+00, -5.3954e-01, -1.3335e+00,  ..., -6.2494e-02,\n",
            "            4.6263e-01,  4.1961e-01],\n",
            "          [-9.1062e-01, -1.3479e-01, -1.0901e+00,  ..., -1.4924e-01,\n",
            "           -7.7588e-02, -4.0615e-01]],\n",
            "\n",
            "         [[ 5.8355e-03,  8.3123e-03, -3.2599e-02,  ..., -1.1337e-02,\n",
            "           -9.8552e-03, -2.7780e-01],\n",
            "          [ 7.6498e-01, -2.0660e-01, -8.2448e-01,  ..., -7.6268e-01,\n",
            "            3.0543e-01,  8.6906e-01],\n",
            "          [ 2.3028e-01,  8.9017e-02, -9.8438e-02,  ..., -3.2619e-01,\n",
            "           -2.1203e-01,  7.7147e-01],\n",
            "          ...,\n",
            "          [ 4.4696e-03,  4.2156e-01, -9.5897e-02,  ..., -4.8549e-01,\n",
            "            6.7314e-01,  1.1162e+00],\n",
            "          [-2.8966e-02,  5.9453e-01,  8.2254e-02,  ..., -6.9803e-02,\n",
            "            8.6908e-02,  1.2620e+00],\n",
            "          [-4.8753e-01,  2.0944e-02, -7.7142e-01,  ..., -8.0898e-01,\n",
            "            2.2657e-01,  1.1591e+00]],\n",
            "\n",
            "         [[-2.0656e-03, -5.6699e-03, -7.7006e-03,  ...,  1.0817e-02,\n",
            "           -1.7625e-03,  4.8367e-03],\n",
            "          [ 6.9075e-01, -4.7128e-01,  1.9077e-01,  ..., -2.2609e-01,\n",
            "            4.9835e-01,  7.9074e-01],\n",
            "          [ 9.2359e-02, -3.2323e-01,  3.3967e-01,  ...,  1.7292e-02,\n",
            "           -3.1358e-01,  9.1238e-02],\n",
            "          ...,\n",
            "          [ 4.4705e-01, -6.2954e-01,  2.1585e-01,  ...,  1.2586e-01,\n",
            "            1.2017e-01,  8.9582e-01],\n",
            "          [ 6.0660e-02, -5.6496e-01,  7.0671e-01,  ...,  1.9083e-01,\n",
            "           -4.2222e-01, -2.7146e-01],\n",
            "          [ 9.2858e-01, -3.8512e-01, -1.5255e-01,  ...,  5.5820e-01,\n",
            "            1.8071e-01, -7.7008e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7540e-02, -1.4945e-03,  1.6465e-02,  ..., -1.3323e-02,\n",
            "            6.8269e-03, -3.6410e-02],\n",
            "          [ 3.1695e-01, -6.5448e-01, -4.7483e-02,  ...,  8.4911e-02,\n",
            "            5.0998e-01, -2.4605e-01],\n",
            "          [ 6.7902e-01,  7.2098e-02,  3.5488e-01,  ...,  1.0126e+00,\n",
            "            1.4095e-01, -4.2570e-01],\n",
            "          ...,\n",
            "          [ 7.8421e-01,  1.1854e-01,  6.2360e-01,  ...,  9.4032e-01,\n",
            "            6.5513e-01, -6.7242e-01],\n",
            "          [ 1.3284e+00, -5.0949e-02,  8.9929e-02,  ...,  2.1683e-01,\n",
            "           -1.0917e-01, -4.0644e-01],\n",
            "          [ 7.6948e-01,  2.2113e-01,  5.1602e-01,  ...,  2.1519e-01,\n",
            "            1.6913e-01, -3.5802e-01]],\n",
            "\n",
            "         [[-1.0023e-02, -7.0487e-03, -1.5604e-02,  ..., -4.9281e-03,\n",
            "           -3.8321e-01, -2.0990e-03],\n",
            "          [ 4.6411e-01, -1.3405e-01,  2.8729e-01,  ..., -4.2457e-01,\n",
            "            5.4032e-01, -2.7524e-01],\n",
            "          [ 8.4653e-03, -1.8570e-01,  1.0183e-01,  ..., -3.1031e-01,\n",
            "            4.2992e-01,  5.9281e-02],\n",
            "          ...,\n",
            "          [-8.3971e-02, -3.5163e-01,  3.6744e-01,  ..., -2.6588e-01,\n",
            "            4.0694e-02, -1.3732e-01],\n",
            "          [-2.9393e-01, -4.1895e-01,  2.4837e-01,  ..., -2.8362e-01,\n",
            "            3.7887e-01, -2.8177e-01],\n",
            "          [-3.9399e-01,  8.7299e-02,  7.1899e-01,  ..., -4.8452e-01,\n",
            "            7.1787e-01, -3.2399e-01]],\n",
            "\n",
            "         [[-6.0182e-03,  1.7024e-03,  2.5430e-02,  ..., -2.6667e-03,\n",
            "           -2.5776e-04, -2.2333e-03],\n",
            "          [ 3.8851e-01,  3.7485e-01, -5.5233e-01,  ...,  1.6924e-01,\n",
            "           -5.9051e-01,  3.4618e-01],\n",
            "          [ 1.4762e-02,  2.4608e-01,  1.7900e-01,  ...,  4.3570e-02,\n",
            "           -6.2560e-01, -4.5815e-02],\n",
            "          ...,\n",
            "          [-1.6334e-01, -4.4299e-01,  4.0203e-02,  ...,  8.0507e-01,\n",
            "           -4.6194e-01, -7.2684e-02],\n",
            "          [-4.0417e-01, -2.7718e-01, -1.1557e-01,  ...,  9.7602e-01,\n",
            "           -6.0071e-01, -6.8299e-01],\n",
            "          [-3.3726e-02,  2.6695e-01,  9.7486e-02,  ...,  2.7726e-01,\n",
            "           -1.1183e-01, -1.9666e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-2.5508e-02,  2.3299e-05, -3.9556e-02,  ...,  4.1954e-01,\n",
            "            1.0550e-01,  2.3210e-01],\n",
            "          [ 1.1312e+00, -1.0444e+00, -8.0120e-02,  ...,  2.9064e-01,\n",
            "           -5.2573e-02,  4.9910e-01],\n",
            "          [ 2.4279e+00,  3.8938e-02,  6.4649e-01,  ...,  5.1718e-01,\n",
            "            8.2941e-01,  7.2534e-01],\n",
            "          ...,\n",
            "          [-1.2091e+00, -1.0230e-03, -7.6937e-02,  ...,  6.7804e-01,\n",
            "            5.6825e-01,  4.0384e-01],\n",
            "          [ 2.6812e-01, -4.1521e-03, -4.7128e-01,  ...,  1.2900e+00,\n",
            "            6.6988e-01,  9.5584e-01],\n",
            "          [ 7.0087e-01,  5.2390e-01, -7.6339e-01,  ...,  3.2426e-01,\n",
            "            1.1372e+00, -3.9706e-01]],\n",
            "\n",
            "         [[-7.7073e-03,  7.5233e-04, -1.5608e-02,  ...,  2.2992e-01,\n",
            "            4.3669e-01,  6.8931e-01],\n",
            "          [-4.2570e-01,  4.3067e-01,  3.0199e-01,  ..., -5.4418e-01,\n",
            "           -5.2103e-01, -1.2867e+00],\n",
            "          [ 6.5583e-01,  5.6803e-01, -7.5185e-02,  ..., -1.1006e+00,\n",
            "            4.4788e-01,  4.8742e-01],\n",
            "          ...,\n",
            "          [-7.6032e-01, -3.4275e-01, -1.1058e-01,  ..., -1.6600e+00,\n",
            "           -4.2227e-01, -1.5018e+00],\n",
            "          [-9.0157e-02,  2.1956e-01,  2.1403e-01,  ..., -1.3229e+00,\n",
            "           -8.5488e-01, -8.0786e-01],\n",
            "          [ 2.1135e-01,  8.8476e-01,  2.3277e-01,  ..., -1.4923e-01,\n",
            "           -3.5527e-02, -5.7512e-01]],\n",
            "\n",
            "         [[-5.7359e-03,  3.1969e-02, -1.3369e-02,  ..., -5.6318e-02,\n",
            "            1.0831e-01,  3.6578e-01],\n",
            "          [-1.6534e+00, -3.3070e-01,  1.8249e-01,  ...,  1.2237e+00,\n",
            "            7.3297e-01,  5.0824e-02],\n",
            "          [-1.7567e+00, -8.4905e-01,  6.4866e-01,  ...,  1.2269e-02,\n",
            "           -7.9110e-01,  1.5091e+00],\n",
            "          ...,\n",
            "          [ 1.2503e+00, -1.3255e-01, -1.2812e+00,  ...,  7.3399e-01,\n",
            "           -1.1862e+00,  2.4605e+00],\n",
            "          [ 2.1041e-01,  2.2440e-01,  5.8000e-02,  ...,  1.0420e+00,\n",
            "           -8.1156e-01,  1.1433e+00],\n",
            "          [-1.0382e+00, -3.4409e-02,  1.1648e+00,  ...,  7.8177e-01,\n",
            "           -1.3070e+00,  3.1200e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.7931e-03, -5.3756e-03, -2.7593e-03,  ..., -1.1355e-02,\n",
            "           -3.1683e-01, -5.3637e-01],\n",
            "          [ 2.9021e-01, -8.4826e-01, -9.1443e-01,  ...,  1.6723e-01,\n",
            "            2.4380e+00, -4.3470e-01],\n",
            "          [ 5.2508e-01,  6.6355e-01, -8.3212e-02,  ..., -5.5448e-01,\n",
            "            4.3131e-01,  1.6680e+00],\n",
            "          ...,\n",
            "          [-3.3752e-01, -1.6001e-01, -1.5701e-02,  ...,  4.4146e-01,\n",
            "            7.1874e-01,  1.1561e+00],\n",
            "          [-9.5315e-02, -5.4813e-01, -5.7579e-01,  ...,  9.9928e-02,\n",
            "            6.1572e-01,  1.5387e+00],\n",
            "          [ 5.9979e-01, -3.2228e-01, -5.8871e-01,  ...,  3.2310e-01,\n",
            "            6.9954e-01,  7.5469e-01]],\n",
            "\n",
            "         [[ 3.3136e-02, -2.4939e-02, -1.5835e-02,  ..., -7.6888e-02,\n",
            "           -1.6627e+00, -5.5969e-02],\n",
            "          [ 1.8699e-01, -1.0036e+00,  8.1153e-02,  ..., -1.2341e+00,\n",
            "            3.1730e+00,  9.2743e-01],\n",
            "          [ 1.4975e+00, -4.1591e-01, -8.8947e-02,  ..., -2.3245e+00,\n",
            "            4.0518e+00,  7.9600e-01],\n",
            "          ...,\n",
            "          [-4.3565e-01,  2.8671e-01,  6.4167e-02,  ..., -3.3140e-02,\n",
            "            5.9755e+00, -1.1855e+00],\n",
            "          [ 4.6511e-01, -3.9733e-01,  4.1154e-02,  ...,  8.4219e-01,\n",
            "            6.0649e+00,  4.5936e-01],\n",
            "          [ 9.2585e-01, -7.0432e-01,  5.8831e-01,  ..., -9.9000e-01,\n",
            "            6.0028e+00,  1.1745e+00]],\n",
            "\n",
            "         [[-1.3917e-02,  2.0697e-02,  2.0311e-02,  ...,  1.5434e-01,\n",
            "            6.8406e-01,  1.4250e-01],\n",
            "          [ 1.0849e+00,  3.6623e-01,  6.7073e-01,  ..., -1.2660e+00,\n",
            "           -2.6562e+00, -1.6278e+00],\n",
            "          [-4.8270e-01,  7.1949e-01,  3.3592e-01,  ..., -2.4060e+00,\n",
            "           -4.0643e+00, -8.0912e-01],\n",
            "          ...,\n",
            "          [ 1.0257e+00,  2.1263e-01, -1.3219e-04,  ..., -2.1510e+00,\n",
            "           -2.3907e+00, -1.0261e+00],\n",
            "          [ 1.5526e+00,  3.7058e-03,  7.3255e-01,  ..., -1.8266e+00,\n",
            "           -2.9972e+00, -8.4681e-01],\n",
            "          [ 5.2221e-01,  2.8989e-01,  7.1402e-01,  ..., -8.7243e-01,\n",
            "           -2.7984e+00, -9.5165e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.1090e-02,  4.5664e-04,  2.4338e-02,  ..., -1.0270e-02,\n",
            "            7.1159e-03,  8.2352e-03],\n",
            "          [ 3.4288e-01,  3.1161e-01,  2.9647e-01,  ...,  6.8388e-01,\n",
            "           -2.1938e-01,  7.6719e-01],\n",
            "          [-8.9366e-01, -1.9352e-01,  9.3545e-01,  ...,  4.7725e-01,\n",
            "            2.1463e-02, -2.3277e-01],\n",
            "          ...,\n",
            "          [-7.0865e-01,  3.8030e-01,  4.4930e-01,  ...,  6.0154e-01,\n",
            "            4.7225e-02,  1.6319e-01],\n",
            "          [-9.6223e-01,  1.1687e+00,  4.8112e-01,  ...,  6.5235e-01,\n",
            "           -2.2640e-01,  3.2803e-01],\n",
            "          [-2.7421e-01,  5.8727e-01,  1.9854e-01,  ...,  1.0235e+00,\n",
            "           -2.7096e-01, -3.1614e-01]],\n",
            "\n",
            "         [[-6.8083e-04, -3.2508e-03, -7.4611e-03,  ..., -2.4651e-03,\n",
            "           -9.8622e-03,  1.3997e-02],\n",
            "          [ 6.5183e-01, -2.0824e-01,  7.7360e-01,  ...,  5.9815e-02,\n",
            "            8.3892e-02, -1.3179e-01],\n",
            "          [-1.3424e-01, -4.4198e-01,  5.8617e-01,  ...,  3.6219e-01,\n",
            "            1.0062e+00, -3.4406e-01],\n",
            "          ...,\n",
            "          [-4.5545e-01, -2.4216e-01,  5.2707e-02,  ..., -4.1454e-01,\n",
            "            6.6490e-01, -4.6879e-01],\n",
            "          [ 4.0425e-01, -4.8456e-02, -3.8980e-01,  ..., -1.1632e+00,\n",
            "            1.1462e+00, -3.6473e-01],\n",
            "          [-1.2521e-02, -3.1336e-01,  4.8110e-01,  ..., -9.6886e-01,\n",
            "            1.4101e-01, -2.2746e-01]],\n",
            "\n",
            "         [[-3.0759e-03,  1.9059e-02,  3.6825e-02,  ...,  3.8655e-02,\n",
            "           -9.3143e-03, -3.4024e-02],\n",
            "          [ 1.1646e+00, -4.8173e-01, -2.2356e-01,  ..., -1.2956e+00,\n",
            "            2.1427e+00,  1.0062e+00],\n",
            "          [ 1.6013e-01, -5.7675e-01, -2.4048e-01,  ..., -4.4225e-02,\n",
            "            2.0716e-01,  2.4817e-01],\n",
            "          ...,\n",
            "          [ 2.3436e-01, -7.1909e-01, -2.0267e-01,  ..., -1.1502e-01,\n",
            "           -4.1663e-01,  6.0860e-01],\n",
            "          [-5.0185e-01, -8.0622e-01, -1.4050e+00,  ..., -5.9486e-01,\n",
            "           -3.7948e-01,  3.9574e-01],\n",
            "          [ 5.9966e-02,  1.1588e-01,  4.4652e-01,  ..., -2.4290e-01,\n",
            "            1.9820e-01,  9.4750e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.0697e-03,  3.9220e-03, -1.3323e-02,  ..., -8.9451e-03,\n",
            "            6.5626e-03,  1.4055e-04],\n",
            "          [-1.0302e-01, -7.4505e-01, -4.9454e-01,  ...,  7.7507e-01,\n",
            "            4.5264e-01, -2.4266e-01],\n",
            "          [ 3.4958e-01, -2.9079e-01, -3.7512e-01,  ...,  6.3278e-01,\n",
            "            8.6744e-01,  2.3789e-01],\n",
            "          ...,\n",
            "          [ 5.4584e-01, -2.6973e-01, -3.8942e-01,  ...,  9.2438e-01,\n",
            "            1.1240e+00,  4.5954e-01],\n",
            "          [ 1.9475e-01, -3.1168e-01, -1.9365e-01,  ...,  1.9607e-02,\n",
            "           -4.8050e-02, -2.6772e-01],\n",
            "          [ 6.4715e-01, -3.0322e-02, -6.8758e-01,  ...,  3.6146e-01,\n",
            "            6.0120e-01,  3.5312e-01]],\n",
            "\n",
            "         [[-4.1365e-03,  8.3607e-03, -4.3169e-03,  ..., -2.9089e-04,\n",
            "            1.0377e-02, -4.9637e-03],\n",
            "          [ 2.2648e-01, -4.5568e-01, -1.1713e+00,  ..., -2.2747e-01,\n",
            "           -7.8854e-01,  4.0972e-01],\n",
            "          [-1.0645e+00, -6.5259e-01, -1.0326e-02,  ..., -9.2949e-02,\n",
            "            4.6024e-01,  3.6208e-01],\n",
            "          ...,\n",
            "          [-1.1553e-01,  1.1974e-01, -2.2745e-01,  ...,  2.3318e-01,\n",
            "            1.1978e+00,  9.3188e-02],\n",
            "          [-2.1397e-01,  2.8702e-01, -5.0551e-01,  ...,  5.4369e-01,\n",
            "           -1.9201e-01, -8.4513e-01],\n",
            "          [-4.8857e-01, -9.2303e-02,  4.7325e-01,  ...,  2.6985e-01,\n",
            "            2.0611e-01,  4.8599e-01]],\n",
            "\n",
            "         [[ 5.1464e-03,  4.2279e-03,  5.3452e-03,  ..., -1.4641e-02,\n",
            "           -8.8742e-03,  8.4063e-03],\n",
            "          [ 5.4814e-01,  4.0333e-01,  5.1851e-01,  ...,  1.0195e-01,\n",
            "            3.5322e-02, -2.1024e-01],\n",
            "          [ 1.5764e-01, -2.7992e-01, -1.6331e-01,  ..., -7.4625e-02,\n",
            "           -4.1008e-01,  2.1306e-01],\n",
            "          ...,\n",
            "          [ 4.5605e-01, -5.2034e-02, -8.0692e-01,  ...,  3.9114e-01,\n",
            "           -1.6914e-01, -4.9755e-01],\n",
            "          [ 3.1549e-01,  8.8714e-03, -3.0072e-01,  ..., -6.1818e-02,\n",
            "           -3.1474e-01, -3.6288e-01],\n",
            "          [ 3.5467e-01, -1.5930e-01, -2.1379e-01,  ..., -5.7501e-01,\n",
            "           -6.9226e-01, -2.0486e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-1.7504e-02,  1.6446e-02,  3.1223e-03,  ..., -2.6133e-01,\n",
            "            5.8192e-01,  8.3399e-01],\n",
            "          [-1.3846e+00, -1.2044e+00,  4.2263e-01,  ...,  1.2936e+00,\n",
            "           -1.5631e-01, -7.6448e-01],\n",
            "          [ 1.4519e-01, -4.2571e-01, -3.4348e-01,  ..., -8.5267e-01,\n",
            "           -1.0850e+00, -7.5550e-01],\n",
            "          ...,\n",
            "          [-1.8087e+00, -1.9948e-01, -5.4779e-01,  ...,  4.6479e-01,\n",
            "            2.8876e-01, -7.8193e-01],\n",
            "          [-2.9265e+00, -9.7105e-01, -8.0398e-01,  ...,  3.2714e-01,\n",
            "           -4.8847e-01, -1.4176e+00],\n",
            "          [-1.5194e-01, -1.8605e-01, -4.4210e-02,  ...,  6.0490e-02,\n",
            "            8.0066e-01, -5.7969e-01]],\n",
            "\n",
            "         [[-1.3880e-02,  3.2801e-03,  2.9618e-03,  ..., -3.7240e-01,\n",
            "            1.1767e-01,  8.5021e-03],\n",
            "          [ 5.2292e-01,  5.0041e-01, -2.9415e-01,  ...,  3.7675e-01,\n",
            "            2.0804e+00,  3.0155e-01],\n",
            "          [ 1.5794e-01, -4.9375e-02, -2.4135e-01,  ..., -2.1514e+00,\n",
            "           -3.1291e-02, -6.7289e-01],\n",
            "          ...,\n",
            "          [ 1.0144e+00,  5.0287e-02,  1.0636e-01,  ..., -1.2507e+00,\n",
            "            5.9131e-01, -1.0402e+00],\n",
            "          [ 1.4201e+00,  1.2214e+00,  1.6022e-02,  ..., -1.4600e+00,\n",
            "            3.0483e-01, -8.2827e-01],\n",
            "          [ 1.0433e+00,  6.7567e-01,  9.5082e-02,  ..., -6.8481e-01,\n",
            "            1.0394e+00, -4.0266e-01]],\n",
            "\n",
            "         [[ 1.7879e-02,  1.7846e-03,  5.0179e-03,  ..., -7.4230e-02,\n",
            "            2.1044e-01, -1.5238e-01],\n",
            "          [-1.0871e+00, -6.9775e-03,  1.6624e-01,  ...,  6.4311e-01,\n",
            "           -1.3286e+00, -4.8801e-01],\n",
            "          [ 1.6116e-01,  9.8778e-02,  2.4292e-01,  ..., -4.8447e-01,\n",
            "           -1.3151e+00, -4.8594e-01],\n",
            "          ...,\n",
            "          [-6.3995e-01, -8.9741e-02,  2.7326e-01,  ..., -8.5960e-01,\n",
            "           -1.3244e+00,  6.9535e-01],\n",
            "          [-3.9989e-01,  2.3457e-01,  4.5537e-01,  ..., -9.3638e-01,\n",
            "           -1.0600e+00, -3.0741e-02],\n",
            "          [ 2.6912e-02,  2.9449e-01,  1.5192e-02,  ..., -2.1441e-01,\n",
            "           -1.1645e+00,  2.7647e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.3914e-03,  5.8514e-03,  1.6941e-02,  ..., -1.3343e-01,\n",
            "           -2.3151e-01,  1.4044e-01],\n",
            "          [-1.5085e-01,  1.0947e+00,  1.2866e+00,  ..., -1.3928e-01,\n",
            "           -2.0661e+00, -4.7585e-01],\n",
            "          [ 8.0303e-01,  6.3711e-01,  3.8820e-01,  ...,  1.4722e-02,\n",
            "           -1.9438e-01,  6.1962e-01],\n",
            "          ...,\n",
            "          [-6.5856e-01, -1.5287e+00, -1.9117e-01,  ...,  3.0144e-01,\n",
            "           -3.2392e-01,  4.0247e-01],\n",
            "          [-1.0698e+00, -4.2835e-01, -3.7182e-01,  ..., -5.3501e-02,\n",
            "            1.4508e-01, -1.8803e-03],\n",
            "          [ 1.0507e-01,  6.9219e-01,  5.1061e-01,  ...,  4.5595e-01,\n",
            "           -9.1824e-02, -6.4789e-01]],\n",
            "\n",
            "         [[-1.1065e-02,  1.3391e-02, -2.5099e-02,  ...,  1.5067e-02,\n",
            "           -1.6717e-01, -1.8759e-01],\n",
            "          [ 7.2658e-01, -6.7802e-02, -6.3003e-01,  ..., -4.1525e-01,\n",
            "           -2.1442e+00,  4.3107e-02],\n",
            "          [-6.3640e-01,  4.5745e-01, -8.8005e-01,  ...,  4.5611e-01,\n",
            "           -3.3171e+00, -7.1661e-02],\n",
            "          ...,\n",
            "          [ 1.2221e+00, -1.1008e+00,  8.9312e-01,  ..., -6.1204e-01,\n",
            "           -2.9160e+00,  1.0883e+00],\n",
            "          [ 9.3176e-01, -3.3264e-01,  8.3549e-01,  ..., -1.9801e-01,\n",
            "           -1.6027e+00,  7.9283e-01],\n",
            "          [-5.4976e-01, -8.1103e-02,  3.4858e-01,  ...,  7.7286e-01,\n",
            "           -1.8519e+00,  4.6356e-01]],\n",
            "\n",
            "         [[ 4.9839e-03, -1.2841e-03,  2.0608e-03,  ...,  3.0498e-01,\n",
            "            3.5594e-01,  1.4511e-01],\n",
            "          [ 1.7048e+00,  7.4845e-01, -2.0098e-01,  ...,  9.9158e-01,\n",
            "           -3.4827e-01,  2.1506e+00],\n",
            "          [ 1.7201e+00, -4.6333e-01, -5.8100e-01,  ..., -3.0454e-02,\n",
            "           -1.9532e+00,  4.2223e+00],\n",
            "          ...,\n",
            "          [ 9.0149e-01,  1.7867e-01,  4.6334e-01,  ...,  1.5047e+00,\n",
            "           -1.4025e+00,  3.6068e+00],\n",
            "          [ 1.7243e+00,  1.1920e-02,  5.3889e-01,  ...,  2.5802e+00,\n",
            "           -1.2969e-01,  2.8405e+00],\n",
            "          [ 8.2812e-01,  8.2888e-01,  2.2000e-03,  ..., -8.0185e-01,\n",
            "           -1.4914e+00,  3.3841e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.0687e-02,  5.1140e-03,  1.9290e-02,  ..., -1.2001e-02,\n",
            "           -8.1271e-03, -7.4487e-03],\n",
            "          [-6.4680e-01,  5.8861e-01,  2.9893e-01,  ...,  2.7733e-01,\n",
            "           -4.0475e-01, -5.6782e-01],\n",
            "          [-1.4678e-01,  7.8614e-01, -3.5921e-01,  ...,  4.2115e-01,\n",
            "           -8.0904e-02, -3.7300e-01],\n",
            "          ...,\n",
            "          [-4.1850e-01,  4.5261e-01,  8.4748e-01,  ...,  1.4266e-02,\n",
            "           -3.5813e-01, -2.5710e-01],\n",
            "          [ 3.3135e-01,  3.2412e-01,  5.6001e-01,  ..., -3.2936e-01,\n",
            "           -4.7573e-01,  3.7091e-01],\n",
            "          [-4.2096e-01,  2.2365e-01,  3.8052e-01,  ...,  6.6868e-01,\n",
            "           -6.7310e-01, -4.4853e-01]],\n",
            "\n",
            "         [[-3.6918e-03, -2.3568e-02,  8.2465e-03,  ..., -1.1955e-03,\n",
            "           -3.4554e-02, -1.0214e-02],\n",
            "          [-1.0428e+00,  4.6891e-01,  2.8005e-01,  ..., -3.2731e-01,\n",
            "           -4.3285e-02,  9.9282e-01],\n",
            "          [-8.3995e-01,  2.9299e-01,  1.6339e-01,  ...,  3.2582e-01,\n",
            "            4.2940e-01,  6.4603e-01],\n",
            "          ...,\n",
            "          [-3.4151e-01,  1.5219e-01,  1.3184e-01,  ..., -3.9266e-01,\n",
            "            8.3564e-01, -2.8456e-01],\n",
            "          [-5.5122e-01,  2.3818e-01, -8.8671e-02,  ..., -1.8963e-01,\n",
            "            5.0869e-01, -3.0435e-02],\n",
            "          [ 5.9464e-02, -1.2217e-01,  9.2881e-02,  ..., -1.0152e+00,\n",
            "            2.8888e-01, -8.9816e-01]],\n",
            "\n",
            "         [[-5.6188e-05, -8.3982e-03, -3.2065e-02,  ...,  6.0974e-02,\n",
            "           -6.3131e-03, -7.6487e-03],\n",
            "          [ 6.4457e-01, -2.8238e-01, -3.8081e-01,  ..., -2.8509e-02,\n",
            "           -5.0582e-01,  2.1334e-01],\n",
            "          [ 6.6077e-01,  3.7774e-02,  7.8522e-02,  ..., -5.4436e-01,\n",
            "           -4.6817e-01,  1.3114e-01],\n",
            "          ...,\n",
            "          [ 4.5075e-01,  5.5052e-01, -4.2667e-02,  ..., -2.7274e-01,\n",
            "           -7.7416e-01,  4.4526e-01],\n",
            "          [ 7.5770e-01,  6.5722e-01,  8.8879e-02,  ..., -5.0317e-01,\n",
            "           -3.6054e-01,  4.7650e-01],\n",
            "          [-3.6800e-02,  2.9409e-01,  2.2589e-01,  ..., -1.4980e-01,\n",
            "           -8.0537e-01,  4.9502e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4385e-02,  1.4104e-02,  3.8417e-02,  ...,  6.0848e-03,\n",
            "           -2.9791e-02, -1.4894e-02],\n",
            "          [-9.3554e-02, -2.5209e-02,  7.4058e-01,  ...,  2.3221e-01,\n",
            "            2.4304e-03, -3.9563e-01],\n",
            "          [ 3.1806e-01,  4.4874e-01,  1.4377e-01,  ...,  2.4505e-02,\n",
            "           -2.8578e-01, -5.4235e-02],\n",
            "          ...,\n",
            "          [ 6.9373e-01,  3.9430e-01,  2.3744e-01,  ..., -1.8648e-01,\n",
            "            5.0432e-01,  5.3925e-01],\n",
            "          [-1.2061e-01,  2.5386e-01,  6.0459e-01,  ..., -2.8664e-01,\n",
            "            6.7802e-02, -1.1506e-02],\n",
            "          [ 7.2352e-01,  7.4216e-01, -1.5155e-01,  ..., -3.0891e-01,\n",
            "            1.0182e+00,  2.8511e-01]],\n",
            "\n",
            "         [[ 1.9858e-03, -7.0634e-05,  2.4428e-04,  ...,  2.1975e-02,\n",
            "            1.6336e-03,  1.2100e-02],\n",
            "          [-8.2638e-02,  8.4691e-01,  1.3146e-01,  ..., -4.0528e-02,\n",
            "           -3.0137e-01,  2.1754e-01],\n",
            "          [-9.9333e-01,  5.7252e-01,  4.3874e-01,  ...,  2.1790e-01,\n",
            "           -6.0310e-01,  3.4466e-01],\n",
            "          ...,\n",
            "          [-6.4590e-01,  5.3627e-02, -6.6664e-01,  ...,  2.7564e-01,\n",
            "           -7.5719e-01,  7.3482e-01],\n",
            "          [-1.5449e-01,  2.5003e-01,  1.0444e+00,  ...,  1.8967e-01,\n",
            "           -4.2146e-01, -1.7018e-01],\n",
            "          [-8.4920e-01, -1.3499e-01, -3.7077e-01,  ...,  8.7300e-01,\n",
            "           -7.4620e-02, -2.3231e-03]],\n",
            "\n",
            "         [[ 9.6878e-03, -4.0161e-03,  7.6800e-03,  ..., -5.3835e-03,\n",
            "           -4.1271e-03, -3.3618e-03],\n",
            "          [ 2.1693e-01, -2.0528e-01,  5.3924e-01,  ...,  1.9001e-01,\n",
            "            6.0374e-01, -1.6083e-03],\n",
            "          [-5.8389e-01,  3.4296e-02,  1.3586e-01,  ..., -5.9370e-01,\n",
            "            5.7366e-01,  4.2392e-01],\n",
            "          ...,\n",
            "          [-9.7596e-01,  5.0131e-01,  3.9584e-01,  ..., -7.3742e-01,\n",
            "            5.8267e-01,  6.2747e-01],\n",
            "          [-8.9426e-01,  5.5306e-01,  7.1343e-01,  ..., -1.4417e-01,\n",
            "            5.2901e-01,  6.7984e-01],\n",
            "          [-5.5056e-01,  2.4216e-01,  7.2539e-01,  ..., -6.3792e-01,\n",
            "            4.3740e-01,  6.8799e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-7.2242e-02,  1.0197e-02, -3.5058e-02,  ..., -1.2233e-01,\n",
            "            2.4606e-01, -1.3719e+00],\n",
            "          [ 6.6664e-02, -4.4040e-01, -2.2224e-02,  ...,  7.3444e-01,\n",
            "           -2.5700e+00,  3.7286e+00],\n",
            "          [ 3.4192e-01, -4.3936e-01, -2.0983e-01,  ..., -4.4856e-01,\n",
            "           -1.5909e+00,  2.2665e+00],\n",
            "          ...,\n",
            "          [ 8.3011e-02,  4.2179e-01,  1.2193e-01,  ..., -4.9990e-01,\n",
            "           -3.0930e+00,  2.6297e+00],\n",
            "          [ 8.0451e-01,  5.0644e-01,  3.0126e-01,  ..., -1.8947e-01,\n",
            "           -3.6137e+00,  3.6433e+00],\n",
            "          [ 6.7441e-01, -4.8582e-02,  4.0011e-01,  ...,  6.1056e-01,\n",
            "           -1.0887e+00,  2.8922e+00]],\n",
            "\n",
            "         [[ 3.3234e-02, -6.3893e-03,  2.5068e-02,  ..., -5.9368e-02,\n",
            "            1.9833e+00, -2.5352e-02],\n",
            "          [-3.9899e-01,  4.7944e-03,  2.1795e-01,  ...,  2.9732e-01,\n",
            "           -4.4449e+00, -3.8171e-01],\n",
            "          [-5.0573e-01,  5.5566e-01, -2.6833e-01,  ...,  1.2914e+00,\n",
            "           -3.7579e+00,  1.1649e+00],\n",
            "          ...,\n",
            "          [ 1.8138e-01, -2.1381e-02,  3.0451e-01,  ...,  1.9409e+00,\n",
            "           -5.0234e+00,  2.0679e+00],\n",
            "          [ 1.3932e-01,  5.6111e-01,  3.0225e-01,  ...,  2.5924e+00,\n",
            "           -5.7424e+00,  9.2104e-01],\n",
            "          [-5.2227e-01,  2.9601e-01,  2.1547e-01,  ...,  6.6861e-01,\n",
            "           -4.7054e+00,  3.8759e-01]],\n",
            "\n",
            "         [[-1.9225e-02, -2.7622e-02, -2.0134e-04,  ...,  3.2109e-02,\n",
            "           -6.3875e-01,  6.5302e-01],\n",
            "          [-2.3128e+00, -3.7757e-01, -7.7438e-01,  ...,  9.9565e-01,\n",
            "           -1.6177e+00, -1.3826e+00],\n",
            "          [-1.9425e+00, -7.1870e-01,  2.0484e-02,  ...,  1.3550e+00,\n",
            "           -1.7109e+00, -1.6955e+00],\n",
            "          ...,\n",
            "          [ 1.3197e-02, -2.6425e-01, -6.3390e-01,  ...,  1.8065e+00,\n",
            "           -1.7048e+00, -1.1322e+00],\n",
            "          [-1.0004e+00, -1.4674e-01, -7.6303e-01,  ...,  1.4646e+00,\n",
            "           -1.3086e+00, -8.0701e-01],\n",
            "          [-5.7175e-01, -6.2662e-01,  3.5233e-01,  ...,  8.6453e-01,\n",
            "           -1.0834e+00, -1.4469e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.4020e-02, -3.7531e-02,  1.6861e-02,  ..., -6.2447e-01,\n",
            "           -5.6881e-02, -1.7755e-01],\n",
            "          [ 4.1625e-01,  2.0437e-01, -4.0596e-01,  ...,  4.8782e-01,\n",
            "            9.3005e-01, -2.6474e+00],\n",
            "          [ 1.0184e+00, -5.0930e-01,  5.3509e-01,  ...,  1.7761e+00,\n",
            "            9.6432e-01, -1.4100e+00],\n",
            "          ...,\n",
            "          [-4.4850e-01,  3.4054e-01, -2.6088e-01,  ...,  2.1623e+00,\n",
            "            2.0057e+00, -1.9364e+00],\n",
            "          [ 5.1958e-01,  3.7250e-01, -7.9526e-01,  ...,  2.4603e+00,\n",
            "            5.7345e-01, -4.8476e-01],\n",
            "          [ 5.5546e-01, -1.8079e-01, -3.2898e-01,  ...,  3.3389e+00,\n",
            "           -5.3263e-01,  4.9473e-02]],\n",
            "\n",
            "         [[ 2.4548e-02,  1.0228e-02, -1.3571e-02,  ...,  3.0513e-01,\n",
            "           -1.2625e+00,  9.4536e-01],\n",
            "          [ 1.0025e+00,  7.8797e-02, -2.7252e-01,  ..., -9.3464e-01,\n",
            "            1.2174e+00,  5.2648e-01],\n",
            "          [ 7.6284e-01,  1.8923e-01,  6.2770e-01,  ...,  6.6388e-01,\n",
            "            7.5528e-02,  7.5096e-01],\n",
            "          ...,\n",
            "          [ 1.0975e+00, -6.8136e-01, -7.2972e-02,  ...,  1.4430e+00,\n",
            "            1.3951e+00,  1.1251e+00],\n",
            "          [ 1.5273e+00, -2.0027e-01, -5.8078e-01,  ...,  5.2690e-03,\n",
            "            1.5326e+00,  4.4739e-01],\n",
            "          [ 7.0692e-01, -3.5014e-02, -1.1410e-01,  ..., -6.8425e-01,\n",
            "            1.2678e+00, -9.0577e-01]],\n",
            "\n",
            "         [[-1.2951e-02, -6.5120e-03,  1.1936e-02,  ..., -1.0780e-01,\n",
            "           -5.3472e-01,  1.4482e+00],\n",
            "          [-1.6840e+00, -1.0970e-01,  4.4355e-02,  ...,  2.2080e+00,\n",
            "            1.2227e+00, -4.5746e+00],\n",
            "          [-1.7468e+00,  2.0365e-02, -6.5012e-02,  ...,  1.6777e-01,\n",
            "           -5.1539e-01, -3.5996e+00],\n",
            "          ...,\n",
            "          [ 1.1854e-02, -2.6623e-01,  3.3752e-01,  ...,  2.0018e-01,\n",
            "            8.7313e-01, -5.2611e+00],\n",
            "          [-9.7791e-01, -1.7007e+00, -9.9093e-02,  ...,  4.5170e-01,\n",
            "            9.7495e-01, -4.5086e+00],\n",
            "          [-1.1123e+00, -3.3728e-01, -4.8998e-01,  ...,  4.4624e-01,\n",
            "            1.1427e+00, -4.1458e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 4.5031e-03, -2.5024e-02,  6.6604e-03,  ...,  6.0648e-03,\n",
            "           -3.1954e-03, -8.1647e-03],\n",
            "          [ 3.8258e-01,  1.9168e-01, -2.9126e-01,  ..., -5.4362e-02,\n",
            "            3.5054e-01, -2.9295e-01],\n",
            "          [ 8.2460e-01,  7.1063e-02,  1.3956e-01,  ..., -6.6693e-01,\n",
            "           -6.3683e-02, -2.0594e-01],\n",
            "          ...,\n",
            "          [ 1.6800e-01,  7.2714e-02,  4.0650e-01,  ..., -1.5097e-01,\n",
            "           -1.0995e-01, -5.1039e-02],\n",
            "          [ 1.0717e+00,  2.8331e-01,  4.6917e-01,  ..., -3.8219e-02,\n",
            "           -4.2837e-02, -4.1759e-01],\n",
            "          [ 1.4580e-01,  8.9450e-02,  3.6277e-01,  ...,  1.9405e-01,\n",
            "           -1.3777e-01,  5.1965e-02]],\n",
            "\n",
            "         [[-1.3020e-03,  2.7104e-04, -9.9731e-03,  ...,  1.8190e-02,\n",
            "            1.8532e-03,  6.8916e-04],\n",
            "          [ 3.9087e-01,  1.9848e-01, -1.6565e-01,  ..., -4.2452e-02,\n",
            "           -4.3648e-01,  4.5147e-01],\n",
            "          [ 7.1338e-02, -1.2162e-01, -5.8879e-03,  ..., -6.0713e-01,\n",
            "            3.9920e-01, -4.3518e-01],\n",
            "          ...,\n",
            "          [ 1.1109e-01, -1.5200e-02,  6.1182e-02,  ...,  1.4362e-01,\n",
            "            1.0635e-01,  4.3578e-01],\n",
            "          [-1.3433e-01, -3.6630e-01,  1.5293e-01,  ..., -3.8332e-01,\n",
            "            3.0745e-01,  3.5296e-01],\n",
            "          [ 4.0163e-01,  2.5042e-01, -2.0478e-01,  ..., -2.4247e-02,\n",
            "            3.4174e-01,  3.0432e-02]],\n",
            "\n",
            "         [[ 4.0183e-03, -7.0346e-03,  4.3845e-03,  ...,  4.9430e-03,\n",
            "            1.0709e-02, -8.3107e-04],\n",
            "          [-1.9180e-01, -1.9867e-01, -3.5928e-01,  ...,  3.9413e-01,\n",
            "            1.8125e-01,  1.3151e-01],\n",
            "          [-9.4993e-02, -5.1948e-01,  1.0207e+00,  ...,  4.6654e-01,\n",
            "           -3.0360e-02,  2.8178e-01],\n",
            "          ...,\n",
            "          [ 6.0161e-02, -4.6067e-01,  6.6789e-01,  ...,  4.1325e-01,\n",
            "            5.4146e-02,  5.1851e-01],\n",
            "          [ 1.8067e-01, -1.7316e-01,  6.3138e-01,  ...,  5.1344e-01,\n",
            "            1.6243e-01,  1.8279e-01],\n",
            "          [ 9.4847e-02,  3.9699e-01,  8.3325e-01,  ...,  5.2332e-01,\n",
            "           -1.1793e-01, -1.4457e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.8470e-03,  1.7871e-03, -3.9760e-03,  ..., -1.1957e-02,\n",
            "            5.2358e-03, -9.4573e-03],\n",
            "          [ 7.4422e-01, -1.4012e-01,  6.3163e-01,  ..., -1.0794e+00,\n",
            "           -4.4267e-01, -3.5620e-01],\n",
            "          [-4.2414e-01, -5.3282e-01,  6.9046e-01,  ...,  1.0712e+00,\n",
            "           -2.9197e-01,  3.7504e-01],\n",
            "          ...,\n",
            "          [ 3.4736e-01,  8.9110e-03,  8.4308e-02,  ..., -1.3702e-02,\n",
            "           -4.0676e-01,  6.5003e-01],\n",
            "          [-9.1185e-02,  1.7929e-01,  4.1368e-01,  ...,  2.3997e-01,\n",
            "           -4.0393e-01,  3.0401e-01],\n",
            "          [-2.0440e-01, -1.4415e-02,  4.4005e-01,  ...,  8.8561e-02,\n",
            "            7.8072e-02,  3.5197e-01]],\n",
            "\n",
            "         [[ 4.0363e-03, -1.7074e-02,  3.4699e-05,  ...,  8.0084e-03,\n",
            "           -9.7552e-03, -3.0828e-03],\n",
            "          [ 7.4363e-01, -1.8518e-01,  1.7704e-01,  ...,  2.6778e-01,\n",
            "           -7.5603e-02,  3.6677e-01],\n",
            "          [ 3.8756e-01,  5.0141e-01, -4.0202e-01,  ...,  3.5191e-01,\n",
            "            1.2314e-01,  8.4531e-02],\n",
            "          ...,\n",
            "          [ 2.5214e-01,  1.3288e-01,  2.9797e-01,  ..., -8.7022e-02,\n",
            "            2.4886e-01,  5.3416e-01],\n",
            "          [ 6.4971e-01,  5.1482e-01, -4.1649e-01,  ..., -5.6701e-01,\n",
            "            1.0422e-01,  3.7462e-01],\n",
            "          [ 1.1192e-01, -5.1306e-01,  3.7242e-01,  ..., -1.1522e-01,\n",
            "            1.9020e-01,  2.4538e-01]],\n",
            "\n",
            "         [[-1.6352e-03,  1.5690e-02, -3.9594e-03,  ..., -5.0426e-03,\n",
            "           -1.7233e-02,  2.4241e-04],\n",
            "          [ 3.2957e-01, -1.2319e+00,  4.6350e-01,  ...,  5.0708e-01,\n",
            "            4.1711e-01, -8.3615e-01],\n",
            "          [ 1.1448e-01, -5.5044e-01, -7.7183e-02,  ..., -1.9088e-02,\n",
            "            3.5079e-01, -1.7610e-01],\n",
            "          ...,\n",
            "          [ 4.0319e-01, -1.4328e+00,  5.2304e-01,  ...,  7.6552e-01,\n",
            "           -4.2031e-01, -1.4640e-01],\n",
            "          [ 4.5998e-01, -7.7344e-01,  4.9361e-01,  ...,  5.9337e-01,\n",
            "            1.4353e-03,  1.6751e-01],\n",
            "          [-2.1328e-01, -3.3922e-01,  3.6957e-01,  ...,  5.2377e-01,\n",
            "           -1.9529e-01, -5.0934e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 6.6256e-03, -1.8156e-02, -1.8656e-02,  ...,  1.4149e-01,\n",
            "            5.1449e-03, -1.7037e-01],\n",
            "          [ 1.3277e+00,  3.6107e-02,  7.3445e-01,  ...,  1.7534e-01,\n",
            "           -2.1889e-02, -5.5191e-01],\n",
            "          [-8.6954e-01, -4.6119e-01, -1.9528e-01,  ...,  8.3090e-01,\n",
            "           -1.2390e+00,  1.1518e+00],\n",
            "          ...,\n",
            "          [ 1.5159e+00,  9.6393e-01,  3.6867e-01,  ...,  2.2861e+00,\n",
            "           -1.0620e+00,  1.2761e+00],\n",
            "          [ 1.1105e+00,  3.4131e-01,  3.2934e-01,  ...,  1.5884e+00,\n",
            "           -8.2419e-02,  1.0088e+00],\n",
            "          [ 4.6815e-01,  3.3552e-01,  5.6609e-01,  ...,  2.0393e+00,\n",
            "           -8.1772e-01,  1.1421e+00]],\n",
            "\n",
            "         [[ 1.2144e-02,  1.6865e-03,  1.8037e-02,  ...,  4.4013e-01,\n",
            "            1.1576e+00, -3.9238e-01],\n",
            "          [-1.5862e-01,  1.1155e-01,  3.1442e-01,  ..., -2.8111e+00,\n",
            "           -2.6553e+00,  2.5914e+00],\n",
            "          [ 1.0355e-01, -2.3725e-01,  2.0949e-01,  ..., -1.2600e+00,\n",
            "            2.3498e-01,  3.3417e+00],\n",
            "          ...,\n",
            "          [-3.2321e-01,  4.7372e-02,  3.5083e-01,  ..., -1.5670e+00,\n",
            "           -2.4784e-01,  3.5004e+00],\n",
            "          [-2.9349e-01, -3.0916e-02,  1.9556e-01,  ..., -3.3438e+00,\n",
            "           -1.3952e+00,  4.0545e+00],\n",
            "          [ 1.8962e-01, -1.3779e-01,  5.3299e-02,  ..., -7.0931e-01,\n",
            "           -1.7427e+00,  1.2562e+00]],\n",
            "\n",
            "         [[-2.5919e-02, -2.9640e-02, -2.6901e-02,  ...,  5.1034e-01,\n",
            "            3.7148e-01, -3.2498e-01],\n",
            "          [-7.3231e-01, -9.2452e-03, -3.8669e-01,  ..., -2.1608e+00,\n",
            "           -1.9524e+00,  7.4309e-01],\n",
            "          [-1.7098e-01,  5.7453e-01,  2.6549e-01,  ...,  2.3427e-01,\n",
            "           -8.5662e-01,  7.2379e-01],\n",
            "          ...,\n",
            "          [ 2.1977e-01, -4.8344e-01,  1.1778e-01,  ...,  1.9236e+00,\n",
            "           -6.7438e-01,  1.0630e+00],\n",
            "          [ 4.8687e-01,  2.5256e-01, -1.1637e+00,  ...,  1.3822e-01,\n",
            "           -8.4892e-01,  1.3488e+00],\n",
            "          [ 4.4214e-01,  3.2481e-01, -6.2313e-01,  ...,  1.1218e+00,\n",
            "           -1.0262e+00,  4.9254e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0069e-02, -6.9871e-03, -8.2251e-03,  ...,  2.0922e+00,\n",
            "           -3.9890e-01, -4.2414e-01],\n",
            "          [ 6.2080e-02, -4.4732e-01, -2.0635e-01,  ..., -6.1567e+00,\n",
            "            1.5636e+00, -1.8424e+00],\n",
            "          [ 4.0587e-01, -2.6018e-01, -4.9222e-01,  ..., -4.1247e+00,\n",
            "            2.4956e+00, -2.3988e+00],\n",
            "          ...,\n",
            "          [-1.9473e-01, -2.2792e-01,  4.5684e-02,  ..., -5.0066e+00,\n",
            "            2.1013e+00, -2.6336e+00],\n",
            "          [ 2.2995e-01, -2.0991e-01, -3.6132e-01,  ..., -7.0343e+00,\n",
            "            2.1778e+00, -2.1333e+00],\n",
            "          [ 2.1213e-01, -3.6086e-01, -2.6561e-01,  ..., -5.4680e+00,\n",
            "            1.3013e+00,  7.8954e-01]],\n",
            "\n",
            "         [[ 2.4786e-02, -2.5973e-02,  6.0373e-03,  ..., -1.4180e+00,\n",
            "           -3.9950e-01, -1.1292e-01],\n",
            "          [-5.4968e-01, -3.6330e-01,  7.9203e-02,  ...,  5.2676e+00,\n",
            "            1.3924e+00,  3.8412e+00],\n",
            "          [-8.6166e-01,  5.2850e-02,  3.9796e-02,  ...,  5.1641e+00,\n",
            "            3.8822e-01,  3.2822e+00],\n",
            "          ...,\n",
            "          [ 5.9621e-01, -1.9167e-01, -6.2632e-02,  ...,  6.2332e+00,\n",
            "           -7.3192e-01,  2.6217e+00],\n",
            "          [-6.2080e-01, -4.2706e-01,  1.2104e-01,  ...,  6.2149e+00,\n",
            "            2.9668e-01,  9.5250e-01],\n",
            "          [-9.7332e-01, -4.5656e-01, -1.2687e-01,  ...,  5.3654e+00,\n",
            "            1.2036e-01,  1.1137e+00]],\n",
            "\n",
            "         [[ 1.6488e-02,  2.9937e-03,  2.0157e-02,  ...,  6.1391e-02,\n",
            "            1.2075e+00, -7.1502e-02],\n",
            "          [-5.1168e-01, -4.1507e-01, -1.6042e-01,  ..., -3.8790e+00,\n",
            "           -3.2570e+00,  5.7665e-01],\n",
            "          [-7.2818e-01,  1.1610e-01,  1.7133e-01,  ..., -2.6817e+00,\n",
            "           -2.0661e+00,  2.3096e+00],\n",
            "          ...,\n",
            "          [ 2.2340e-01, -5.2683e-01, -9.4305e-02,  ..., -3.2887e+00,\n",
            "           -1.4155e+00,  3.1093e+00],\n",
            "          [-2.0415e-01, -5.6361e-01,  5.5057e-01,  ..., -4.4700e+00,\n",
            "           -2.9405e+00,  1.9737e-01],\n",
            "          [-4.9086e-01, -7.8249e-01,  6.5699e-01,  ..., -2.0688e+00,\n",
            "           -1.2905e+00,  1.4405e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 2.0911e-02,  1.9974e-02,  4.2863e-03,  ...,  6.2532e-03,\n",
            "            1.5020e-03,  4.5943e-03],\n",
            "          [ 4.2933e-01,  1.6287e-01, -7.7684e-01,  ..., -7.5336e-02,\n",
            "           -3.0459e-01,  2.9320e-01],\n",
            "          [-4.5782e-02, -1.9603e-01, -3.7904e-01,  ..., -1.6675e-01,\n",
            "            3.6790e-02,  3.5034e-03],\n",
            "          ...,\n",
            "          [-1.8042e-01,  1.2979e-01, -6.8229e-01,  ..., -5.6415e-01,\n",
            "            6.0916e-01,  8.7317e-01],\n",
            "          [-1.4544e-01, -5.6633e-02, -5.6190e-01,  ..., -2.0221e-01,\n",
            "            5.9181e-01,  4.7358e-01],\n",
            "          [-3.9757e-01,  1.9259e-01, -7.0596e-01,  ..., -3.9496e-01,\n",
            "            9.4290e-01,  9.6502e-01]],\n",
            "\n",
            "         [[-3.2789e-03,  1.5138e-02, -1.3224e-02,  ...,  5.4446e-04,\n",
            "           -1.9797e-02,  1.6592e-02],\n",
            "          [-1.0326e-01, -8.4227e-01,  8.8634e-01,  ..., -4.8901e-01,\n",
            "            3.5737e-01,  9.3728e-01],\n",
            "          [-5.0263e-01, -6.5655e-01,  2.1119e-01,  ..., -2.3104e-01,\n",
            "            7.1224e-01,  6.2878e-01],\n",
            "          ...,\n",
            "          [-4.3864e-01, -6.7663e-01, -2.5176e-01,  ..., -6.9608e-01,\n",
            "           -4.6035e-01, -4.6675e-02],\n",
            "          [ 2.2254e-01,  5.1126e-01, -5.9618e-02,  ...,  2.1810e-01,\n",
            "           -5.9272e-01,  1.8732e-02],\n",
            "          [-2.6584e-01, -3.4386e-01,  1.2935e-01,  ...,  7.6490e-02,\n",
            "           -5.3521e-01,  4.2522e-01]],\n",
            "\n",
            "         [[ 3.5629e-03,  2.4892e-02, -7.7270e-03,  ..., -4.5479e-02,\n",
            "           -2.2080e-03,  7.4162e-04],\n",
            "          [-6.3818e-02,  1.4899e+00, -4.0729e-01,  ..., -1.1810e+00,\n",
            "            5.3569e-01, -1.2555e-01],\n",
            "          [ 2.5329e-01, -2.2799e-01, -1.1382e+00,  ..., -8.7550e-01,\n",
            "            2.1891e-01, -6.6523e-01],\n",
            "          ...,\n",
            "          [-5.6996e-02,  2.1336e-01, -1.1033e+00,  ..., -1.2520e-01,\n",
            "           -3.6956e-01, -6.5295e-01],\n",
            "          [-7.7729e-02,  6.6639e-02, -2.6186e-01,  ..., -6.6944e-01,\n",
            "            9.4524e-02, -7.7659e-01],\n",
            "          [ 1.6473e-01,  3.0326e-01, -5.9372e-01,  ...,  1.0382e-01,\n",
            "            3.2610e-01, -5.8753e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7941e-02,  3.9896e-03,  3.3658e-03,  ..., -2.6906e-03,\n",
            "            2.5613e-03, -1.4568e-02],\n",
            "          [ 3.6419e-01, -1.6230e-01, -1.8518e-01,  ..., -3.2913e-02,\n",
            "            7.1966e-01, -6.1521e-01],\n",
            "          [ 3.8929e-01,  3.6880e-02, -6.6699e-01,  ..., -4.0691e-02,\n",
            "            2.0804e-01, -3.2930e-02],\n",
            "          ...,\n",
            "          [-7.8613e-02,  3.0968e-01, -2.1202e-02,  ..., -2.8829e-01,\n",
            "           -5.0234e-01,  1.9830e-01],\n",
            "          [ 5.8524e-02,  4.9829e-01, -1.6159e-01,  ...,  2.2966e-01,\n",
            "           -1.3737e-01, -3.8259e-01],\n",
            "          [ 1.4545e-02, -7.4204e-02, -6.4994e-01,  ..., -4.2489e-02,\n",
            "           -2.8250e-01,  1.9929e-01]],\n",
            "\n",
            "         [[-6.7892e-03, -3.0033e-03,  2.5184e-03,  ...,  1.1282e-02,\n",
            "           -5.1977e-03, -1.5981e-02],\n",
            "          [-4.1496e-01,  1.3819e+00, -1.3104e+00,  ...,  4.6270e-01,\n",
            "            1.9089e-01,  7.3480e-01],\n",
            "          [ 7.0871e-01, -8.9316e-01, -4.6882e-01,  ...,  1.1725e+00,\n",
            "           -4.5146e-01, -1.0417e+00],\n",
            "          ...,\n",
            "          [ 1.5455e-01, -4.1348e-01,  8.7668e-01,  ..., -2.4287e-01,\n",
            "           -4.8886e-01, -1.0935e-01],\n",
            "          [ 1.0251e+00, -5.3468e-01,  2.9278e-01,  ...,  5.5063e-01,\n",
            "            6.5558e-01,  3.2795e-01],\n",
            "          [ 1.0875e-01, -1.9784e-01,  5.1875e-01,  ...,  1.1698e-02,\n",
            "           -3.1412e-01,  5.0336e-01]],\n",
            "\n",
            "         [[ 3.6447e-03, -8.2835e-03, -1.2041e-01,  ..., -1.7636e-04,\n",
            "           -6.7571e-03,  6.3601e-03],\n",
            "          [ 8.2000e-01,  1.6324e-01,  1.0898e-01,  ...,  7.3253e-01,\n",
            "           -5.5834e-01, -3.9187e-01],\n",
            "          [ 4.3582e-01, -5.4419e-02,  1.8735e-01,  ..., -3.1321e-01,\n",
            "           -7.2169e-01, -4.3374e-02],\n",
            "          ...,\n",
            "          [ 3.3680e-01, -3.8309e-01,  3.5372e-01,  ...,  4.0053e-03,\n",
            "           -6.5892e-01,  3.2503e-01],\n",
            "          [ 4.3713e-02, -7.8630e-02,  6.5869e-01,  ..., -3.8487e-01,\n",
            "           -5.8753e-01,  7.5801e-02],\n",
            "          [ 2.6686e-02,  2.7984e-01,  9.9816e-01,  ..., -2.0224e-01,\n",
            "            3.8248e-02, -1.9292e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 9.2064e-03, -1.4807e-03,  2.1454e-02,  ..., -2.9816e-01,\n",
            "            4.5272e-02, -6.7371e-01],\n",
            "          [-5.9292e-01,  7.2881e-01, -5.1846e-01,  ...,  1.3125e+00,\n",
            "            2.0073e+00,  6.9712e-01],\n",
            "          [ 1.6404e-01,  1.2863e+00, -4.2687e-01,  ...,  2.0585e+00,\n",
            "            1.5756e+00,  4.4913e-01],\n",
            "          ...,\n",
            "          [-1.2409e+00, -2.5457e-01,  3.8058e-02,  ...,  1.8547e+00,\n",
            "            8.4260e-01,  7.1916e-01],\n",
            "          [-6.3340e-01,  4.4762e-01,  2.4731e-01,  ...,  2.5800e+00,\n",
            "            9.3034e-01,  1.4673e-01],\n",
            "          [ 4.0008e-02,  3.6292e-01,  3.8025e-01,  ...,  2.1431e+00,\n",
            "            1.0076e+00,  5.8438e-01]],\n",
            "\n",
            "         [[-2.1055e-02, -5.3728e-03,  2.5141e-02,  ..., -2.4382e-02,\n",
            "            3.6695e-01, -5.5607e-02],\n",
            "          [-7.5885e-01,  6.1211e-01, -9.5896e-01,  ...,  8.2207e-01,\n",
            "           -2.2645e-02,  1.4875e+00],\n",
            "          [-8.2863e-01, -7.7515e-01, -6.0584e-01,  ...,  1.0692e+00,\n",
            "           -1.7419e+00,  7.1637e-01],\n",
            "          ...,\n",
            "          [ 1.6240e-01,  4.5020e-01,  9.8018e-01,  ...,  1.1574e+00,\n",
            "           -1.0904e+00,  3.3147e-01],\n",
            "          [-1.1595e+00, -1.0435e-01,  8.2382e-01,  ...,  1.4421e+00,\n",
            "           -2.2128e-01,  7.7757e-01],\n",
            "          [-7.6417e-01, -9.1581e-01, -3.7462e-01,  ...,  7.1482e-01,\n",
            "           -6.2825e-01,  5.5969e-01]],\n",
            "\n",
            "         [[-3.0800e-03,  1.3392e-02, -9.9971e-04,  ...,  6.6125e-02,\n",
            "            1.3776e+00, -4.3705e-02],\n",
            "          [ 9.3933e-01, -1.3621e+00, -1.2004e+00,  ..., -1.7441e+00,\n",
            "           -3.2308e+00, -4.9280e-01],\n",
            "          [ 5.3762e-01, -3.0740e-02,  1.3463e+00,  ..., -1.0781e-01,\n",
            "           -3.7877e+00,  6.5176e-01],\n",
            "          ...,\n",
            "          [ 1.7964e-01,  3.3910e-01, -1.6526e+00,  ..., -2.7755e-02,\n",
            "           -4.1582e+00,  2.3800e-01],\n",
            "          [ 1.7333e+00, -1.2712e+00, -1.2856e+00,  ...,  2.4044e-01,\n",
            "           -3.8043e+00,  2.1387e-01],\n",
            "          [ 4.6610e-01, -1.0040e+00,  4.7429e-01,  ...,  1.3464e-02,\n",
            "           -4.1662e+00, -4.0499e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0957e-02,  3.8970e-02,  1.2743e-02,  ...,  4.6651e-01,\n",
            "           -1.3840e-01, -3.7900e-01],\n",
            "          [-4.4420e-01,  5.3785e-02,  1.5959e-01,  ...,  7.0249e-02,\n",
            "            1.6082e+00,  1.0460e+00],\n",
            "          [-7.1477e-01,  9.5385e-02,  5.1303e-01,  ..., -8.3785e-01,\n",
            "           -9.6546e-01,  4.0713e-02],\n",
            "          ...,\n",
            "          [ 7.4008e-01,  1.8320e-01, -4.0902e-01,  ..., -9.1516e-01,\n",
            "            2.7058e-01,  6.7507e-01],\n",
            "          [ 6.6712e-01, -9.7936e-02,  1.4687e-04,  ..., -2.3978e+00,\n",
            "            5.2640e-01,  9.3399e-01],\n",
            "          [-1.1557e+00, -5.9159e-02, -1.4746e+00,  ..., -2.2162e+00,\n",
            "            1.3194e+00,  2.1742e+00]],\n",
            "\n",
            "         [[-1.2780e-02,  3.1025e-02,  2.1291e-02,  ...,  2.1517e+00,\n",
            "            2.3798e-01,  4.5305e-01],\n",
            "          [ 9.1021e-01, -6.6777e-02, -4.2362e-01,  ..., -4.3762e+00,\n",
            "           -4.6560e-01, -5.0649e+00],\n",
            "          [-6.3404e-01, -9.0981e-01, -5.9596e-01,  ..., -4.1395e+00,\n",
            "           -3.0472e+00, -3.8454e+00],\n",
            "          ...,\n",
            "          [ 7.1618e-01,  9.1023e-01,  2.5060e-01,  ..., -4.9823e+00,\n",
            "           -4.3278e+00, -2.2201e+00],\n",
            "          [ 1.1328e+00,  1.1188e-01,  5.2108e-01,  ..., -5.4215e+00,\n",
            "           -2.0784e+00, -3.5176e+00],\n",
            "          [ 3.0552e-01, -2.9785e-01,  7.5879e-01,  ..., -5.9860e+00,\n",
            "           -2.2456e+00, -1.7489e+00]],\n",
            "\n",
            "         [[ 3.9052e-02,  5.2439e-02, -2.7326e-02,  ..., -1.1303e+00,\n",
            "           -2.8939e-01,  2.9515e-01],\n",
            "          [-3.7126e-01,  2.6286e-01, -2.2670e-02,  ...,  2.0684e+00,\n",
            "            2.1761e+00,  1.7860e+00],\n",
            "          [ 5.1425e-01, -6.3500e-01, -9.6533e-02,  ...,  1.2024e+00,\n",
            "            2.4507e+00, -2.1685e-01],\n",
            "          ...,\n",
            "          [-1.6186e-01,  1.2792e-01,  8.8955e-01,  ...,  1.5765e+00,\n",
            "            2.2283e+00,  4.8651e-01],\n",
            "          [ 5.0774e-02, -7.4074e-01,  3.7460e-01,  ...,  1.9469e+00,\n",
            "            2.4121e+00, -9.4367e-01],\n",
            "          [ 2.0021e-01,  1.7252e-01, -4.4013e-02,  ...,  2.5957e+00,\n",
            "            3.2123e+00,  6.4733e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-5.8780e-03, -1.5519e-02, -2.3996e-02,  ...,  4.2383e-04,\n",
            "            2.7107e-03,  5.6664e-03],\n",
            "          [-8.9658e-01,  1.5228e-01, -1.4760e-01,  ..., -7.5372e-01,\n",
            "           -5.1334e-01, -1.2190e-01],\n",
            "          [-6.5626e-01,  3.6019e-01,  3.5585e-01,  ..., -3.3290e-01,\n",
            "           -7.8089e-01,  1.7392e-01],\n",
            "          ...,\n",
            "          [-9.8214e-01,  3.1109e-01,  1.3783e-01,  ..., -4.0235e-01,\n",
            "           -9.0147e-01,  3.4724e-01],\n",
            "          [-7.0199e-01, -3.2799e-01,  6.0213e-02,  ..., -8.5158e-01,\n",
            "           -3.4428e-01,  7.7229e-01],\n",
            "          [-7.0174e-01, -1.1332e-01,  3.4217e-01,  ..., -2.9992e-01,\n",
            "           -5.0732e-01, -1.7970e-01]],\n",
            "\n",
            "         [[ 1.1247e-02,  1.4844e-02,  3.2317e-03,  ...,  2.7020e-03,\n",
            "           -3.9726e-03, -1.7377e-02],\n",
            "          [ 7.6985e-01, -1.0589e+00, -3.3392e-01,  ...,  4.9451e-01,\n",
            "           -2.8797e-01,  3.4493e-01],\n",
            "          [ 1.5472e-01, -6.7491e-01,  1.4366e-01,  ...,  6.8995e-01,\n",
            "            5.6659e-02,  1.9627e-02],\n",
            "          ...,\n",
            "          [ 7.5557e-01, -3.3395e-01,  3.8829e-01,  ..., -1.3857e-01,\n",
            "           -1.2387e-01,  4.8402e-01],\n",
            "          [ 3.9183e-01,  1.0088e-01,  4.8697e-01,  ..., -6.0406e-01,\n",
            "            1.8298e-02, -3.9095e-01],\n",
            "          [ 3.9105e-01, -9.8815e-01,  5.7590e-01,  ...,  2.0442e-01,\n",
            "           -1.4650e-01,  1.0114e+00]],\n",
            "\n",
            "         [[-2.6582e-03,  1.7922e-02,  5.8794e-02,  ...,  1.5329e-02,\n",
            "           -2.3783e-04,  4.4655e-02],\n",
            "          [-3.7531e-01,  1.6048e-02,  4.4985e-01,  ...,  3.9780e-01,\n",
            "           -5.6136e-01,  6.2626e-01],\n",
            "          [-8.2227e-01,  7.7320e-01,  5.6955e-01,  ...,  5.9146e-02,\n",
            "           -2.0713e-01,  6.4298e-01],\n",
            "          ...,\n",
            "          [-3.5950e-01,  1.9324e-01,  1.2275e-01,  ...,  1.3695e-01,\n",
            "           -5.3986e-01,  7.2952e-01],\n",
            "          [-1.1524e+00, -2.6431e-01,  4.6713e-01,  ...,  5.0776e-02,\n",
            "           -2.4046e-01,  3.2125e-01],\n",
            "          [-8.3197e-02, -4.2148e-02,  7.1947e-01,  ...,  8.7290e-01,\n",
            "           -4.0835e-01,  7.6450e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4723e-02,  7.7735e-03,  1.6410e-03,  ..., -1.2369e-02,\n",
            "            1.5395e-02, -7.1158e-04],\n",
            "          [-9.9931e-01, -1.5850e-01,  1.0338e+00,  ..., -7.7336e-01,\n",
            "           -1.8771e+00,  4.2815e-01],\n",
            "          [-6.8601e-01,  2.5950e-01, -7.9812e-01,  ...,  5.2589e-01,\n",
            "           -3.9476e-01,  2.9770e-01],\n",
            "          ...,\n",
            "          [-4.3109e-01, -1.1398e+00, -2.0523e-01,  ...,  1.2486e-01,\n",
            "           -4.5315e-01, -2.0121e-01],\n",
            "          [ 1.5978e-01, -4.6063e-01, -5.0594e-02,  ..., -4.4755e-02,\n",
            "           -7.8162e-01, -1.6776e-01],\n",
            "          [ 1.1770e-01, -4.1730e-01,  1.3558e-01,  ...,  2.5861e-01,\n",
            "           -2.0328e-01, -1.4682e-01]],\n",
            "\n",
            "         [[-1.9819e-03, -2.3334e-02,  1.2381e-03,  ..., -2.2406e-02,\n",
            "           -9.2228e-04, -2.4936e-03],\n",
            "          [ 3.2584e-01, -1.0343e+00, -7.4135e-01,  ..., -1.0685e+00,\n",
            "            6.6366e-01,  7.9064e-01],\n",
            "          [ 2.0304e-02,  3.9476e-01, -2.7763e-02,  ...,  7.5761e-02,\n",
            "           -1.0768e-01,  2.2568e-01],\n",
            "          ...,\n",
            "          [-1.8620e-01,  5.1529e-01, -3.9915e-01,  ..., -2.8808e-01,\n",
            "            2.4891e-01, -6.0151e-01],\n",
            "          [ 5.1733e-01,  1.9180e-01, -6.0820e-01,  ...,  2.1649e-02,\n",
            "            8.8433e-02, -2.1904e-01],\n",
            "          [ 3.3608e-01,  7.8539e-02,  9.7215e-02,  ..., -2.9624e-02,\n",
            "           -4.2042e-02, -1.2475e-01]],\n",
            "\n",
            "         [[ 1.2523e-03,  1.0287e-02, -4.6109e-03,  ...,  2.3173e-02,\n",
            "           -1.4418e-03, -2.1737e-02],\n",
            "          [-1.3892e+00, -1.7652e-01, -1.9429e-01,  ..., -5.1731e-01,\n",
            "            8.2306e-03,  1.2295e+00],\n",
            "          [ 6.6649e-01,  5.7896e-01, -6.3527e-02,  ..., -2.1224e-01,\n",
            "           -6.8097e-01,  4.9168e-01],\n",
            "          ...,\n",
            "          [-4.7489e-03, -4.5972e-01,  6.3446e-02,  ...,  2.0533e-01,\n",
            "            3.9218e-01,  9.1149e-01],\n",
            "          [ 5.5218e-01, -3.0168e-01,  8.0520e-01,  ..., -4.6980e-01,\n",
            "            2.4383e-01,  1.4031e+00],\n",
            "          [ 4.9516e-01, -8.2742e-01, -9.8310e-01,  ..., -9.2822e-01,\n",
            "           -2.2707e-01,  1.0758e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-5.6493e-03,  2.4741e-02,  1.3126e-02,  ..., -1.6968e-01,\n",
            "            5.8317e-02, -5.7399e-01],\n",
            "          [ 1.4462e-01, -2.4693e-01,  1.8146e-01,  ...,  3.6828e+00,\n",
            "           -2.0153e-01,  1.2434e+00],\n",
            "          [ 1.9058e-01, -1.9081e-02,  2.4236e-01,  ...,  2.7865e+00,\n",
            "           -1.6218e+00,  1.2727e-01],\n",
            "          ...,\n",
            "          [-4.1782e-01,  1.6280e-01, -7.9806e-02,  ...,  2.4114e+00,\n",
            "           -2.1879e-01, -6.3772e-02],\n",
            "          [-5.5280e-01, -2.6916e-01, -3.4151e-01,  ...,  3.5310e+00,\n",
            "           -5.5403e-01,  8.2005e-01],\n",
            "          [ 2.3766e-01, -1.7865e-01,  5.2865e-03,  ...,  8.0654e-02,\n",
            "            5.6824e-02,  1.6893e+00]],\n",
            "\n",
            "         [[ 1.6792e-02, -2.6317e-02,  9.6289e-03,  ...,  3.5550e-01,\n",
            "            5.0767e-01,  4.2265e-01],\n",
            "          [-2.7823e+00, -5.1283e-01,  1.5349e-02,  ...,  8.1372e-01,\n",
            "           -1.2159e+00, -1.1724e+00],\n",
            "          [ 3.7946e-01, -2.4852e-01,  4.9626e-01,  ..., -3.3129e-01,\n",
            "            3.7416e-01, -6.5805e-02],\n",
            "          ...,\n",
            "          [-2.1803e+00,  8.6100e-02, -5.3281e-01,  ..., -3.2459e-01,\n",
            "           -6.3131e-01, -2.3663e-02],\n",
            "          [-1.9521e+00, -3.7875e-01,  1.6696e-01,  ..., -8.5783e-01,\n",
            "            9.0713e-01, -4.9986e-01],\n",
            "          [ 1.6854e-02, -6.9365e-01,  3.9983e-01,  ...,  1.9327e-02,\n",
            "           -2.9284e-01, -1.7470e-01]],\n",
            "\n",
            "         [[ 1.7870e-02, -1.1070e-02, -2.4409e-02,  ..., -3.4564e-01,\n",
            "            5.8172e-02,  2.1618e-01],\n",
            "          [-3.1359e-01,  1.0389e+00, -3.5331e-01,  ...,  3.6376e+00,\n",
            "            4.4381e-01, -1.3120e+00],\n",
            "          [ 4.4808e-01,  6.7081e-01, -3.7015e-01,  ...,  5.7934e-01,\n",
            "            2.3913e-01, -1.5434e+00],\n",
            "          ...,\n",
            "          [-7.4938e-01, -3.1019e-01,  6.5865e-01,  ...,  3.3026e-01,\n",
            "            5.9802e-01, -1.2477e+00],\n",
            "          [-4.5819e-01, -1.1901e-01,  3.0935e-01,  ...,  1.1268e+00,\n",
            "           -1.4395e-01, -1.0038e+00],\n",
            "          [ 9.1573e-01,  2.3715e-01,  3.0757e-02,  ...,  7.5466e-01,\n",
            "           -5.3306e-01, -9.9613e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.4436e-02, -5.5003e-03,  2.5695e-03,  ...,  6.3227e-02,\n",
            "            2.3906e-01, -3.3501e-02],\n",
            "          [-1.3787e+00,  1.6756e+00,  5.7689e-01,  ..., -1.0474e+00,\n",
            "            2.6745e-01, -4.3770e-01],\n",
            "          [ 4.8564e-01,  3.5932e-01, -2.2524e-01,  ...,  5.8800e-01,\n",
            "           -8.6577e-01, -8.0772e-01],\n",
            "          ...,\n",
            "          [ 2.4826e-01,  1.2565e+00,  3.0002e-02,  ...,  7.6606e-01,\n",
            "           -8.4170e-01, -5.9700e-01],\n",
            "          [-8.6339e-01,  1.5241e+00,  1.3995e-01,  ...,  1.2510e+00,\n",
            "           -1.0137e+00, -1.2012e+00],\n",
            "          [-5.0296e-01,  6.6271e-01, -8.2453e-01,  ..., -3.1067e-02,\n",
            "           -4.4426e-01, -1.8414e-01]],\n",
            "\n",
            "         [[-1.7677e-02, -2.1692e-02,  2.9919e-02,  ..., -2.4791e-01,\n",
            "            5.9508e-01,  1.3649e-01],\n",
            "          [ 1.5593e-01, -4.2018e-02, -1.4981e-01,  ...,  2.3060e+00,\n",
            "           -5.6786e-01, -1.6126e+00],\n",
            "          [-2.7558e-01,  1.2380e-02, -6.6711e-01,  ...,  2.2002e+00,\n",
            "           -1.4543e+00, -1.7270e+00],\n",
            "          ...,\n",
            "          [ 6.5975e-01, -2.0417e-01,  6.0144e-01,  ...,  2.2429e+00,\n",
            "           -5.6992e-01, -1.0961e+00],\n",
            "          [ 9.4042e-02, -9.2737e-02,  3.6274e-01,  ...,  3.2113e+00,\n",
            "           -1.9333e+00, -9.2350e-01],\n",
            "          [-6.2233e-02, -2.6708e-01,  5.3350e-02,  ...,  1.4870e+00,\n",
            "           -1.5647e+00, -1.3861e+00]],\n",
            "\n",
            "         [[ 9.7664e-03, -5.7733e-03,  1.9799e-02,  ...,  4.3473e-01,\n",
            "            8.4466e-01,  4.7257e-01],\n",
            "          [ 2.0919e-01, -3.5436e-01,  2.4417e-01,  ..., -2.5765e+00,\n",
            "           -1.7782e+00, -1.9696e+00],\n",
            "          [ 5.3635e-02, -5.5600e-01, -1.0044e-02,  ..., -1.2196e+00,\n",
            "           -1.6684e+00, -2.2547e+00],\n",
            "          ...,\n",
            "          [-1.3405e-01,  2.5628e-01,  1.7874e-03,  ..., -6.1700e-01,\n",
            "           -2.3065e+00, -3.9716e+00],\n",
            "          [-2.1868e-01,  1.5036e-01,  1.3339e-02,  ..., -1.9697e+00,\n",
            "           -2.4098e+00, -2.5908e+00],\n",
            "          [-5.8283e-02,  1.4263e-02,  2.1712e-02,  ..., -8.2340e-01,\n",
            "           -2.6462e+00, -1.5639e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-0.0025,  0.0029, -0.0039,  ..., -0.0112, -0.0268, -0.0023],\n",
            "          [ 0.2375, -0.0177,  0.0483,  ..., -0.8229, -1.2528, -0.2004],\n",
            "          [-0.6731, -0.0280, -0.7748,  ...,  0.4902, -0.3636,  0.3097],\n",
            "          ...,\n",
            "          [-0.7309, -0.0474, -0.5620,  ...,  0.2156,  0.1950,  0.1855],\n",
            "          [-0.3421, -0.1544,  0.2305,  ..., -0.0913,  0.0024, -0.3889],\n",
            "          [-0.7996, -0.0935, -0.3483,  ..., -0.2560,  0.2415,  0.2144]],\n",
            "\n",
            "         [[ 0.0136,  0.0022,  0.0200,  ...,  0.0072,  0.0137, -0.0097],\n",
            "          [ 0.0810,  0.0863,  0.4328,  ...,  0.1718, -0.0232, -0.2195],\n",
            "          [-0.2604, -0.5955, -0.3211,  ..., -0.4090, -0.0671, -1.3668],\n",
            "          ...,\n",
            "          [-0.2951, -0.6928, -0.4836,  ..., -0.2696, -0.0174, -1.1337],\n",
            "          [-0.7813, -0.2314,  0.3747,  ..., -0.3931, -0.0656, -0.3327],\n",
            "          [ 0.2674, -0.1069, -0.4700,  ..., -0.2833, -0.3554, -0.9963]],\n",
            "\n",
            "         [[ 0.0051, -0.0052, -0.0072,  ...,  0.0204, -0.0293, -0.0056],\n",
            "          [ 1.3374, -0.9147, -0.2245,  ...,  0.2086,  1.1208,  1.0208],\n",
            "          [ 0.5061, -0.9826, -0.0600,  ...,  0.0694,  1.4351,  0.4182],\n",
            "          ...,\n",
            "          [ 1.1840, -0.7499,  0.1772,  ..., -0.2274,  0.1933,  0.8139],\n",
            "          [ 0.7637, -1.0243, -0.1524,  ..., -0.6459,  0.9258,  1.3959],\n",
            "          [ 0.7257, -0.5282, -0.7830,  ..., -0.4056,  0.0245,  0.1125]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0175, -0.0069,  0.0017,  ...,  0.0124,  0.0357, -0.0035],\n",
            "          [-0.7173,  0.7709, -0.4344,  ...,  0.2684, -0.0377,  0.1049],\n",
            "          [ 0.4366,  0.2929,  0.3880,  ...,  0.6002,  0.1068, -0.2893],\n",
            "          ...,\n",
            "          [ 0.3499,  0.2432,  0.0430,  ...,  0.1084,  0.2389, -0.7156],\n",
            "          [-0.6057, -0.4191,  0.7065,  ...,  0.9258,  0.1770,  0.2016],\n",
            "          [ 0.0228,  1.0547, -0.0614,  ...,  0.1036, -0.4075, -1.2512]],\n",
            "\n",
            "         [[-0.0112,  0.0117, -0.0195,  ...,  0.0019,  0.0054, -0.0171],\n",
            "          [-0.8614,  0.1954,  0.2161,  ...,  1.1580, -0.4022,  0.7657],\n",
            "          [-0.0724, -0.2580,  0.9144,  ...,  0.4119, -0.5945, -0.0302],\n",
            "          ...,\n",
            "          [-0.5489, -0.3504,  0.1232,  ...,  0.1066, -0.9245, -0.2074],\n",
            "          [ 0.0995, -0.8991,  0.3329,  ...,  0.2716, -0.0173,  0.0921],\n",
            "          [-0.8460, -0.2779, -0.2066,  ...,  0.5023, -0.9803,  0.1477]],\n",
            "\n",
            "         [[-0.0022,  0.0084, -0.0086,  ..., -0.0028, -0.0205, -0.0020],\n",
            "          [-0.5729, -0.5606,  0.0450,  ..., -0.0588,  0.0651,  0.9138],\n",
            "          [ 0.3546, -0.7505,  0.0991,  ..., -0.5154, -0.2474,  0.1414],\n",
            "          ...,\n",
            "          [-0.1011, -0.0991, -0.5393,  ...,  0.7712,  0.4480, -0.3756],\n",
            "          [-0.3303, -0.3759, -0.1439,  ...,  1.0773,  1.1103, -0.4031],\n",
            "          [-0.2011,  0.1598, -0.5229,  ...,  0.2056,  0.5017,  0.0898]]]],\n",
            "       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.5530e-02,  4.0457e-02,  2.8457e-02,  ...,  1.9396e-02,\n",
            "           -5.2373e-02, -1.4699e-01],\n",
            "          [-7.7998e-01, -2.0984e-01, -8.2946e-02,  ..., -6.3650e-01,\n",
            "           -9.6455e-02, -3.9992e-01],\n",
            "          [ 1.1034e+00, -8.2865e-01, -9.3671e-01,  ..., -1.6678e+00,\n",
            "            1.2092e+00, -7.1938e-01],\n",
            "          ...,\n",
            "          [-9.0053e-01,  5.9799e-01,  9.1456e-01,  ..., -1.6375e+00,\n",
            "            1.4159e+00, -3.6326e-01],\n",
            "          [-1.1405e+00,  8.3059e-01,  3.8875e-01,  ..., -2.6020e+00,\n",
            "            8.7535e-01,  6.7581e-01],\n",
            "          [ 6.7696e-01,  6.4524e-01,  4.7029e-01,  ..., -1.4960e+00,\n",
            "            2.0298e+00,  3.6584e-01]],\n",
            "\n",
            "         [[-2.0389e-02, -1.8766e-02,  2.4674e-02,  ..., -2.5713e-01,\n",
            "           -2.4665e-01, -6.9810e-01],\n",
            "          [-2.2740e+00, -1.6608e-01, -1.4418e-01,  ...,  3.7478e-01,\n",
            "           -5.1705e-01, -5.8357e-01],\n",
            "          [-1.7779e+00, -4.4068e-02, -1.1554e+00,  ..., -9.3969e-02,\n",
            "           -1.6401e+00,  2.2401e-01],\n",
            "          ...,\n",
            "          [-4.4029e-01, -3.0675e-01,  8.3966e-01,  ...,  1.0240e+00,\n",
            "           -2.7797e+00,  5.4846e-01],\n",
            "          [-2.1160e+00, -1.2374e-01,  9.4743e-01,  ...,  1.7923e+00,\n",
            "           -8.1421e-01,  4.6659e-01],\n",
            "          [-1.1896e+00, -1.1606e+00,  5.3379e-01,  ...,  9.4719e-01,\n",
            "           -1.6249e+00,  8.6523e-01]],\n",
            "\n",
            "         [[ 1.1574e-02, -5.7624e-04,  1.2579e-02,  ..., -3.7034e-01,\n",
            "            7.9849e-02,  2.2484e-01],\n",
            "          [-1.0162e+00, -9.2150e-01,  2.1804e-01,  ...,  5.5236e-01,\n",
            "           -2.2431e+00,  4.2248e-01],\n",
            "          [-3.2041e-01, -3.3479e-01, -3.5084e-02,  ...,  1.3954e+00,\n",
            "           -1.7638e-01, -9.9835e-01],\n",
            "          ...,\n",
            "          [-7.8135e-01,  3.5087e-03, -6.3153e-01,  ...,  2.0855e+00,\n",
            "           -1.0356e+00, -7.6187e-01],\n",
            "          [-7.4451e-01, -5.8227e-01,  4.0365e-02,  ...,  1.6741e+00,\n",
            "           -2.1724e-01, -2.7670e-01],\n",
            "          [-4.4864e-01, -1.1093e+00, -2.8096e-01,  ...,  2.1823e+00,\n",
            "           -1.0205e+00, -1.3870e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5781e-02,  1.9121e-02,  1.5972e-02,  ...,  1.9085e+00,\n",
            "            1.5617e-01, -8.4411e-01],\n",
            "          [-7.0751e-01, -4.7929e-01, -7.6274e-02,  ..., -2.9365e+00,\n",
            "            2.0042e+00,  3.7299e+00],\n",
            "          [-5.3920e-01, -1.0278e+00, -3.3515e-01,  ..., -2.5826e+00,\n",
            "            9.9069e-01,  2.6763e+00],\n",
            "          ...,\n",
            "          [ 4.6091e-01,  6.7592e-01,  1.5387e-01,  ..., -5.0720e+00,\n",
            "            1.5207e+00,  1.1814e+00],\n",
            "          [-4.1233e-01,  6.7754e-01,  1.0095e+00,  ..., -4.1785e+00,\n",
            "            1.5199e+00,  2.4775e+00],\n",
            "          [-9.0559e-01, -1.9284e-01,  4.6995e-01,  ..., -4.1309e+00,\n",
            "            4.5739e-01,  4.1907e-01]],\n",
            "\n",
            "         [[-2.5078e-03, -1.1751e-02, -3.2980e-02,  ..., -7.1236e-01,\n",
            "            8.1915e-02,  5.2528e-01],\n",
            "          [-1.3493e-01,  1.0060e-01,  3.1911e-02,  ..., -6.4014e-01,\n",
            "           -8.3920e-01, -1.4967e+00],\n",
            "          [-9.3289e-01,  1.3002e-01,  1.1294e-01,  ...,  1.7215e-01,\n",
            "           -1.0886e+00, -8.9015e-01],\n",
            "          ...,\n",
            "          [ 3.2594e-01,  4.8003e-02, -3.5194e-01,  ...,  4.4382e-01,\n",
            "           -3.2389e+00,  8.3028e-01],\n",
            "          [-8.4563e-01,  5.2559e-01, -2.4519e-01,  ...,  1.1181e+00,\n",
            "           -1.2946e+00,  5.6881e-01],\n",
            "          [-1.1524e+00,  1.4730e-01,  3.2063e-01,  ...,  7.1240e-01,\n",
            "            1.9660e-01, -2.9650e-01]],\n",
            "\n",
            "         [[-2.7154e-02,  1.8132e-02, -2.3332e-03,  ...,  1.5377e-01,\n",
            "           -8.7706e-02,  5.7676e-02],\n",
            "          [ 1.3543e+00, -2.6821e-01, -4.6889e-01,  ...,  4.0496e-01,\n",
            "            7.8813e-01,  9.9061e-02],\n",
            "          [ 3.4938e-01,  7.8166e-01, -1.0460e+00,  ...,  3.8591e-01,\n",
            "            1.0742e-01,  5.1895e-01],\n",
            "          ...,\n",
            "          [ 3.6526e-01, -4.7785e-01,  1.5685e+00,  ...,  8.6658e-01,\n",
            "            8.5538e-02,  6.6727e-02],\n",
            "          [ 5.9331e-01, -8.8610e-01,  9.8421e-01,  ...,  1.0303e+00,\n",
            "            5.1791e-02, -7.8725e-01],\n",
            "          [ 2.2424e-01,  1.0231e+00, -3.9496e-01,  ...,  5.1793e-01,\n",
            "           -6.7580e-01, -4.2996e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-4.9753e-03,  1.0969e-02, -4.1857e-04,  ...,  2.1673e-03,\n",
            "           -9.9053e-03,  5.4047e-03],\n",
            "          [ 3.8638e-01,  1.2646e-01,  4.5168e-01,  ...,  8.8921e-01,\n",
            "           -1.2237e-01, -4.0199e-01],\n",
            "          [ 5.2890e-02,  7.0534e-01, -6.1602e-02,  ...,  8.9872e-01,\n",
            "            4.2996e-01, -1.5878e-01],\n",
            "          ...,\n",
            "          [ 2.1267e-01,  7.7456e-02,  8.9943e-01,  ...,  7.9260e-01,\n",
            "           -4.5300e-01, -3.6849e-01],\n",
            "          [-5.6689e-01,  3.3934e-03,  8.6537e-01,  ...,  3.8184e-01,\n",
            "           -1.2751e-01, -1.0787e+00],\n",
            "          [-4.3675e-01, -5.6149e-02,  4.2800e-01,  ...,  6.3531e-01,\n",
            "           -3.6134e-01, -4.0565e-01]],\n",
            "\n",
            "         [[-1.4367e-02,  1.4322e-02, -2.7089e-02,  ...,  2.9682e-02,\n",
            "           -1.3014e-02, -5.5371e-03],\n",
            "          [-3.6014e-01,  3.6819e-01,  1.1001e-02,  ..., -2.5981e-01,\n",
            "           -1.0840e-02,  5.7962e-01],\n",
            "          [-3.2912e-03, -4.9809e-02, -3.5570e-01,  ..., -8.3247e-03,\n",
            "            5.6317e-01, -3.3834e-02],\n",
            "          ...,\n",
            "          [-1.1892e-01,  5.3303e-01,  2.7008e-02,  ...,  2.9399e-01,\n",
            "            2.5324e-01, -1.4057e-01],\n",
            "          [-2.5425e-01, -2.4668e-01,  8.0876e-02,  ...,  7.0565e-02,\n",
            "            3.5120e-01,  3.2895e-01],\n",
            "          [-5.2259e-03,  6.6305e-01,  3.1877e-01,  ..., -1.8316e-02,\n",
            "           -2.4088e-01,  3.5545e-02]],\n",
            "\n",
            "         [[-1.0670e-02,  6.2336e-03,  1.4323e-03,  ..., -1.6694e-03,\n",
            "            4.7251e-03,  8.9127e-03],\n",
            "          [ 4.9151e-01, -4.6726e-02,  1.6797e+00,  ...,  5.6354e-01,\n",
            "           -2.6640e-01,  5.5908e-01],\n",
            "          [-4.3917e-01, -4.1283e-01,  7.8053e-01,  ..., -1.9153e-01,\n",
            "            1.2300e+00,  4.4548e-01],\n",
            "          ...,\n",
            "          [-3.1984e-01, -5.5241e-01,  5.4584e-01,  ...,  4.1649e-01,\n",
            "            8.7336e-01,  5.7892e-01],\n",
            "          [-4.0928e-01, -6.4046e-01,  7.8205e-02,  ...,  4.3803e-01,\n",
            "            4.4885e-01,  8.8920e-01],\n",
            "          [-9.3825e-02, -1.4518e-01,  2.3450e-01,  ...,  1.0475e-01,\n",
            "            5.3845e-01,  1.0773e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.5874e-03,  1.5554e-03, -7.9845e-03,  ..., -1.6038e-02,\n",
            "           -7.9780e-03,  2.7770e-03],\n",
            "          [-1.2028e+00,  2.9778e-01,  5.8390e-01,  ...,  1.4915e+00,\n",
            "           -6.5028e-01,  4.9516e-02],\n",
            "          [ 1.7235e+00, -8.7774e-01, -3.5939e-01,  ..., -2.7186e-01,\n",
            "           -1.2663e+00, -4.1936e-01],\n",
            "          ...,\n",
            "          [ 1.7223e-01, -2.2536e-01,  9.3982e-02,  ...,  9.7080e-01,\n",
            "           -4.0442e-02, -2.3443e-01],\n",
            "          [ 7.7472e-01, -4.5871e-01, -2.7863e-01,  ...,  4.2782e-01,\n",
            "            3.1458e-01,  7.2863e-01],\n",
            "          [-6.0757e-01,  7.7761e-01,  2.9212e-01,  ...,  1.4202e-01,\n",
            "            2.6055e-01,  1.6754e-02]],\n",
            "\n",
            "         [[-3.6152e-03, -1.3456e-02,  1.2156e-02,  ...,  2.0890e-03,\n",
            "            2.0336e-02,  1.3949e-02],\n",
            "          [ 2.0860e-01, -1.3143e+00, -2.1410e-01,  ..., -5.8852e-01,\n",
            "            5.1049e-01, -1.1925e+00],\n",
            "          [ 6.5072e-01, -1.3776e-02, -2.6490e-01,  ..., -4.3068e-01,\n",
            "            9.6760e-02, -1.5384e-01],\n",
            "          ...,\n",
            "          [ 5.9556e-01, -3.8869e-01, -4.9499e-01,  ...,  1.2500e-01,\n",
            "           -3.7160e-01, -4.9426e-01],\n",
            "          [ 4.4365e-01, -1.3590e+00, -1.9708e-01,  ..., -6.6978e-01,\n",
            "            3.9043e-01,  9.1572e-02],\n",
            "          [ 8.0140e-01, -2.9519e-01, -3.7265e-01,  ...,  5.0962e-01,\n",
            "            1.5643e-01,  1.2716e-01]],\n",
            "\n",
            "         [[ 1.1183e-02, -1.0703e-02, -1.6305e-02,  ...,  1.8433e-03,\n",
            "            1.0162e-02, -1.0763e-02],\n",
            "          [ 1.5747e-01, -1.0013e+00, -6.6819e-03,  ..., -7.7514e-01,\n",
            "            1.9330e-02, -4.0709e-01],\n",
            "          [ 9.0620e-02,  2.4792e-01,  4.8967e-01,  ..., -2.2140e-01,\n",
            "            2.2709e-01,  4.9214e-01],\n",
            "          ...,\n",
            "          [ 3.3636e-01,  9.6147e-01,  3.0687e-01,  ...,  8.0878e-01,\n",
            "            1.1429e-01,  1.6303e-01],\n",
            "          [ 2.3971e-01, -1.5221e-01, -2.7695e-01,  ...,  7.6060e-01,\n",
            "            9.0234e-01,  2.2336e-01],\n",
            "          [-1.6528e-02,  9.6575e-01,  4.0428e-01,  ...,  5.9562e-01,\n",
            "            2.5120e-01,  5.8192e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.7053e-02,  1.5309e-04,  1.5663e-02,  ...,  2.2306e-01,\n",
            "           -2.8128e-01, -1.4616e+00],\n",
            "          [-3.5362e-01,  4.1586e-01, -6.0355e-01,  ..., -1.1485e+00,\n",
            "            2.0583e+00,  3.5832e+00],\n",
            "          [ 3.5804e-01, -4.5163e-02,  3.5236e-02,  ..., -8.9297e-02,\n",
            "            1.7002e+00,  3.4338e+00],\n",
            "          ...,\n",
            "          [-1.3075e-02,  4.7423e-02,  4.0846e-01,  ...,  5.9308e-01,\n",
            "            1.9083e+00,  4.2600e+00],\n",
            "          [ 1.7619e-01,  1.9588e-01,  5.7199e-01,  ...,  6.7282e-01,\n",
            "            2.0988e+00,  4.5709e+00],\n",
            "          [ 1.4601e-01,  3.3652e-01,  5.5357e-01,  ..., -1.8203e-01,\n",
            "            1.6080e+00,  5.3961e+00]],\n",
            "\n",
            "         [[-1.7833e-02,  3.7046e-02,  2.7860e-02,  ..., -2.2277e-01,\n",
            "            4.0575e-02,  2.3151e-01],\n",
            "          [-5.0337e-01, -7.5686e-01,  2.4742e-01,  ...,  1.1665e+00,\n",
            "            7.6351e-01,  2.2344e+00],\n",
            "          [-8.5539e-01,  3.3735e-01,  7.3111e-01,  ..., -6.8214e-01,\n",
            "            5.8091e-02, -5.3922e-01],\n",
            "          ...,\n",
            "          [ 8.7165e-02, -2.7664e-01, -8.3352e-01,  ..., -7.9699e-01,\n",
            "            8.1493e-01,  4.8967e-01],\n",
            "          [-8.2183e-02, -4.5164e-01,  7.0946e-01,  ..., -7.4885e-02,\n",
            "            4.8639e-01,  9.9032e-01],\n",
            "          [-1.2777e-01, -5.7303e-01, -2.5431e-01,  ...,  3.2134e-01,\n",
            "            4.5171e-01, -3.6711e-01]],\n",
            "\n",
            "         [[ 3.0729e-02,  1.2574e-02,  8.9481e-03,  ..., -8.1920e-04,\n",
            "           -2.8971e-02, -1.3585e-01],\n",
            "          [-9.5085e-02,  5.2271e-01,  5.4844e-01,  ...,  1.8503e+00,\n",
            "           -5.4786e-01,  3.7559e-01],\n",
            "          [-6.4316e-01, -6.2010e-01,  9.9803e-01,  ...,  1.3584e+00,\n",
            "            1.1862e+00,  1.9299e+00],\n",
            "          ...,\n",
            "          [ 2.1552e-01,  2.6148e-01, -1.8721e-01,  ...,  1.4458e+00,\n",
            "            1.5260e+00,  1.8086e+00],\n",
            "          [-3.7779e-01, -2.4634e-01, -7.9741e-01,  ...,  1.4194e+00,\n",
            "            1.4889e+00,  8.2461e-01],\n",
            "          [-1.3417e+00, -2.1531e-01, -8.7479e-01,  ...,  1.0145e+00,\n",
            "            8.4863e-01,  1.3663e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.5987e-02,  1.4362e-02,  1.4110e-02,  ..., -1.2820e-01,\n",
            "            1.4682e-01,  9.7824e-02],\n",
            "          [ 2.3482e-01, -2.0827e-01, -1.0096e-01,  ..., -6.5417e-01,\n",
            "           -1.2019e+00, -1.0510e+00],\n",
            "          [-8.7446e-01, -6.2965e-01, -6.7663e-01,  ..., -5.7159e-01,\n",
            "           -1.8427e+00, -1.4339e+00],\n",
            "          ...,\n",
            "          [ 8.7473e-01, -6.1258e-02,  1.3352e-01,  ..., -1.4126e-01,\n",
            "           -1.4737e+00, -7.8330e-01],\n",
            "          [ 8.2273e-01, -1.7015e+00,  5.5560e-01,  ...,  4.9519e-01,\n",
            "           -2.2579e+00, -1.1512e+00],\n",
            "          [-3.4827e-02, -4.3978e-01,  1.7050e-01,  ..., -2.2239e-01,\n",
            "           -8.1860e-01, -1.0609e-01]],\n",
            "\n",
            "         [[-1.8806e-06, -3.0222e-02,  1.4874e-02,  ...,  2.1681e-01,\n",
            "           -9.4621e-02, -1.0053e-01],\n",
            "          [ 7.6529e-02,  2.5719e-01,  1.9724e+00,  ..., -5.2864e-01,\n",
            "            4.2104e-01, -9.1449e-01],\n",
            "          [-7.4384e-01, -5.3346e-01, -1.1405e+00,  ...,  3.4632e-01,\n",
            "            1.5885e-01,  1.0611e-01],\n",
            "          ...,\n",
            "          [ 9.9249e-02,  1.6945e-01,  1.2341e+00,  ...,  1.5638e-02,\n",
            "            5.0230e-02, -3.3454e-01],\n",
            "          [-1.0886e+00,  5.4840e-01,  1.4174e+00,  ...,  8.1433e-01,\n",
            "           -1.0394e+00, -3.3629e-02],\n",
            "          [-8.1265e-01, -9.1441e-02,  1.2414e-01,  ...,  5.1392e-01,\n",
            "           -8.0045e-01, -7.3026e-01]],\n",
            "\n",
            "         [[-3.2307e-03,  4.0931e-02, -4.6968e-02,  ..., -3.4466e-02,\n",
            "           -2.7756e-01,  3.2525e-01],\n",
            "          [-3.5341e-01, -4.1343e-01,  3.9389e-01,  ...,  1.7293e+00,\n",
            "            7.3862e-01, -1.7883e+00],\n",
            "          [ 4.2605e-01, -5.9568e-01,  3.4012e-02,  ...,  1.8861e-01,\n",
            "            1.6118e+00, -2.8110e+00],\n",
            "          ...,\n",
            "          [ 5.2230e-01,  8.6101e-01, -8.9412e-01,  ...,  3.4811e-02,\n",
            "            7.7733e-01, -1.5580e+00],\n",
            "          [ 7.6276e-01,  9.7067e-02,  1.3228e-01,  ..., -1.8698e-01,\n",
            "            7.2972e-01, -1.3962e+00],\n",
            "          [ 3.3305e-01, -7.2026e-01,  2.2949e-01,  ...,  3.7023e-01,\n",
            "           -3.7106e-01, -1.0832e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-1.3843e-02,  2.6418e-02,  7.8852e-03,  ..., -1.3778e-02,\n",
            "           -2.3069e-03,  4.7916e-03],\n",
            "          [-7.5190e-01,  5.7637e-01,  4.6790e-01,  ..., -2.4496e-01,\n",
            "            9.7751e-01, -7.6816e-01],\n",
            "          [ 1.1572e-01,  2.2590e-01, -5.4454e-02,  ...,  5.1554e-01,\n",
            "            4.4408e-01, -2.3593e-02],\n",
            "          ...,\n",
            "          [-1.3312e+00,  1.9372e-02, -3.5008e-01,  ..., -2.7785e-01,\n",
            "            8.8314e-01, -3.4190e-01],\n",
            "          [-5.6634e-01, -1.4142e+00, -8.2692e-01,  ..., -5.9475e-01,\n",
            "            8.0743e-01,  3.6150e-01],\n",
            "          [-5.4013e-01, -2.7995e-01, -2.2901e-01,  ...,  2.5171e-01,\n",
            "            9.4625e-01,  1.7415e-01]],\n",
            "\n",
            "         [[-2.6144e-02, -1.4743e-02,  8.9724e-03,  ...,  1.2608e-02,\n",
            "            1.5818e-02, -3.4013e-03],\n",
            "          [-3.8316e-02,  5.7667e-01, -1.1475e+00,  ..., -4.5759e-01,\n",
            "           -4.7608e-01, -4.9399e-01],\n",
            "          [-4.5336e-01, -3.3478e-01, -5.0177e-01,  ..., -2.4154e-01,\n",
            "            3.0715e-01,  4.8391e-01],\n",
            "          ...,\n",
            "          [-1.9791e-01, -5.4185e-01, -1.2693e+00,  ..., -7.6187e-01,\n",
            "            8.2161e-01, -1.9220e-01],\n",
            "          [-6.6970e-01, -7.8245e-01, -9.1481e-01,  ..., -6.8394e-01,\n",
            "            9.5577e-02,  7.7289e-01],\n",
            "          [ 3.3140e-01, -1.4550e-01, -5.4509e-01,  ..., -2.4309e-01,\n",
            "            4.1349e-01, -3.7230e-02]],\n",
            "\n",
            "         [[-1.1387e-02,  1.2088e-02,  1.0188e-02,  ...,  1.2409e-02,\n",
            "            1.7040e-03,  2.7412e-02],\n",
            "          [ 1.5566e+00,  1.4058e-01,  4.7651e-01,  ...,  2.9965e-01,\n",
            "           -8.1874e-02,  2.2216e-01],\n",
            "          [-3.0544e-01, -6.6380e-01, -1.1160e-01,  ...,  3.9554e-01,\n",
            "            2.5563e-01,  2.5635e-01],\n",
            "          ...,\n",
            "          [ 1.8178e-01,  2.8236e-01,  5.0193e-01,  ..., -2.7906e-02,\n",
            "            1.4812e-01, -1.0352e+00],\n",
            "          [-1.1852e-01, -1.3880e-01, -1.2471e-01,  ..., -4.5887e-01,\n",
            "            6.3999e-01, -1.0100e+00],\n",
            "          [ 4.5047e-02, -1.9778e-02,  2.1687e-01,  ...,  1.6000e-01,\n",
            "            2.8294e-01, -8.7367e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3448e-02,  3.2316e-02,  2.7615e-02,  ...,  8.9337e-03,\n",
            "            3.9437e-03,  8.9889e-03],\n",
            "          [ 1.0816e-02,  8.6884e-01,  5.5429e-02,  ..., -1.7204e-01,\n",
            "            6.6090e-01, -7.4899e-01],\n",
            "          [-1.7895e-01,  4.4421e-02, -8.9814e-02,  ..., -9.2007e-01,\n",
            "           -4.6411e-01, -5.0041e-01],\n",
            "          ...,\n",
            "          [ 5.6427e-02,  1.9167e-01, -3.0529e-01,  ..., -3.5149e-01,\n",
            "           -4.7309e-02, -2.6048e-01],\n",
            "          [ 2.2614e-02, -3.1915e-01,  2.7145e-01,  ...,  6.0810e-02,\n",
            "            2.9819e-01, -4.3137e-01],\n",
            "          [-6.4309e-02, -2.6069e-01, -3.0217e-01,  ..., -3.6334e-01,\n",
            "            6.2263e-01, -1.4958e-01]],\n",
            "\n",
            "         [[-1.8092e-02,  1.3742e-02, -2.3184e-02,  ..., -9.6006e-04,\n",
            "           -2.3766e-02,  3.7126e-02],\n",
            "          [-1.8430e-01, -2.1468e-01, -9.9343e-01,  ..., -1.7732e-01,\n",
            "            1.4747e+00, -1.3837e+00],\n",
            "          [-5.7511e-01, -3.3758e-02, -1.2105e+00,  ...,  1.9097e-01,\n",
            "           -1.4167e-01, -8.2787e-01],\n",
            "          ...,\n",
            "          [-2.1120e-01, -2.1719e-01, -1.4536e+00,  ...,  8.9482e-01,\n",
            "            1.1834e+00, -3.5464e-01],\n",
            "          [-7.7403e-01,  4.4985e-01, -9.8694e-01,  ...,  5.1314e-01,\n",
            "            7.8044e-01, -8.7764e-01],\n",
            "          [-3.6719e-02, -1.4400e-01, -7.3801e-01,  ...,  1.5532e+00,\n",
            "            5.5848e-01,  1.3152e-01]],\n",
            "\n",
            "         [[-6.4151e-03, -6.8793e-03, -9.9214e-04,  ..., -1.3977e-02,\n",
            "           -7.0429e-03, -1.1034e-02],\n",
            "          [-6.2807e-01, -1.5274e-02, -1.5628e-01,  ...,  1.5761e+00,\n",
            "            1.1174e+00, -3.7489e-01],\n",
            "          [-5.4513e-01,  2.9946e-01,  9.2585e-03,  ...,  7.0787e-01,\n",
            "           -7.5363e-01, -8.8181e-01],\n",
            "          ...,\n",
            "          [-8.9792e-01, -2.6209e-01, -6.0946e-02,  ...,  7.1115e-01,\n",
            "           -5.8461e-01, -1.0443e+00],\n",
            "          [-1.0834e-02,  6.3508e-01,  2.5961e-01,  ...,  1.0016e+00,\n",
            "            7.9871e-01, -6.3160e-01],\n",
            "          [-8.1456e-01, -1.0234e+00, -6.4761e-01,  ...,  1.3160e-01,\n",
            "           -6.5640e-01, -9.4598e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 2.1754e-03, -2.3582e-02,  1.0639e-03,  ...,  1.7589e-01,\n",
            "            1.1116e-01,  1.4020e-01],\n",
            "          [ 4.0666e-01,  5.1842e-01,  2.4609e-01,  ..., -2.3264e-01,\n",
            "            3.5085e-01, -3.8893e-01],\n",
            "          [ 9.0782e-01,  7.3436e-01, -2.2533e-01,  ..., -2.4423e-01,\n",
            "           -6.1727e-01, -7.1620e-01],\n",
            "          ...,\n",
            "          [-6.4258e-01, -3.1771e-01, -1.2017e-01,  ...,  4.1816e-01,\n",
            "           -8.1602e-01,  5.4827e-01],\n",
            "          [-1.6494e-01,  1.2365e+00,  3.1590e-01,  ...,  3.8172e-01,\n",
            "           -6.9909e-01,  7.0989e-01],\n",
            "          [-3.2256e-01,  1.3813e+00,  1.3563e-01,  ...,  8.4424e-01,\n",
            "           -5.4984e-02,  6.1669e-01]],\n",
            "\n",
            "         [[-1.1425e-02, -1.7402e-02, -1.1900e-04,  ..., -1.7520e-01,\n",
            "            1.8259e-01, -2.1728e-01],\n",
            "          [ 1.0623e-01, -2.4691e-01, -3.8389e-01,  ...,  1.3574e+00,\n",
            "            7.0670e-01,  1.2685e+00],\n",
            "          [-1.6178e-01, -1.0372e+00, -6.7899e-02,  ...,  4.9099e-01,\n",
            "           -2.4688e-01,  1.8731e+00],\n",
            "          ...,\n",
            "          [ 2.3346e-01,  1.7472e+00, -3.0025e-01,  ...,  6.8698e-01,\n",
            "           -1.1309e-02,  2.2859e+00],\n",
            "          [-4.8516e-01,  6.8631e-01, -6.9661e-02,  ...,  2.1365e-01,\n",
            "            1.3666e-01,  1.9648e+00],\n",
            "          [ 2.2427e-01,  1.8155e-01, -5.9773e-01,  ...,  6.9264e-01,\n",
            "           -2.6254e-01,  1.7475e+00]],\n",
            "\n",
            "         [[-6.0227e-03, -1.8924e-03, -2.0775e-02,  ...,  3.8510e-02,\n",
            "            1.6287e-01, -1.1156e-01],\n",
            "          [ 5.1327e-01, -9.0213e-01,  1.2952e-01,  ...,  2.8268e-01,\n",
            "            1.9476e-01, -7.0663e-01],\n",
            "          [-3.6097e-01,  2.5799e-01,  6.2206e-01,  ...,  7.5617e-01,\n",
            "           -7.6072e-01,  6.8143e-01],\n",
            "          ...,\n",
            "          [ 2.9620e-01, -7.0408e-01, -1.0484e+00,  ..., -2.8102e-01,\n",
            "            1.1897e+00, -4.0666e-01],\n",
            "          [-1.8557e-01, -7.2557e-01, -1.3971e-01,  ...,  4.6745e-02,\n",
            "            4.6384e-01, -1.2009e+00],\n",
            "          [-1.7876e-01,  5.8734e-02,  2.0457e-01,  ..., -9.0265e-01,\n",
            "           -1.0428e-02,  7.1412e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.0478e-02, -2.6519e-02, -6.3910e-03,  ...,  2.1732e-01,\n",
            "            1.3191e-01, -5.3573e-01],\n",
            "          [ 5.5519e-01,  6.0165e-01,  4.6600e-01,  ..., -1.1437e+00,\n",
            "           -1.0450e+00,  1.1480e+00],\n",
            "          [-5.3057e-01,  8.7396e-01,  7.2955e-01,  ..., -1.3650e+00,\n",
            "           -6.0822e-01,  1.2246e+00],\n",
            "          ...,\n",
            "          [ 4.6007e-01, -6.5547e-01, -6.5153e-01,  ..., -1.5274e+00,\n",
            "           -2.0777e+00,  1.1747e+00],\n",
            "          [ 1.9673e-01, -3.0308e-01, -3.1550e-01,  ..., -2.3803e+00,\n",
            "           -1.4490e+00,  1.0668e+00],\n",
            "          [ 1.4973e-01, -4.2699e-01,  3.6589e-01,  ..., -8.5276e-01,\n",
            "           -1.0585e+00,  8.9055e-01]],\n",
            "\n",
            "         [[-1.1668e-02, -2.7609e-02,  1.0476e-02,  ...,  2.6917e+00,\n",
            "           -3.2976e-01, -5.0223e-01],\n",
            "          [ 1.2979e+00, -6.0985e-01,  9.3414e-01,  ..., -8.6697e+00,\n",
            "            1.4643e+00,  2.2138e-01],\n",
            "          [ 4.9238e+00, -2.0241e-01,  1.4238e+00,  ..., -9.5854e+00,\n",
            "           -1.2740e+00,  1.2997e+00],\n",
            "          ...,\n",
            "          [-3.0097e+00,  9.8350e-01, -1.8448e+00,  ..., -9.9819e+00,\n",
            "           -1.7902e+00,  1.5406e+00],\n",
            "          [ 1.1845e-01,  1.6257e+00, -4.8019e-02,  ..., -1.0768e+01,\n",
            "           -1.2370e+00,  8.0581e-01],\n",
            "          [ 1.9973e+00,  1.7869e-01,  7.1917e-01,  ..., -9.1931e+00,\n",
            "           -2.5141e+00,  9.1661e-01]],\n",
            "\n",
            "         [[ 9.9123e-03, -1.0099e-02, -6.8640e-03,  ..., -1.8621e+00,\n",
            "            1.3641e-02,  5.7515e-01],\n",
            "          [-2.0833e-01,  5.1969e-01, -6.1017e-01,  ...,  9.9644e+00,\n",
            "            2.5183e+00, -8.7624e-01],\n",
            "          [-2.9358e-01, -1.2089e-01,  4.2113e-02,  ...,  9.5205e+00,\n",
            "           -8.0751e-01, -2.0114e+00],\n",
            "          ...,\n",
            "          [ 4.9731e-01,  2.9807e-01, -5.7033e-04,  ...,  1.0492e+01,\n",
            "           -8.4531e-01, -1.3932e+00],\n",
            "          [-4.0314e-01,  1.8445e-01, -1.8945e-01,  ...,  1.0713e+01,\n",
            "           -5.2132e-03, -1.9398e+00],\n",
            "          [-2.7807e-01,  3.6539e-01,  1.5109e-01,  ...,  9.9887e+00,\n",
            "           -5.5486e-01, -7.9150e-01]]]], grad_fn=<AddBackward0>), tensor([[[[-7.4234e-02, -1.9183e-02,  4.9242e-02,  ..., -5.7378e-02,\n",
            "           -2.7381e-02,  5.8045e-02],\n",
            "          [ 3.7602e-01, -1.2906e+00,  1.1950e+00,  ...,  1.0519e+00,\n",
            "            7.2192e-01, -1.8015e+00],\n",
            "          [ 8.7321e-01, -5.3419e-01,  8.6677e-01,  ...,  1.6288e+00,\n",
            "            6.9151e-01, -5.9424e-01],\n",
            "          ...,\n",
            "          [ 8.4056e-01, -5.9590e-01,  7.0915e-01,  ...,  1.8240e+00,\n",
            "            5.5368e-01, -4.7467e-02],\n",
            "          [ 5.0565e-01, -6.8915e-01,  6.1378e-02,  ...,  1.7919e+00,\n",
            "            2.7592e-01, -2.3822e-01],\n",
            "          [ 2.6997e-01, -6.2214e-01,  6.2648e-01,  ...,  1.7221e+00,\n",
            "            3.1709e-01, -3.7818e-01]],\n",
            "\n",
            "         [[-3.0299e-04,  2.0323e-02,  2.1398e-02,  ...,  8.6559e-03,\n",
            "           -6.6064e-03,  9.8819e-03],\n",
            "          [-9.2623e-01, -1.2973e+00,  9.7035e-01,  ..., -4.0440e-01,\n",
            "           -1.4577e+00,  9.1963e-01],\n",
            "          [ 4.9523e-01,  4.4291e-01,  5.4704e-01,  ..., -5.4853e-01,\n",
            "           -4.2726e-01, -2.8367e-01],\n",
            "          ...,\n",
            "          [ 2.2549e-01,  3.6695e-02,  4.7408e-01,  ..., -1.1405e-01,\n",
            "           -7.6529e-03, -4.7207e-03],\n",
            "          [ 5.5043e-01, -3.6059e-01,  1.0471e+00,  ...,  8.4583e-01,\n",
            "           -6.7404e-01, -6.6610e-02],\n",
            "          [-1.1870e+00, -1.5215e-02,  5.2742e-01,  ...,  5.3477e-01,\n",
            "           -6.2429e-01, -3.4560e-01]],\n",
            "\n",
            "         [[ 1.9417e-02,  3.3761e-02, -1.7069e-02,  ...,  1.3965e-02,\n",
            "           -2.6510e-02,  9.6966e-04],\n",
            "          [-1.0691e+00,  3.7312e-01,  7.2071e-01,  ..., -6.1200e-01,\n",
            "           -1.4032e+00, -1.3972e+00],\n",
            "          [ 3.9714e-01,  1.0121e-01, -5.2260e-01,  ...,  5.4231e-01,\n",
            "            1.7304e-01, -8.7768e-01],\n",
            "          ...,\n",
            "          [ 2.5655e-01,  2.1452e-01,  5.8939e-01,  ...,  6.7606e-01,\n",
            "           -3.6254e-01, -4.2698e-01],\n",
            "          [-7.3245e-01, -1.6897e-02,  8.8818e-01,  ...,  7.0730e-03,\n",
            "            3.2748e-01,  1.0859e+00],\n",
            "          [-4.3818e-01, -1.6473e-01,  1.4076e-01,  ...,  2.1445e-01,\n",
            "            2.3524e-01, -4.3105e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.1288e-03, -8.0989e-04, -1.9101e-02,  ...,  7.6021e-02,\n",
            "           -6.5750e-03,  2.1998e-02],\n",
            "          [ 5.7329e-01,  4.6454e-01, -1.0866e+00,  ...,  9.3967e-01,\n",
            "           -3.5380e-01, -5.7663e-01],\n",
            "          [ 2.7893e-01,  9.6553e-01, -1.6743e-01,  ..., -2.1008e-01,\n",
            "            4.6691e-01,  1.9859e-02],\n",
            "          ...,\n",
            "          [ 4.4317e-01,  2.2629e+00, -8.8808e-01,  ..., -1.8293e-01,\n",
            "            4.5622e-01, -2.0124e-01],\n",
            "          [ 6.0880e-01,  2.0028e+00, -1.8227e-01,  ..., -7.8158e-01,\n",
            "            4.0605e-01, -2.8108e-01],\n",
            "          [-8.9531e-02,  1.5541e+00, -6.3786e-01,  ..., -2.5063e-01,\n",
            "           -1.1177e-01,  1.5148e-01]],\n",
            "\n",
            "         [[-4.1274e-02,  6.3926e-03, -4.1628e-02,  ...,  1.2320e-02,\n",
            "            2.4485e-03, -8.6583e-03],\n",
            "          [-3.6001e-01,  1.0264e-02, -7.3239e-02,  ...,  6.6544e-02,\n",
            "           -1.3681e-01, -6.3300e-01],\n",
            "          [ 2.3968e-01, -3.1876e-01,  2.2616e-01,  ..., -1.6457e-02,\n",
            "            2.6154e-01,  4.8995e-01],\n",
            "          ...,\n",
            "          [ 1.3359e-01, -2.0933e-01,  6.8619e-01,  ...,  9.4925e-01,\n",
            "            6.7877e-01, -3.3377e-01],\n",
            "          [ 2.5216e-01, -1.6714e-01,  5.9587e-01,  ...,  1.4199e+00,\n",
            "            6.8904e-01, -2.5836e-01],\n",
            "          [ 1.1185e-01, -5.3506e-02,  6.5806e-01,  ...,  1.0536e+00,\n",
            "            7.6012e-01, -8.4947e-01]],\n",
            "\n",
            "         [[ 6.2006e-03,  3.6933e-03, -1.9621e-02,  ..., -1.7221e-02,\n",
            "            4.6690e-03,  4.7243e-03],\n",
            "          [ 7.3403e-01, -6.7586e-01,  3.4719e-01,  ...,  2.8725e-01,\n",
            "            9.0454e-02,  1.0107e+00],\n",
            "          [-6.3954e-02, -2.9507e-01,  5.6875e-01,  ...,  5.2244e-02,\n",
            "           -8.7541e-01,  4.8243e-01],\n",
            "          ...,\n",
            "          [-5.7652e-01, -8.0065e-04, -3.9070e-01,  ...,  1.9827e-01,\n",
            "           -2.4166e-02,  5.4619e-01],\n",
            "          [-6.3309e-01,  2.6394e-01,  1.1401e-01,  ..., -2.6189e-01,\n",
            "           -5.2808e-01,  1.1886e+00],\n",
            "          [-3.9082e-01,  4.4488e-01,  1.6997e-01,  ...,  3.7784e-01,\n",
            "           -5.7476e-01,  8.5794e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-8.7492e-03, -1.8177e-02,  4.3471e-03,  ...,  6.1614e-02,\n",
            "           -1.4217e-01, -5.6232e-02],\n",
            "          [-9.9142e-01, -1.0677e-01,  5.8616e-01,  ...,  2.2853e+00,\n",
            "           -7.0819e-01,  1.0612e+00],\n",
            "          [-1.7073e+00,  1.4673e-01,  6.2658e-01,  ..., -4.4954e-02,\n",
            "           -2.0449e-01,  4.8517e-02],\n",
            "          ...,\n",
            "          [-5.2677e-01, -3.9379e-01, -7.5381e-01,  ..., -1.4372e+00,\n",
            "            4.3251e-01, -8.7932e-02],\n",
            "          [-2.3524e+00, -9.9186e-01, -1.5981e-01,  ..., -1.3134e+00,\n",
            "           -1.2334e+00,  3.0291e-01],\n",
            "          [-1.7548e+00, -1.1654e+00, -2.5157e-01,  ..., -7.7130e-01,\n",
            "           -5.7038e-02,  1.0553e-01]],\n",
            "\n",
            "         [[ 2.3533e-02,  2.0440e-02, -1.6176e-02,  ..., -1.3486e-02,\n",
            "            1.2831e-01, -2.2733e-01],\n",
            "          [ 6.9836e-02, -1.8231e-01,  8.0219e-01,  ..., -8.8657e-01,\n",
            "           -8.3521e-01,  3.5306e-01],\n",
            "          [ 9.8155e-01, -4.2141e-01,  2.4891e-01,  ...,  7.6337e-01,\n",
            "           -7.6437e-01,  1.4042e-01],\n",
            "          ...,\n",
            "          [-2.9437e-01,  5.3405e-01, -1.0326e+00,  ...,  6.7075e-01,\n",
            "           -1.0710e+00,  7.6009e-01],\n",
            "          [ 5.4663e-01,  6.8164e-01, -2.0378e-01,  ...,  6.6937e-01,\n",
            "           -2.0673e+00,  7.6775e-01],\n",
            "          [ 9.5434e-01,  9.1199e-02,  3.5248e-01,  ...,  5.1321e-01,\n",
            "           -5.6347e-01,  9.9788e-02]],\n",
            "\n",
            "         [[-5.0065e-02,  4.5638e-03,  3.7694e-02,  ..., -2.9123e-02,\n",
            "            2.6154e-01, -8.9883e-01],\n",
            "          [-2.3966e+00,  1.6610e-01,  1.3012e+00,  ..., -4.4447e-01,\n",
            "           -5.5615e-01,  3.8378e+00],\n",
            "          [-5.8649e-02, -1.7705e+00,  1.1246e+00,  ..., -1.7807e+00,\n",
            "            1.3957e-01,  3.6538e+00],\n",
            "          ...,\n",
            "          [-2.3403e+00,  1.5511e+00, -1.1831e+00,  ..., -1.1949e+00,\n",
            "           -6.7989e-02,  3.8724e+00],\n",
            "          [-3.2319e+00,  1.2555e+00, -2.0454e-01,  ..., -1.2566e+00,\n",
            "           -3.4722e-01,  3.9128e+00],\n",
            "          [-1.2737e+00,  6.0795e-01, -3.5442e-01,  ..., -9.2547e-01,\n",
            "            9.0997e-01,  3.3313e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.5706e-02,  2.7838e-02,  1.0221e-02,  ...,  2.5245e-01,\n",
            "            4.8144e-02, -1.0986e-01],\n",
            "          [-1.8469e+00, -2.1282e-01,  2.1483e-01,  ...,  2.9630e+00,\n",
            "           -7.8071e-01, -2.2849e+00],\n",
            "          [-1.9859e+00,  1.2278e-01,  3.8126e-02,  ...,  3.3019e-01,\n",
            "            5.4609e-01, -6.7740e-01],\n",
            "          ...,\n",
            "          [ 7.3800e-01, -3.0802e-01, -1.4230e-01,  ..., -1.4101e+00,\n",
            "           -7.6843e-01,  1.9775e+00],\n",
            "          [-8.6776e-01, -1.0285e-01, -5.4780e-02,  ..., -1.5862e+00,\n",
            "            5.9340e-01,  7.3093e-01],\n",
            "          [-1.0351e+00,  4.9371e-01,  3.3565e-03,  ..., -1.7159e+00,\n",
            "           -9.2717e-01,  7.9161e-01]],\n",
            "\n",
            "         [[-1.7237e-02,  3.1562e-03, -2.7392e-02,  ..., -3.9827e-04,\n",
            "            3.8092e-01,  4.8573e-02],\n",
            "          [-4.8740e-01,  5.3589e-01,  1.1664e+00,  ...,  6.1200e-01,\n",
            "           -6.9050e-01, -2.1593e+00],\n",
            "          [-3.5768e-02, -1.4352e+00,  4.4821e-01,  ..., -1.4772e-01,\n",
            "           -1.4197e+00, -8.6264e-01],\n",
            "          ...,\n",
            "          [ 1.0425e+00,  8.6473e-01, -7.7900e-01,  ..., -2.7063e-01,\n",
            "           -1.0543e+00, -7.2694e-01],\n",
            "          [ 7.0092e-02,  4.5787e-01,  5.0053e-01,  ..., -5.4201e-01,\n",
            "           -9.4168e-01, -2.8648e-01],\n",
            "          [-5.6805e-01, -4.7763e-01,  8.5329e-01,  ...,  6.2482e-02,\n",
            "           -7.5973e-01, -1.2099e+00]],\n",
            "\n",
            "         [[ 2.5083e-02, -1.4120e-02,  1.3261e-02,  ...,  4.2450e-02,\n",
            "            3.4504e-01, -1.6250e+00],\n",
            "          [ 1.4060e+00,  1.1264e+00, -2.0490e-01,  ...,  2.2521e-01,\n",
            "           -1.3396e+00,  7.4256e+00],\n",
            "          [-1.6477e-01,  9.9639e-01,  7.1812e-01,  ..., -9.0964e-02,\n",
            "           -1.2715e+00,  6.9325e+00],\n",
            "          ...,\n",
            "          [ 1.2660e+00, -3.8899e-01, -6.7416e-01,  ...,  4.0328e-01,\n",
            "           -9.3764e-01,  7.5199e+00],\n",
            "          [ 1.0076e+00, -3.4168e-02, -3.7565e-01,  ...,  7.3579e-01,\n",
            "           -8.4845e-01,  7.4771e+00],\n",
            "          [ 2.4811e-02, -1.9939e-02,  5.2082e-02,  ..., -2.5578e-01,\n",
            "           -1.1141e+00,  7.4042e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 5.5127e-02, -4.6126e-03, -4.7991e-03,  ..., -6.5161e-03,\n",
            "           -9.9204e-03,  2.3386e-02],\n",
            "          [ 7.3001e-01, -6.5409e-01, -4.8263e-01,  ..., -6.2037e-01,\n",
            "            1.1215e+00,  3.6508e-01],\n",
            "          [ 7.3483e-01, -2.7977e-01, -3.6764e-02,  ..., -6.0370e-01,\n",
            "            3.8192e-01,  8.7776e-01],\n",
            "          ...,\n",
            "          [ 4.6527e-01, -3.0419e-01, -6.1847e-02,  ..., -1.1091e+00,\n",
            "            2.6568e-01,  7.5430e-01],\n",
            "          [ 3.4039e-01, -2.6386e-01, -3.5601e-02,  ..., -5.7406e-01,\n",
            "           -1.2648e-01,  8.5877e-01],\n",
            "          [ 1.0261e+00, -8.5615e-02,  9.1544e-03,  ..., -9.2409e-01,\n",
            "           -3.5702e-01,  6.7238e-01]],\n",
            "\n",
            "         [[-1.8199e-02, -2.2323e-02, -3.5449e-03,  ..., -3.3677e-03,\n",
            "           -2.5830e-03, -1.0015e-04],\n",
            "          [-7.9512e-01,  4.1726e-01, -4.1423e-01,  ...,  3.3168e-01,\n",
            "           -8.5317e-02, -6.3533e-01],\n",
            "          [ 8.1935e-01,  3.1969e-01,  3.9004e-01,  ...,  3.9713e-01,\n",
            "           -1.6917e-02,  6.4409e-01],\n",
            "          ...,\n",
            "          [ 8.4964e-01,  8.2594e-01, -5.3457e-01,  ...,  4.4013e-01,\n",
            "           -2.2926e-01,  4.4582e-01],\n",
            "          [ 2.8272e-02,  1.7948e-01, -4.5253e-01,  ...,  2.9028e-01,\n",
            "           -9.6213e-01,  8.2218e-01],\n",
            "          [ 5.9104e-01,  6.9860e-01, -2.0929e-01,  ...,  1.5038e-01,\n",
            "           -2.8002e-01,  6.4508e-01]],\n",
            "\n",
            "         [[ 1.5470e-03,  2.3239e-02,  1.7707e-02,  ..., -2.6632e-03,\n",
            "            1.2765e-02, -6.6515e-03],\n",
            "          [ 7.5614e-01,  5.4353e-01, -6.8688e-02,  ..., -1.1229e+00,\n",
            "           -1.5065e+00,  1.1942e-01],\n",
            "          [ 3.9326e-01, -5.9225e-02,  5.9545e-01,  ..., -3.8987e-01,\n",
            "            3.9832e-01,  4.8121e-01],\n",
            "          ...,\n",
            "          [-1.1875e-02, -8.0696e-01,  7.1222e-01,  ..., -2.7733e-02,\n",
            "           -1.9002e-01,  1.0560e+00],\n",
            "          [-3.1632e-01, -9.0305e-01,  1.0580e+00,  ..., -3.2473e-01,\n",
            "            2.5141e-01,  5.4151e-01],\n",
            "          [ 6.1498e-02, -4.3651e-01,  1.8976e-01,  ..., -1.6966e-01,\n",
            "            1.6461e-01,  7.7729e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2508e-02,  1.7834e-02,  1.5820e-02,  ..., -5.5430e-03,\n",
            "           -2.1499e-02, -1.6692e-02],\n",
            "          [ 6.5241e-01,  1.9858e+00, -2.2317e+00,  ...,  1.6147e-01,\n",
            "           -5.8471e-02, -7.3555e-01],\n",
            "          [-3.0671e-01,  8.1793e-01,  2.9512e-01,  ..., -6.0203e-01,\n",
            "           -1.2445e+00, -4.1629e-01],\n",
            "          ...,\n",
            "          [-9.6147e-02, -1.2467e+00, -4.4572e-01,  ..., -1.2272e+00,\n",
            "           -6.8319e-01,  4.2139e-01],\n",
            "          [ 8.6334e-01,  2.8105e-01,  8.6815e-01,  ..., -1.6250e+00,\n",
            "            6.0423e-01, -1.1116e-01],\n",
            "          [ 5.9662e-01, -4.5382e-01, -1.7370e-01,  ..., -8.1849e-01,\n",
            "            3.6576e-01, -6.1320e-01]],\n",
            "\n",
            "         [[-2.7362e-02,  1.2103e-02, -4.6523e-03,  ..., -3.2106e-02,\n",
            "            7.4573e-03, -9.5462e-02],\n",
            "          [-7.6504e-02, -1.2860e+00,  1.8515e-01,  ...,  2.9831e-01,\n",
            "            8.4467e-01, -1.0168e+00],\n",
            "          [-2.4231e-02, -7.4597e-01,  4.1708e-01,  ...,  1.7013e+00,\n",
            "           -5.3841e-02,  1.2069e-01],\n",
            "          ...,\n",
            "          [ 3.7937e-02, -1.1972e+00,  1.8734e-01,  ...,  9.1324e-01,\n",
            "            1.0795e-01,  2.5502e-01],\n",
            "          [ 1.0951e-01, -9.3146e-01,  6.5828e-01,  ...,  1.6379e+00,\n",
            "            2.5660e-01,  1.5146e-01],\n",
            "          [-2.8645e-01, -2.3173e-01,  3.1810e-01,  ...,  2.7683e-01,\n",
            "           -7.7353e-01,  1.2377e-01]],\n",
            "\n",
            "         [[-2.4368e-02, -5.0381e-03, -2.4568e-02,  ..., -8.9524e-03,\n",
            "           -2.8363e-03, -8.3831e-03],\n",
            "          [-1.0900e-01,  2.8198e-01, -1.5847e-01,  ..., -6.6118e-03,\n",
            "            3.0181e-01,  1.7527e-01],\n",
            "          [-4.7481e-02,  1.7710e-01, -1.1468e-01,  ..., -2.6820e-01,\n",
            "           -8.1824e-01,  1.5473e+00],\n",
            "          ...,\n",
            "          [ 6.3821e-01, -3.0173e-01, -2.9044e-01,  ...,  2.6825e-01,\n",
            "           -9.8157e-01,  4.6467e-01],\n",
            "          [ 6.6190e-01, -8.3808e-01,  7.1047e-02,  ..., -6.0270e-01,\n",
            "           -1.0557e+00,  7.2104e-01],\n",
            "          [ 7.2283e-02, -1.0674e-01, -4.6729e-03,  ...,  4.0963e-01,\n",
            "           -6.0506e-01,  9.6278e-02]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[ 4.5629e-03,  4.0769e-02,  1.7304e-02,  ...,  1.4313e-01,\n",
            "            5.5468e-01, -2.0174e-01],\n",
            "          [ 2.6147e-01,  1.0388e+00,  8.6712e-01,  ..., -4.4094e-01,\n",
            "           -2.2699e+00,  2.4418e-01],\n",
            "          [ 1.7815e+00,  2.5757e-02, -3.9516e-01,  ..., -3.2839e-01,\n",
            "           -2.5177e+00,  8.4683e-02],\n",
            "          ...,\n",
            "          [-7.0281e-01,  1.9439e-01,  3.1661e-01,  ..., -8.2742e-01,\n",
            "           -2.4293e+00, -1.5334e-01],\n",
            "          [ 1.7874e-01, -5.4791e-02,  2.8212e-01,  ..., -4.9250e-01,\n",
            "           -1.9781e+00, -1.1197e+00],\n",
            "          [ 6.8389e-01,  6.8213e-01,  6.4552e-01,  ..., -4.4620e-01,\n",
            "           -1.2408e+00,  4.6258e-01]],\n",
            "\n",
            "         [[-2.9714e-02, -8.9774e-03, -1.3764e-02,  ..., -1.1405e-01,\n",
            "            2.6634e-02, -1.5023e-01],\n",
            "          [ 6.2756e-01,  3.3652e-01,  1.1250e-02,  ...,  1.0655e+00,\n",
            "            1.4070e-01,  5.4735e-01],\n",
            "          [ 8.5071e-01, -6.5712e-01,  1.7642e-01,  ..., -1.1729e+00,\n",
            "            8.1329e-02,  8.9931e-01],\n",
            "          ...,\n",
            "          [ 4.3589e-01,  6.7750e-01, -3.5011e-02,  ..., -6.4347e-01,\n",
            "            5.5061e-01,  1.1366e+00],\n",
            "          [ 1.3473e+00, -1.4731e-01,  1.1650e-01,  ..., -2.3195e+00,\n",
            "           -4.9750e-01,  1.8139e+00],\n",
            "          [ 1.4947e+00, -3.6235e-01,  2.4616e-01,  ..., -7.6576e-02,\n",
            "           -2.5919e-01,  1.5368e+00]],\n",
            "\n",
            "         [[-1.5470e-02, -3.7138e-02,  1.3252e-02,  ...,  4.6733e-01,\n",
            "            3.2884e-01, -2.5518e-01],\n",
            "          [-1.3232e+00,  3.8064e-01,  6.1129e-01,  ..., -9.6518e-04,\n",
            "           -4.6790e+00,  1.5598e+00],\n",
            "          [-4.5384e-01, -6.3170e-01, -9.2229e-01,  ..., -5.9378e-01,\n",
            "           -3.5217e+00,  9.5798e-02],\n",
            "          ...,\n",
            "          [-7.5992e-01,  3.0902e-02,  1.1349e+00,  ..., -3.8450e-01,\n",
            "           -3.6826e+00, -8.4987e-01],\n",
            "          [-1.1568e+00, -3.2838e-01,  1.5316e+00,  ..., -6.9333e-01,\n",
            "           -4.2113e+00, -9.5158e-01],\n",
            "          [-1.1138e+00, -4.2610e-01,  3.6867e-01,  ..., -4.2471e-01,\n",
            "           -2.4057e+00,  2.8375e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4124e-02,  2.6792e-02, -3.3667e-02,  ..., -1.3523e+00,\n",
            "            1.9039e-01, -4.6358e-01],\n",
            "          [ 3.2353e-01,  4.4691e-01,  2.4325e-01,  ...,  3.7092e+00,\n",
            "           -2.4688e+00, -1.5918e+00],\n",
            "          [-7.7072e-01,  3.7990e-01,  2.5451e-01,  ...,  4.4396e+00,\n",
            "           -1.3989e+00, -3.6102e-01],\n",
            "          ...,\n",
            "          [ 6.6326e-01,  1.2229e+00,  3.7597e-01,  ...,  4.6610e+00,\n",
            "           -1.5685e+00, -5.2102e-01],\n",
            "          [ 7.1718e-01,  1.1077e+00,  4.9945e-01,  ...,  4.8181e+00,\n",
            "           -1.6963e+00, -5.4118e-01],\n",
            "          [-4.5444e-01,  6.1913e-01,  5.9432e-02,  ...,  4.0121e+00,\n",
            "           -9.4828e-01, -4.8030e-01]],\n",
            "\n",
            "         [[ 1.8616e-02,  1.8692e-02, -2.6537e-02,  ..., -2.7586e-02,\n",
            "           -1.2598e-02,  3.2950e-02],\n",
            "          [-7.5134e-01,  5.2733e-01,  1.4630e+00,  ...,  1.0038e-01,\n",
            "           -1.5035e+00,  1.8599e+00],\n",
            "          [ 3.8368e-02,  2.7753e-01, -2.7223e-01,  ...,  6.9737e-01,\n",
            "            1.5798e-01,  1.9918e+00],\n",
            "          ...,\n",
            "          [-1.4079e+00,  1.1634e+00, -7.8548e-01,  ...,  7.3693e-02,\n",
            "            3.0771e-01,  2.7098e+00],\n",
            "          [ 4.6633e-01,  1.5984e+00,  1.0934e-01,  ...,  6.3200e-02,\n",
            "            1.6633e-03,  2.9851e+00],\n",
            "          [ 6.8941e-01,  9.0145e-01,  6.2366e-01,  ..., -2.9245e-02,\n",
            "           -5.8460e-01,  2.7648e+00]],\n",
            "\n",
            "         [[-2.3876e-02, -8.9947e-03,  1.6534e-03,  ..., -2.2046e-01,\n",
            "           -3.5006e-02, -1.4778e-01],\n",
            "          [ 2.0001e+00, -1.3676e+00,  7.1847e-01,  ...,  6.6951e-01,\n",
            "           -6.5322e-01,  2.0100e+00],\n",
            "          [ 5.0433e-01, -2.0090e+00,  4.7812e-01,  ..., -5.4232e-01,\n",
            "            5.6619e-01,  1.0166e+00],\n",
            "          ...,\n",
            "          [ 1.6016e+00,  1.5561e+00, -3.7935e-01,  ..., -4.1064e-01,\n",
            "            1.4729e-01,  1.9711e-01],\n",
            "          [ 1.7758e+00, -4.1974e-01,  1.1480e-01,  ...,  4.8992e-01,\n",
            "            2.9643e-01,  1.0155e+00],\n",
            "          [ 6.5078e-01, -1.7035e+00,  1.1951e+00,  ..., -5.4671e-02,\n",
            "            7.7543e-01, -3.0108e-01]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.5527e-02, -1.6346e-02,  3.0143e-02,  ..., -1.8927e-02,\n",
            "           -2.3628e-02, -1.1505e-02],\n",
            "          [ 1.2312e+00,  1.0212e+00, -7.0892e-01,  ...,  1.1144e+00,\n",
            "            3.0576e-01, -5.3397e-01],\n",
            "          [ 8.7457e-01,  2.8670e-01,  1.4761e+00,  ..., -4.4363e-01,\n",
            "            3.6492e-01,  1.0775e+00],\n",
            "          ...,\n",
            "          [ 1.0622e+00,  3.4749e-01,  1.0383e+00,  ..., -5.8980e-01,\n",
            "            5.7821e-01,  7.3241e-01],\n",
            "          [ 1.6330e+00, -4.2277e-01,  1.3616e+00,  ..., -8.8067e-01,\n",
            "            5.5956e-02,  4.1616e-01],\n",
            "          [ 4.0690e-01,  3.0616e-01,  1.0580e-01,  ..., -8.5224e-01,\n",
            "            4.2470e-01, -1.1868e-01]],\n",
            "\n",
            "         [[-1.1721e-02, -1.6178e-02,  9.3024e-03,  ...,  9.4064e-03,\n",
            "            3.0621e-03, -1.4206e-02],\n",
            "          [ 2.0338e-01, -8.6781e-01, -3.9962e-01,  ..., -1.1740e-01,\n",
            "           -7.0464e-01,  4.1617e-01],\n",
            "          [ 1.1829e-02, -7.1665e-01,  2.3586e-01,  ...,  1.5905e-01,\n",
            "           -1.3743e-01, -4.5100e-01],\n",
            "          ...,\n",
            "          [ 3.2881e-01, -2.3202e-01, -8.2235e-01,  ...,  2.8368e-01,\n",
            "           -3.1456e-01, -3.4842e-01],\n",
            "          [ 1.8226e-01, -5.2474e-01, -3.1990e-01,  ..., -1.0730e-01,\n",
            "           -3.8812e-01, -7.1254e-01],\n",
            "          [ 1.8281e-02, -7.8772e-01, -8.6498e-01,  ...,  2.2293e-01,\n",
            "            1.5296e-01,  2.6964e-01]],\n",
            "\n",
            "         [[-1.2774e-03, -2.9002e-02,  1.5686e-02,  ..., -3.9945e-02,\n",
            "            9.0253e-03,  2.7216e-02],\n",
            "          [-6.8517e-01, -3.7376e-01,  4.1892e-01,  ...,  5.4266e-01,\n",
            "           -9.7134e-01,  2.4807e-01],\n",
            "          [-2.2761e-01, -9.3449e-01,  2.0662e-02,  ...,  1.5885e+00,\n",
            "           -6.9918e-01, -7.2962e-01],\n",
            "          ...,\n",
            "          [-8.0164e-01, -8.3080e-01,  3.9259e-01,  ...,  2.4210e+00,\n",
            "           -4.7939e-01, -2.9633e-01],\n",
            "          [-1.4124e-01, -7.5389e-01, -2.9581e-01,  ...,  1.4755e+00,\n",
            "           -8.1822e-01,  4.8186e-01],\n",
            "          [-4.6114e-01, -1.0267e+00, -6.3721e-02,  ...,  2.5838e+00,\n",
            "            7.7858e-03, -8.9636e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.4580e-02,  2.4411e-02, -2.1341e-02,  ...,  7.5322e-03,\n",
            "            2.1886e-02, -1.5487e-02],\n",
            "          [-2.5111e-01,  3.8824e-03, -9.0510e-01,  ..., -9.3185e-01,\n",
            "           -1.2804e+00,  4.6538e-01],\n",
            "          [-1.8985e-01,  4.3975e-01,  1.1856e+00,  ...,  5.3102e-01,\n",
            "           -4.9713e-01, -1.0671e+00],\n",
            "          ...,\n",
            "          [-2.6353e-01, -1.8032e-01,  8.7653e-01,  ...,  8.8473e-01,\n",
            "           -5.1469e-01, -6.8928e-01],\n",
            "          [-5.3650e-01, -3.3255e-02,  7.8179e-01,  ...,  1.0523e+00,\n",
            "           -3.3024e-01, -4.3505e-01],\n",
            "          [-2.4584e-01,  1.4495e-01,  7.4284e-01,  ...,  1.4939e+00,\n",
            "           -3.3469e-01, -8.6019e-01]],\n",
            "\n",
            "         [[-2.1834e-03,  1.5276e-02,  6.2051e-05,  ...,  1.3396e-03,\n",
            "            4.0209e-02, -3.8777e-02],\n",
            "          [-1.5678e+00,  9.3892e-01, -1.2334e+00,  ..., -3.0835e-03,\n",
            "            4.4588e-01, -6.9855e-01],\n",
            "          [-1.1558e+00,  2.5991e-01,  9.1110e-01,  ...,  2.8734e-02,\n",
            "            1.8114e-01,  4.6907e-01],\n",
            "          ...,\n",
            "          [-1.2470e+00, -1.6001e-01,  2.6714e-01,  ...,  3.5231e-01,\n",
            "            1.5746e-01,  6.8327e-01],\n",
            "          [-2.8451e-01, -6.7536e-01,  5.7775e-01,  ...,  1.8178e-01,\n",
            "           -1.0724e-01,  6.9616e-01],\n",
            "          [-1.6165e+00,  4.1481e-01, -2.2559e-01,  ..., -5.4051e-02,\n",
            "           -1.5108e-01,  4.7443e-01]],\n",
            "\n",
            "         [[ 2.9157e-02,  2.6629e-03, -3.8244e-03,  ..., -1.2861e-03,\n",
            "            2.8004e-04,  1.6861e-02],\n",
            "          [ 2.7897e-01, -1.5933e-01,  7.4641e-01,  ...,  9.6633e-01,\n",
            "            8.8251e-01,  8.7806e-01],\n",
            "          [ 6.2127e-02, -2.7881e-02, -5.1934e-01,  ...,  3.5283e-01,\n",
            "            1.0109e+00,  9.5020e-01],\n",
            "          ...,\n",
            "          [ 1.1482e+00,  1.9910e-01,  4.1409e-01,  ...,  3.9972e-01,\n",
            "            7.4796e-01,  7.4321e-01],\n",
            "          [ 5.7472e-01, -1.6009e-01,  4.3639e-01,  ...,  3.7187e-01,\n",
            "            5.1742e-02,  9.2247e-01],\n",
            "          [ 9.8581e-01, -5.9189e-02,  4.1110e-01,  ..., -3.0292e-01,\n",
            "            9.6637e-01,  5.2274e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-8.9613e-03,  5.3083e-02, -9.8900e-03,  ...,  3.1374e-01,\n",
            "           -2.7627e-02, -1.4555e-01],\n",
            "          [-1.7437e+00, -5.6832e-01, -5.7016e-02,  ...,  6.3829e-01,\n",
            "            6.7121e-01, -1.1376e+00],\n",
            "          [-8.8917e-01, -6.4543e-01,  1.4401e-02,  ..., -1.3165e+00,\n",
            "            5.2458e-02, -5.4144e-01],\n",
            "          ...,\n",
            "          [-1.6356e+00,  5.7417e-01,  3.9166e-01,  ..., -1.1000e+00,\n",
            "            4.0026e-01,  3.5663e-01],\n",
            "          [-2.1556e+00, -1.3127e-01,  3.2404e-01,  ..., -1.2337e+00,\n",
            "            6.6286e-02,  6.2944e-01],\n",
            "          [-4.6944e-01, -3.8259e-01, -3.7738e-01,  ..., -8.0254e-01,\n",
            "            7.4383e-02,  3.7421e-01]],\n",
            "\n",
            "         [[-2.4949e-02, -2.2400e-02,  1.8123e-02,  ...,  5.2626e-01,\n",
            "            3.9721e-01,  4.8219e-01],\n",
            "          [ 5.2882e-01,  3.7381e-01, -1.6627e-02,  ..., -1.9206e+00,\n",
            "           -3.7060e-01, -1.8405e+00],\n",
            "          [ 2.7361e-01, -1.8270e-01,  9.6322e-01,  ..., -1.1772e+00,\n",
            "           -8.1574e-01, -2.8157e+00],\n",
            "          ...,\n",
            "          [-6.6198e-01,  2.7924e-01, -1.0903e+00,  ..., -2.0633e+00,\n",
            "           -4.9447e-01, -2.6948e+00],\n",
            "          [ 4.6959e-01, -8.0673e-01, -3.6267e-01,  ..., -1.9232e+00,\n",
            "           -1.0318e+00, -2.6765e+00],\n",
            "          [ 4.9618e-01, -9.7333e-01, -4.9119e-01,  ..., -2.6456e+00,\n",
            "           -1.0270e-01, -2.3459e+00]],\n",
            "\n",
            "         [[ 3.7775e-02,  1.4798e-02, -1.1130e-02,  ...,  6.0320e-02,\n",
            "            1.0977e+00, -1.2103e+00],\n",
            "          [ 6.8001e-01, -7.7715e-02,  1.4321e-01,  ..., -1.9436e+00,\n",
            "           -3.8927e+00,  3.7951e+00],\n",
            "          [-1.7590e-01, -1.6103e-01,  3.3300e-01,  ..., -4.7803e-01,\n",
            "           -2.3056e+00,  4.3672e+00],\n",
            "          ...,\n",
            "          [ 4.9788e-01,  3.0352e-01,  3.0537e-01,  ..., -4.0714e-01,\n",
            "           -2.6779e+00,  4.8111e+00],\n",
            "          [ 4.4167e-01,  1.1640e-02,  1.9516e-01,  ..., -1.1475e+00,\n",
            "           -3.1845e+00,  3.3337e+00],\n",
            "          [ 1.6403e-01, -2.3934e-02,  5.5759e-03,  ..., -5.1844e-01,\n",
            "           -1.9547e+00,  3.8145e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7315e-03, -6.7003e-03, -1.7007e-02,  ..., -2.0216e-01,\n",
            "           -3.5468e-01, -1.7029e-01],\n",
            "          [ 1.0462e+00, -7.5073e-01,  9.7594e-01,  ..., -3.4217e-01,\n",
            "            9.5957e-01,  7.0346e-01],\n",
            "          [-1.3424e+00, -6.1825e-01,  4.5292e-01,  ..., -1.0563e+00,\n",
            "           -5.7212e-01,  6.8143e-01],\n",
            "          ...,\n",
            "          [ 1.5953e+00,  5.8862e-01, -5.0005e-01,  ..., -7.8933e-01,\n",
            "           -2.6295e-01, -2.2223e-01],\n",
            "          [ 1.7396e+00, -3.1533e-01, -6.2739e-01,  ..., -1.8163e+00,\n",
            "            1.1569e-01,  1.0161e+00],\n",
            "          [ 2.4233e-02, -7.2395e-01, -2.8955e-01,  ..., -9.7345e-01,\n",
            "           -7.4102e-01, -7.1426e-01]],\n",
            "\n",
            "         [[ 3.1715e-02, -1.5339e-03, -6.2354e-03,  ...,  9.0706e-01,\n",
            "           -1.6331e-01,  2.5527e-01],\n",
            "          [ 1.0676e+00,  3.7749e-01,  3.4161e-01,  ..., -2.0835e+00,\n",
            "            8.3732e-01,  1.0167e+00],\n",
            "          [ 3.9873e-01,  1.1233e+00,  8.0836e-01,  ..., -3.1509e+00,\n",
            "            8.3221e-01,  6.4503e-01],\n",
            "          ...,\n",
            "          [ 4.5138e-02, -1.5092e+00, -9.6302e-01,  ..., -4.2240e+00,\n",
            "            1.5528e+00, -1.1306e-01],\n",
            "          [ 6.5966e-01, -7.1624e-01, -8.3725e-01,  ..., -3.2719e+00,\n",
            "            1.1902e+00,  2.5400e-01],\n",
            "          [ 3.0967e-01,  1.0977e-01, -5.0307e-01,  ..., -3.8397e+00,\n",
            "            2.3279e+00, -2.0008e-01]],\n",
            "\n",
            "         [[-5.0665e-04,  1.6451e-02,  3.1476e-02,  ...,  1.1650e-01,\n",
            "           -3.1705e-01, -7.3294e-01],\n",
            "          [ 1.1680e+00,  8.8573e-01, -5.3171e-01,  ..., -1.9465e+00,\n",
            "            1.4651e+00,  4.4458e+00],\n",
            "          [ 6.5259e-01, -1.0157e-01, -3.0250e-03,  ..., -5.5996e-01,\n",
            "            1.8603e+00,  2.7735e+00],\n",
            "          ...,\n",
            "          [ 4.0818e-01,  3.6477e-01,  2.3700e-01,  ..., -3.2299e-01,\n",
            "            1.6493e+00,  3.2864e+00],\n",
            "          [ 1.9826e-01,  7.8576e-01, -3.6113e-01,  ..., -5.2619e-01,\n",
            "            2.5699e+00,  4.4909e+00],\n",
            "          [-2.0418e-01,  1.1653e+00, -6.2989e-01,  ..., -1.5930e+00,\n",
            "            2.8767e+00,  3.8813e+00]]]], grad_fn=<AddBackward0>), tensor([[[[ 1.3840e-02, -9.8600e-03, -2.2131e-02,  ...,  4.1950e-03,\n",
            "           -8.7772e-03, -1.8881e-02],\n",
            "          [ 1.4587e+00,  9.6099e-01, -2.8658e-01,  ...,  1.4090e+00,\n",
            "           -2.3019e-01,  1.9421e-01],\n",
            "          [ 6.4477e-01,  3.6990e-01,  3.4241e-01,  ...,  3.7677e-01,\n",
            "            8.6888e-01, -3.8267e-01],\n",
            "          ...,\n",
            "          [ 1.1079e+00,  5.5212e-02,  5.3646e-01,  ...,  3.4134e-01,\n",
            "            2.6315e-01, -5.0114e-01],\n",
            "          [ 1.0104e+00,  1.6963e-01,  1.1703e+00,  ...,  2.1528e-01,\n",
            "           -2.8796e-01, -2.2001e-01],\n",
            "          [ 9.8941e-01,  8.3046e-02,  7.0189e-01,  ...,  5.1484e-01,\n",
            "            2.9343e-01, -9.1290e-02]],\n",
            "\n",
            "         [[ 2.2750e-02,  4.1253e-03,  8.2720e-03,  ...,  1.1099e-03,\n",
            "           -3.2041e-02, -4.1977e-03],\n",
            "          [ 5.9101e-01, -1.0194e+00, -3.7263e-01,  ...,  2.3235e-01,\n",
            "           -1.1677e+00,  8.5398e-02],\n",
            "          [-1.4000e+00,  5.0355e-01, -1.7249e-01,  ...,  2.9717e-01,\n",
            "           -3.9761e-01,  2.2903e-01],\n",
            "          ...,\n",
            "          [-8.4725e-01, -3.2934e-01, -6.4752e-01,  ...,  4.0683e-01,\n",
            "           -7.7537e-02,  4.2997e-01],\n",
            "          [-8.8977e-01,  2.9144e-01, -4.9928e-01,  ...,  5.0818e-01,\n",
            "           -2.0981e-01,  7.9997e-01],\n",
            "          [-8.2165e-01, -2.5756e-01, -1.6695e-01,  ..., -7.3311e-02,\n",
            "            6.9420e-01,  2.9955e-02]],\n",
            "\n",
            "         [[-4.6475e-02,  1.3660e-02, -2.7129e-02,  ...,  5.4262e-02,\n",
            "            7.9335e-02, -4.7950e-02],\n",
            "          [-2.9118e-01,  1.5330e+00,  2.3932e-01,  ...,  1.3121e-02,\n",
            "           -4.8813e-01,  3.2903e-02],\n",
            "          [-6.1890e-01,  2.8484e-01,  7.6388e-02,  ..., -2.8312e-01,\n",
            "           -2.1445e-01, -2.5355e-01],\n",
            "          ...,\n",
            "          [-2.0488e-01,  8.4296e-01,  2.1736e-01,  ...,  2.4147e-02,\n",
            "           -1.8845e-01, -2.4849e-01],\n",
            "          [-4.4763e-01,  3.0618e-01,  1.9594e-02,  ..., -2.4815e-01,\n",
            "            2.5103e-01, -5.2348e-01],\n",
            "          [ 5.1365e-02,  7.3400e-01,  3.6471e-01,  ..., -1.2373e-01,\n",
            "           -5.4211e-01, -4.8478e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.6108e-03,  3.7930e-03,  8.3038e-03,  ..., -3.5058e-02,\n",
            "           -3.6018e-02, -3.5870e-02],\n",
            "          [-3.9716e-01,  5.9685e-01, -3.0067e-01,  ...,  4.5058e-01,\n",
            "           -2.9387e-04, -2.2330e-01],\n",
            "          [ 5.8486e-01, -7.8095e-01,  9.1977e-01,  ..., -4.1602e-02,\n",
            "            3.1767e-01,  8.3896e-01],\n",
            "          ...,\n",
            "          [ 6.5622e-01,  2.0506e-01,  1.2731e-01,  ...,  6.5601e-01,\n",
            "            3.1415e-01,  5.2581e-01],\n",
            "          [ 4.6910e-01,  8.9488e-02,  2.2631e-01,  ...,  8.7142e-01,\n",
            "           -1.8991e-01, -4.2105e-02],\n",
            "          [ 6.3500e-01, -2.0845e-01,  1.1493e-01,  ...,  3.7492e-01,\n",
            "            4.5336e-01,  5.2410e-01]],\n",
            "\n",
            "         [[-6.5298e-03,  8.3198e-03, -6.5554e-02,  ...,  3.6483e-02,\n",
            "           -4.3870e-02,  2.8665e-02],\n",
            "          [ 5.2975e-01,  6.0881e-01, -1.6491e+00,  ..., -1.3500e+00,\n",
            "           -1.3757e+00,  3.1702e-01],\n",
            "          [-3.6824e-01,  5.0000e-01, -1.0343e+00,  ..., -1.0386e-01,\n",
            "            6.4097e-02, -5.0002e-01],\n",
            "          ...,\n",
            "          [-2.7645e-01,  1.8079e-01, -1.1845e+00,  ..., -3.3010e-01,\n",
            "           -1.5530e-01,  2.6216e-02],\n",
            "          [-4.0181e-01,  2.9791e-01, -6.7871e-01,  ...,  1.6401e-01,\n",
            "           -6.9408e-01, -2.2977e-01],\n",
            "          [-3.9346e-01,  5.0830e-01, -4.5271e-01,  ..., -4.8467e-01,\n",
            "           -6.8072e-03,  4.7271e-01]],\n",
            "\n",
            "         [[ 2.4913e-02, -4.1538e-02, -2.2587e-02,  ...,  6.2131e-02,\n",
            "           -6.3370e-03, -2.0693e-03],\n",
            "          [ 4.0533e-02,  3.7655e-02, -3.8608e-02,  ..., -5.2871e-02,\n",
            "           -9.6060e-01,  1.1001e-01],\n",
            "          [ 2.4702e-01,  6.1391e-01,  3.5092e-01,  ...,  1.4582e+00,\n",
            "            1.5883e-01,  2.1903e-01],\n",
            "          ...,\n",
            "          [ 3.7682e-01, -4.5976e-02,  4.4756e-01,  ..., -2.4567e-01,\n",
            "            2.0016e-01, -4.0296e-01],\n",
            "          [ 6.0836e-01, -9.0579e-02,  1.2845e+00,  ..., -3.6173e-01,\n",
            "           -8.5819e-02,  5.2710e-01],\n",
            "          [ 1.8461e-01, -9.6729e-03,  4.7059e-01,  ...,  5.8469e-01,\n",
            "            5.7682e-01,  1.2497e-01]]]], grad_fn=<TransposeBackward0>)), (tensor([[[[-3.6900e-03,  2.0810e-02, -1.1837e-03,  ..., -6.9399e-02,\n",
            "           -1.1720e-01, -5.4865e-04],\n",
            "          [-6.4621e-01,  3.1368e-01,  7.0813e-01,  ..., -2.1656e+00,\n",
            "            1.6916e-01,  6.8345e-01],\n",
            "          [-1.4467e-01, -5.2099e-02,  2.7399e-01,  ..., -1.6253e+00,\n",
            "           -1.1524e+00,  1.2970e+00],\n",
            "          ...,\n",
            "          [-3.4752e-01, -1.5859e-01, -5.1806e-01,  ..., -1.1690e+00,\n",
            "           -8.6866e-01,  8.7556e-01],\n",
            "          [-1.0180e+00,  6.4838e-02, -4.6344e-01,  ..., -7.2088e-01,\n",
            "           -1.3274e+00,  1.1729e+00],\n",
            "          [ 2.3299e-01,  7.5741e-01,  3.3689e-01,  ..., -1.5400e+00,\n",
            "           -1.5977e-01, -5.8144e-01]],\n",
            "\n",
            "         [[ 1.2202e-02,  6.8537e-03,  8.1735e-03,  ..., -9.5835e-02,\n",
            "            1.2529e-01, -1.9365e-01],\n",
            "          [ 2.6277e-01, -6.8567e-02,  6.6989e-01,  ..., -1.0565e+00,\n",
            "           -3.9421e-01,  6.4704e-01],\n",
            "          [ 3.7922e-01, -6.9611e-01,  5.5456e-02,  ..., -3.0025e-02,\n",
            "            6.6446e-01, -4.8205e-01],\n",
            "          ...,\n",
            "          [ 8.1330e-01,  3.9077e-01, -2.9990e-01,  ..., -7.7685e-01,\n",
            "           -4.9868e-01, -5.2400e-01],\n",
            "          [ 1.1756e+00,  4.5048e-01, -3.0066e-01,  ..., -1.8444e-01,\n",
            "           -6.1963e-01, -1.6660e+00],\n",
            "          [ 2.5036e-01,  1.8735e-01, -3.0608e-01,  ..., -6.7186e-01,\n",
            "           -3.9517e-02,  3.6321e-01]],\n",
            "\n",
            "         [[ 2.7733e-02,  1.5482e-02, -9.6590e-03,  ..., -1.6383e-01,\n",
            "            2.2446e-02,  1.5803e-01],\n",
            "          [ 2.4288e+00, -1.9528e-01,  6.5325e-01,  ..., -1.3932e+00,\n",
            "           -1.1047e+00, -6.4112e-01],\n",
            "          [-4.4107e-01,  9.1116e-01, -7.8825e-01,  ..., -4.4169e-01,\n",
            "           -2.4408e+00, -2.1606e+00],\n",
            "          ...,\n",
            "          [ 1.5756e+00, -8.3880e-01,  7.6699e-01,  ..., -2.5503e-01,\n",
            "           -2.1401e+00, -1.6790e+00],\n",
            "          [ 2.5874e+00, -1.3915e+00,  7.6329e-01,  ..., -4.7752e-01,\n",
            "           -2.3508e+00, -1.3404e+00],\n",
            "          [ 1.6688e-01, -3.0945e-01,  5.3846e-01,  ..., -4.8273e-01,\n",
            "           -2.7883e+00, -1.0567e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.2380e-02, -5.6356e-02, -3.1937e-02,  ...,  3.7699e-01,\n",
            "            4.5607e-01,  3.8389e-01],\n",
            "          [-2.0780e-01, -8.8257e-01, -7.6761e-01,  ...,  5.1736e-01,\n",
            "           -2.1469e+00, -7.5738e-01],\n",
            "          [-1.0554e-01,  1.0166e+00, -2.2438e-01,  ...,  5.3846e-01,\n",
            "           -5.5193e-01, -2.5063e+00],\n",
            "          ...,\n",
            "          [ 5.1635e-01, -7.4178e-01,  1.6779e-01,  ...,  9.0346e-01,\n",
            "            9.9763e-01, -2.7260e+00],\n",
            "          [ 4.7777e-01, -5.3139e-01, -3.7859e-01,  ...,  1.6958e+00,\n",
            "            1.6196e-01, -2.4721e+00],\n",
            "          [-1.5475e-01, -5.4346e-01,  9.2591e-01,  ...,  9.9755e-01,\n",
            "            1.8361e-01, -1.6430e+00]],\n",
            "\n",
            "         [[ 3.1217e-02, -1.6457e-02, -2.8517e-03,  ..., -1.2550e-01,\n",
            "           -5.8877e-02,  2.1845e-01],\n",
            "          [ 4.0288e-01,  1.3064e-01, -1.1349e+00,  ..., -3.3539e-01,\n",
            "            5.5810e-01,  1.0817e+00],\n",
            "          [-8.9345e-01, -8.9806e-01,  2.6519e-01,  ..., -7.1465e-01,\n",
            "            5.2701e-02, -5.1502e-01],\n",
            "          ...,\n",
            "          [ 7.9721e-01,  7.4100e-01, -1.0555e+00,  ..., -5.5231e-01,\n",
            "            9.5664e-02, -4.3027e-01],\n",
            "          [ 6.1290e-01,  5.1716e-01, -6.2430e-01,  ..., -7.8619e-01,\n",
            "           -7.4323e-01, -3.4970e-01],\n",
            "          [-6.5128e-02, -4.4823e-01,  3.0246e-01,  ..., -7.6943e-02,\n",
            "           -4.4730e-01,  7.5735e-02]],\n",
            "\n",
            "         [[ 2.0469e-02,  4.7702e-03, -1.9593e-03,  ...,  1.1408e-01,\n",
            "           -1.8971e-02,  3.7480e-02],\n",
            "          [ 3.0895e+00, -7.3441e-01, -1.9319e+00,  ..., -9.3845e-01,\n",
            "           -5.8994e-01,  9.3230e-01],\n",
            "          [ 8.9417e-01, -1.1076e+00, -1.3932e+00,  ...,  2.4059e-01,\n",
            "            5.6355e-01, -4.9643e-02],\n",
            "          ...,\n",
            "          [ 8.1779e-01,  2.9610e-01,  1.3598e+00,  ...,  2.5611e-01,\n",
            "            4.3463e-02, -1.1985e+00],\n",
            "          [ 1.6728e+00, -1.2767e+00,  1.9664e+00,  ...,  5.2517e-01,\n",
            "           -9.8851e-01, -8.7032e-01],\n",
            "          [ 8.6832e-01, -1.8008e+00,  1.4929e+00,  ...,  5.3269e-01,\n",
            "            2.1770e-01, -1.1632e+00]]]], grad_fn=<AddBackward0>), tensor([[[[-0.0510,  0.0405, -0.0521,  ...,  0.0475,  0.0553,  0.0234],\n",
            "          [ 0.7901,  0.6234, -0.9584,  ..., -0.0927,  0.2431,  1.0876],\n",
            "          [ 0.2683,  1.3931,  0.4818,  ..., -0.7829,  0.5313, -0.2706],\n",
            "          ...,\n",
            "          [ 0.4377,  1.4585,  0.2594,  ..., -1.2230, -0.0082, -0.2222],\n",
            "          [ 0.3253,  1.4605,  0.7208,  ..., -0.6536,  0.5408,  0.0496],\n",
            "          [ 0.3543,  1.2567,  0.9822,  ..., -0.9630,  0.3102, -0.5525]],\n",
            "\n",
            "         [[-0.0119, -0.0145, -0.0262,  ...,  0.0334,  0.0278,  0.0316],\n",
            "          [-1.1343, -1.3219,  0.0790,  ...,  0.9202, -0.0713,  0.4866],\n",
            "          [ 0.2554, -0.1220,  0.4208,  ...,  0.6771,  0.3200,  1.2275],\n",
            "          ...,\n",
            "          [ 0.3689,  0.0203,  0.6065,  ...,  1.1486,  0.7489,  0.9252],\n",
            "          [ 0.8473,  0.1778,  0.5370,  ...,  0.6640,  0.6172,  1.0233],\n",
            "          [ 0.4442, -0.3722,  0.3297,  ...,  0.8766,  0.6647,  0.7283]],\n",
            "\n",
            "         [[ 0.0192,  0.0243, -0.1025,  ..., -0.0056, -0.0199,  0.0088],\n",
            "          [-1.7457, -0.8394, -0.9111,  ..., -0.5946, -0.4456,  0.0281],\n",
            "          [-1.0547, -0.7753, -2.0230,  ...,  0.3523, -0.0900, -0.2839],\n",
            "          ...,\n",
            "          [-1.3040, -0.8231, -1.9686,  ...,  0.2780,  0.1201, -0.7797],\n",
            "          [-1.2034, -1.2155, -1.3123,  ...,  0.2517, -0.1573, -0.5748],\n",
            "          [-1.0738, -0.4202, -0.7619,  ...,  0.3970, -0.0208, -1.0059]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.1772, -0.0076,  0.1069,  ..., -0.0273, -0.1179,  0.0920],\n",
            "          [ 0.5931, -0.0730, -0.1069,  ...,  0.3631,  1.0969, -0.6311],\n",
            "          [ 0.4257,  0.6685,  0.6099,  ...,  0.7559, -0.1405, -0.6709],\n",
            "          ...,\n",
            "          [ 0.5096, -0.4132,  0.6952,  ...,  0.6698, -0.2937, -0.6158],\n",
            "          [ 0.1612,  0.1919, -0.4097,  ...,  1.0344, -0.9714, -0.2036],\n",
            "          [ 0.1603, -0.4854,  0.6443,  ..., -0.0203, -0.6629, -0.2315]],\n",
            "\n",
            "         [[ 0.0432, -0.0063, -0.0131,  ..., -0.0167, -0.0598, -0.0633],\n",
            "          [-0.7421, -0.6401, -0.7702,  ...,  0.7455,  0.0133,  1.4807],\n",
            "          [ 0.5523, -0.1988,  0.3237,  ..., -0.1625,  0.2023,  0.1272],\n",
            "          ...,\n",
            "          [ 0.0822, -0.5418,  0.5373,  ...,  0.7870, -0.0337, -0.2358],\n",
            "          [ 0.0440,  0.0433,  0.3115,  ...,  0.1928, -0.7847, -0.4767],\n",
            "          [ 0.3532,  0.1258,  0.5605,  ...,  0.6029, -0.2041, -0.0259]],\n",
            "\n",
            "         [[-0.0304,  0.0169, -0.0052,  ...,  0.0220,  0.0786,  0.0488],\n",
            "          [-0.1521, -0.8938, -2.1404,  ..., -0.0270,  0.4495, -0.7294],\n",
            "          [-0.9649,  1.3457, -1.4238,  ..., -0.5065,  0.3284, -0.5370],\n",
            "          ...,\n",
            "          [-1.1545,  1.4242, -1.5193,  ...,  0.1066,  1.1200, -0.8930],\n",
            "          [-1.3800,  1.0189, -0.4406,  ..., -0.3186,  1.4295, -1.0510],\n",
            "          [-0.5530,  0.3638, -1.4726,  ...,  0.2447,  1.5917, -0.4604]]]],\n",
            "       grad_fn=<TransposeBackward0>)), (tensor([[[[-0.1613,  0.0647, -0.1121,  ...,  0.7143, -0.1292,  0.8139],\n",
            "          [ 1.9347, -0.1784, -0.3642,  ...,  1.4868,  0.3459, -0.3716],\n",
            "          [ 0.9051, -0.4548,  0.8419,  ...,  2.6065,  0.5021, -1.8898],\n",
            "          ...,\n",
            "          [ 1.3768,  0.2982, -0.5784,  ...,  3.0011,  0.9696, -1.2590],\n",
            "          [ 2.6856,  0.0628, -1.1966,  ...,  2.0259,  0.5109, -1.1519],\n",
            "          [ 1.1298, -0.7332, -0.8171,  ...,  2.4258,  0.4976, -0.9214]],\n",
            "\n",
            "         [[ 0.0672,  0.0823,  0.2205,  ..., -0.2163,  0.4466, -0.1266],\n",
            "          [-0.8659, -0.4974, -0.6697,  ..., -1.6603, -0.4533, -0.5679],\n",
            "          [-0.5665, -0.4048, -0.0535,  ..., -0.3572, -1.2925,  0.4716],\n",
            "          ...,\n",
            "          [ 0.9864,  0.5416,  0.1878,  ..., -0.9774, -1.6864, -0.0936],\n",
            "          [ 0.3648, -0.2964,  0.9108,  ..., -0.2703, -1.1926, -0.2852],\n",
            "          [ 0.0461, -0.5332,  0.4120,  ..., -0.4124, -1.4818, -0.8640]],\n",
            "\n",
            "         [[-0.0921, -0.0099,  0.0031,  ...,  0.1773, -0.5120,  2.0292],\n",
            "          [-1.2281,  0.2767, -0.3440,  ..., -0.1348, -1.3918,  2.0019],\n",
            "          [-0.8175,  0.4870, -0.1795,  ..., -1.3962, -0.0305,  1.2841],\n",
            "          ...,\n",
            "          [ 0.1608, -0.4302,  0.3188,  ..., -1.8533,  0.0809,  1.7392],\n",
            "          [-0.5979,  0.3708,  0.0915,  ..., -1.2897,  0.0338,  2.5326],\n",
            "          [-0.5711,  0.2743,  0.1982,  ..., -2.5546,  0.0543,  0.6285]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0807,  0.0200,  0.0673,  ..., -1.4222,  0.4444, -2.1323],\n",
            "          [ 0.8511,  2.9206,  0.3217,  ..., -0.1653,  0.6482, -1.7220],\n",
            "          [ 0.6643,  0.7214, -0.2720,  ..., -0.5875,  0.8707, -0.7870],\n",
            "          ...,\n",
            "          [-1.1540,  0.1957,  0.1542,  ..., -0.4206,  0.9235, -0.2065],\n",
            "          [-0.8741,  0.0395,  0.8759,  ..., -0.5262,  1.3170, -1.6901],\n",
            "          [ 0.3550,  0.4182,  1.0909,  ..., -0.3195,  0.5085, -0.2176]],\n",
            "\n",
            "         [[-0.1110,  0.0605,  0.3253,  ...,  1.0065,  0.5941, -0.5059],\n",
            "          [ 0.2568, -0.8498,  0.4333,  ...,  1.0606, -0.4098,  0.5772],\n",
            "          [-0.8861,  0.3503, -0.0136,  ...,  2.2158, -0.6455,  1.8104],\n",
            "          ...,\n",
            "          [ 0.9333, -0.2115,  0.2641,  ...,  2.0840,  0.1693,  2.0061],\n",
            "          [ 0.3122, -0.0742,  0.3986,  ...,  2.1740, -0.5288,  0.7390],\n",
            "          [-0.0847, -0.0147,  0.2424,  ...,  1.7497,  0.2255,  1.7194]],\n",
            "\n",
            "         [[ 0.3387,  0.1529,  0.1291,  ...,  0.7484, -0.6934, -0.0128],\n",
            "          [-2.2295,  0.2406, -0.4650,  ...,  1.4098, -0.3747,  0.2224],\n",
            "          [-2.7241, -0.7276, -0.3087,  ...,  0.9854, -0.1952,  0.0630],\n",
            "          ...,\n",
            "          [ 0.4735,  0.6390,  0.1847,  ...,  0.2596,  0.2507, -0.0688],\n",
            "          [-2.1325,  1.3160, -0.5704,  ...,  0.1624, -0.2297, -0.6351],\n",
            "          [-2.2231,  1.1195, -0.7695,  ...,  0.1869,  0.5019, -0.1587]]]],\n",
            "       grad_fn=<AddBackward0>), tensor([[[[-0.1732,  0.1247,  0.1316,  ...,  0.1988, -0.0497,  0.0850],\n",
            "          [ 0.1007, -0.3797, -0.1900,  ..., -0.0972, -0.1577, -0.4767],\n",
            "          [ 0.3155, -0.2932, -0.1665,  ..., -0.3727,  0.4397,  0.0964],\n",
            "          ...,\n",
            "          [ 0.0961, -0.0507,  0.0814,  ..., -0.3967,  0.5759, -0.4203],\n",
            "          [ 0.1230, -0.5837, -0.1599,  ..., -0.3605,  0.2438, -0.0893],\n",
            "          [ 0.0087,  0.0208,  0.2439,  ..., -0.1332,  0.2044, -0.2444]],\n",
            "\n",
            "         [[ 0.0048,  0.1820,  0.1316,  ...,  0.0157,  0.0853,  0.1790],\n",
            "          [ 0.2011,  0.4897, -0.4114,  ..., -0.6335, -1.1694,  0.1717],\n",
            "          [ 0.1789, -0.1321, -0.4593,  ...,  0.1815,  0.0948,  0.7222],\n",
            "          ...,\n",
            "          [ 0.2443, -0.4239, -0.0317,  ..., -0.2949,  0.1496,  0.9884],\n",
            "          [ 0.0282, -0.3467, -0.0466,  ..., -0.2673, -0.1375,  0.7754],\n",
            "          [-0.0283,  0.0406,  0.1836,  ...,  0.2879,  0.1703,  0.7136]],\n",
            "\n",
            "         [[-0.1085,  0.1451, -0.0907,  ...,  0.0771, -0.1193, -0.0662],\n",
            "          [ 0.0198, -0.6179, -0.4294,  ...,  0.6455, -0.5909,  0.3474],\n",
            "          [-0.6418,  0.2833, -0.7157,  ..., -0.2052, -0.3518, -0.7040],\n",
            "          ...,\n",
            "          [-0.6735,  0.2104, -0.1617,  ..., -0.1385, -0.3408, -0.5055],\n",
            "          [-0.1628,  0.2360, -0.0265,  ...,  0.2617,  0.0183, -0.3832],\n",
            "          [-0.4695, -0.0034, -0.2130,  ..., -0.3525, -1.1885, -0.4518]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0944,  0.1158,  0.4028,  ...,  0.2185, -0.0735, -0.0491],\n",
            "          [-0.1966,  0.0221, -1.1906,  ...,  0.3739,  0.8815, -0.6465],\n",
            "          [-0.5938, -0.5984, -0.3830,  ..., -0.7776,  0.1718, -0.7378],\n",
            "          ...,\n",
            "          [-0.3140, -0.4610, -0.4152,  ..., -0.4530,  0.4765, -0.7273],\n",
            "          [-0.4293, -1.0757, -0.6368,  ..., -0.4620,  0.2310, -0.3410],\n",
            "          [-0.2505, -0.1517, -0.6207,  ..., -0.3987,  0.0625, -0.8231]],\n",
            "\n",
            "         [[ 0.1321,  0.0579,  0.1692,  ..., -0.1820,  0.2276, -0.0450],\n",
            "          [ 0.5519,  0.2643,  0.5038,  ...,  1.3772, -0.0498,  0.9884],\n",
            "          [-0.0998, -0.7261,  0.1319,  ...,  0.3273,  0.6338,  0.3419],\n",
            "          ...,\n",
            "          [ 0.0045, -0.5115, -0.0017,  ...,  0.2464,  0.4111,  0.1232],\n",
            "          [-0.0048, -0.2946, -0.2921,  ...,  0.3407,  0.7903,  0.4557],\n",
            "          [ 0.2525, -0.3960, -0.4112,  ...,  0.3179,  0.5505, -0.2782]],\n",
            "\n",
            "         [[-0.0338,  0.0639, -0.0369,  ..., -0.0247,  0.1603,  0.0389],\n",
            "          [-0.0537,  0.4713,  0.2227,  ...,  0.5212,  0.6627,  1.0537],\n",
            "          [-0.2536,  0.1978,  0.3157,  ..., -0.1111,  0.3811,  0.0585],\n",
            "          ...,\n",
            "          [ 0.1771,  0.1641,  0.2549,  ..., -0.1834,  0.3768,  0.1822],\n",
            "          [ 0.6401,  0.0655,  0.0393,  ..., -0.0457,  0.0666,  0.4167],\n",
            "          [ 0.2650,  0.4702,  0.4138,  ..., -0.1293,  0.3393,  0.2788]]]],\n",
            "       grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import LlamaForCausalLM as HuggingFaceLlamaForCausalLM\n",
        "#hf_model = HuggingFaceLlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
      ],
      "metadata": {
        "id": "aQjSsbmhYB1k"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0].shape"
      ],
      "metadata": {
        "id": "GUmrlUXJGu9B",
        "outputId": "e94d4fb0-0423-4d95-8b54-d612f08772c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 9, 32000])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "6XgUW84A7064",
        "outputId": "98819b56-4d05-47d5-86d1-c8b9ac30a4cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'logits'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3556c0a298f3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   4014\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4016\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   4017\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4018\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_use_source_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_source_tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0mfiltered_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m         legacy_added_tokens = set(self._added_tokens_encoder.keys()) - set(self.all_special_tokens) | {\n\u001b[1;32m   1083\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditional_special_tokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'logits'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xyp_JmF-Ghss"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Como aproveitar ao máximo sua assinatura do Colab",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "79c2a093329b467482392a91f39e7b96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f4dd9535a5b43b5bdda12094b17174e",
              "IPY_MODEL_9b30d495f7b9487eaa117b4ee0503810",
              "IPY_MODEL_c0394f80943b416389a94f02b81bf3ae"
            ],
            "layout": "IPY_MODEL_63cce58a89df465a9150aff966b8264b"
          }
        },
        "3f4dd9535a5b43b5bdda12094b17174e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f201ae4521948d8a36069c7cc274ba4",
            "placeholder": "​",
            "style": "IPY_MODEL_c7e82569b4ad421eb60da92949823174",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9b30d495f7b9487eaa117b4ee0503810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9118771e7f594188b7c119d4084e0aec",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d039988296c427ebebe0d9c6bca0400",
            "value": 2
          }
        },
        "c0394f80943b416389a94f02b81bf3ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3be918aa0cda4c72abb840f2f15f9838",
            "placeholder": "​",
            "style": "IPY_MODEL_f007e9640ebf4eab9e506dbd84b705b9",
            "value": " 2/2 [00:20&lt;00:00,  8.98s/it]"
          }
        },
        "63cce58a89df465a9150aff966b8264b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f201ae4521948d8a36069c7cc274ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e82569b4ad421eb60da92949823174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9118771e7f594188b7c119d4084e0aec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d039988296c427ebebe0d9c6bca0400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3be918aa0cda4c72abb840f2f15f9838": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f007e9640ebf4eab9e506dbd84b705b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}