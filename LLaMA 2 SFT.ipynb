{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegomrodrigues/my_llama_2/blob/main/LLaMA%202%20SFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Custom LLaMA Implementation"
      ],
      "metadata": {
        "id": "H6nCbDvlarog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Config"
      ],
      "metadata": {
        "id": "qK6iKRA7Zeht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PretrainedConfig\n",
        "from typing import Dict, Any, Optional, Union\n",
        "\n",
        "class LlamaConfig(PretrainedConfig):\n",
        "    \"\"\"\n",
        "    Configuração para o modelo LLaMA.\n",
        "\n",
        "    Esta classe define todos os parâmetros necessários para construir e configurar\n",
        "    um modelo LLaMA. Herda de PretrainedConfig da biblioteca Transformers.\n",
        "\n",
        "    Atributos:\n",
        "        vocab_size (int): Tamanho do vocabulário do modelo.\n",
        "        hidden_size (int): Dimensão dos vetores de estado oculto e embeddings.\n",
        "        intermediate_size (int): Dimensão da camada intermediária no MLP.\n",
        "        num_hidden_layers (int): Número de camadas de transformer no modelo.\n",
        "        num_attention_heads (int): Número de cabeças de atenção em cada camada.\n",
        "        num_key_value_heads (int): Número de cabeças para key e value (para atenção agrupada).\n",
        "        hidden_act (str): Função de ativação usada no MLP.\n",
        "        max_position_embeddings (int): Número máximo de posições para embeddings.\n",
        "        initializer_range (float): Desvio padrão da distribuição normal para inicialização de pesos.\n",
        "        rms_norm_eps (float): Epsilon usado na normalização RMS.\n",
        "        use_cache (bool): Se deve usar cache para geração incremental.\n",
        "        pad_token_id (int): ID do token de padding.\n",
        "        bos_token_id (int): ID do token de início de sequência.\n",
        "        eos_token_id (int): ID do token de fim de sequência.\n",
        "        pretraining_tp (int): Grau de paralelismo de tensor usado no pré-treinamento.\n",
        "        tie_word_embeddings (bool): Se deve compartilhar pesos entre embeddings de entrada e saída.\n",
        "        rope_theta (float): Valor theta para RoPE (Rotary Position Embedding).\n",
        "        rope_scaling (Dict[str, Any]): Configuração de escala para RoPE.\n",
        "        attention_bias (bool): Se deve usar bias nos cálculos de atenção.\n",
        "        attention_dropout (float): Taxa de dropout aplicada na camada de atenção.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(\n",
        "        ...     vocab_size=32000,\n",
        "        ...     hidden_size=4096,\n",
        "        ...     intermediate_size=11008,\n",
        "        ...     num_hidden_layers=32,\n",
        "        ...     num_attention_heads=32,\n",
        "        ... )\n",
        "        >>> print(config)\n",
        "    \"\"\"\n",
        "\n",
        "    model_type = \"llama\"\n",
        "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int = 32000,\n",
        "        hidden_size: int = 4096,\n",
        "        intermediate_size: int = 11008,\n",
        "        num_hidden_layers: int = 32,\n",
        "        num_attention_heads: int = 32,\n",
        "        num_key_value_heads: Optional[int] = None,\n",
        "        hidden_act: str = \"silu\",\n",
        "        max_position_embeddings: int = 2048,\n",
        "        initializer_range: float = 0.02,\n",
        "        rms_norm_eps: float = 1e-6,\n",
        "        use_cache: bool = True,\n",
        "        pad_token_id: int = -1,\n",
        "        bos_token_id: int = 1,\n",
        "        eos_token_id: int = 2,\n",
        "        pretraining_tp: int = 1,\n",
        "        tie_word_embeddings: bool = False,\n",
        "        rope_theta: float = 10000.0,\n",
        "        rope_scaling: Optional[Dict[str, Union[float, str]]] = None,\n",
        "        attention_bias: bool = False,\n",
        "        attention_dropout: float = 0.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.hidden_size = hidden_size\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.num_key_value_heads = num_key_value_heads if num_key_value_heads is not None else num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.initializer_range = initializer_range\n",
        "        self.rms_norm_eps = rms_norm_eps\n",
        "        self.pretraining_tp = pretraining_tp\n",
        "        self.use_cache = use_cache\n",
        "        self.rope_theta = rope_theta\n",
        "        self.rope_scaling = rope_scaling\n",
        "        self.attention_bias = attention_bias\n",
        "        self.attention_dropout = attention_dropout\n",
        "\n",
        "        super().__init__(\n",
        "            pad_token_id=pad_token_id,\n",
        "            bos_token_id=bos_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            tie_word_embeddings=tie_word_embeddings,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def head_dim(self) -> int:\n",
        "        \"\"\"\n",
        "        Retorna a dimensão de cada cabeça de atenção.\n",
        "\n",
        "        Returns:\n",
        "            int: Dimensão de cada cabeça de atenção.\n",
        "        \"\"\"\n",
        "        return self.hidden_size // self.num_attention_heads\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Converte a configuração para um dicionário.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: Dicionário contendo todos os parâmetros da configuração.\n",
        "        \"\"\"\n",
        "        output = super().to_dict()\n",
        "        output[\"head_dim\"] = self.head_dim\n",
        "        return output\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path: str, **kwargs) -> \"LlamaConfig\":\n",
        "        \"\"\"\n",
        "        Carrega uma configuração a partir de um modelo pré-treinado.\n",
        "\n",
        "        Args:\n",
        "            pretrained_model_name_or_path (str): Nome ou caminho do modelo pré-treinado.\n",
        "            **kwargs: Argumentos adicionais para sobrescrever valores da configuração carregada.\n",
        "\n",
        "        Returns:\n",
        "            LlamaConfig: Instância de configuração carregada.\n",
        "        \"\"\"\n",
        "        config_dict = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
        "        return cls(**config_dict)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"LlamaConfig(vocab_size={self.vocab_size}, \"\n",
        "                f\"hidden_size={self.hidden_size}, \"\n",
        "                f\"intermediate_size={self.intermediate_size}, \"\n",
        "                f\"num_hidden_layers={self.num_hidden_layers}, \"\n",
        "                f\"num_attention_heads={self.num_attention_heads}, \"\n",
        "                f\"max_position_embeddings={self.max_position_embeddings})\")"
      ],
      "metadata": {
        "id": "JteD23VEpx7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Rotary Embedding"
      ],
      "metadata": {
        "id": "wSnw58m_ZlLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class LlamaRotaryEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,  # Dimensão do embedding\n",
        "        max_position_embeddings: int = 2048,  # Comprimento máximo da sequência\n",
        "        base: int = 10000,  # Base para o cálculo das frequências\n",
        "        device: Optional[torch.device] = None,\n",
        "        scaling_factor: float = 1.0,  # Fator de escala para RoPE\n",
        "        rope_type: str = \"default\",  # Tipo de RoPE (default, linear, dynamic, etc.)\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.base = base\n",
        "        self.scaling_factor = scaling_factor\n",
        "        self.rope_type = rope_type\n",
        "\n",
        "        # Calcula as frequências inversas para RoPE\n",
        "        # Dimensão: [dim/2]\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "        # Cache para sequência máxima\n",
        "        self.max_seq_len_cached = max_position_embeddings\n",
        "        # Dimensão: [max_seq_len_cached]\n",
        "        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
        "        # Dimensão: [max_seq_len_cached, dim/2]\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "        # Diferentes tipos de RoPE podem ter implementações ligeiramente diferentes\n",
        "        if rope_type == \"linear\":\n",
        "            freqs = freqs * scaling_factor\n",
        "        # Dimensão: [max_seq_len_cached, dim]\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        # Dimensão: [1, 1, max_seq_len_cached, dim]\n",
        "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, position_ids: Optional[torch.LongTensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Calcula os embeddings rotacionais para as posições dadas.\n",
        "\n",
        "        Args:\n",
        "            x: Tensor de entrada (batch_size, seq_len, num_heads, head_dim)\n",
        "            position_ids: IDs das posições (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            Tupla de tensores cosseno e seno para os embeddings rotacionais\n",
        "        \"\"\"\n",
        "        if position_ids is None:\n",
        "            # Se position_ids não for fornecido, assume-se sequência contínua\n",
        "            # Dimensão: [seq_len]\n",
        "            position_ids = torch.arange(x.shape[1], device=x.device)\n",
        "\n",
        "        # Verifica se é necessário recalcular o cache para sequências mais longas\n",
        "        if position_ids.max() >= self.max_seq_len_cached:\n",
        "            self._update_cache(position_ids.max())\n",
        "\n",
        "        # Seleciona os valores de cosseno e seno correspondentes às posições\n",
        "        # Dimensão: [1, 1, seq_len, dim]\n",
        "        cos = self.cos_cached[:, :, position_ids, :]\n",
        "        sin = self.sin_cached[:, :, position_ids, :]\n",
        "\n",
        "        return (cos.to(x.device), sin.to(x.device))\n",
        "\n",
        "\n",
        "    def _update_cache(self, max_position: int):\n",
        "        \"\"\"\n",
        "        Atualiza o cache de cossenos e senos para uma sequência mais longa.\n",
        "\n",
        "        Args:\n",
        "            max_position: Nova posição máxima a ser suportada\n",
        "        \"\"\"\n",
        "        self.max_seq_len_cached = max_position\n",
        "        # Dimensão: [max_seq_len_cached]\n",
        "        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
        "        # Dimensão: [max_seq_len_cached, dim/2]\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "        if self.rope_type == \"linear\":\n",
        "            freqs = freqs * self.scaling_factor\n",
        "        # Dimensão: [max_seq_len_cached, dim]\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        # Dimensão: [1, 1, max_seq_len_cached, dim]\n",
        "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n",
        "\n",
        "\n",
        "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Rotaciona metade das dimensões do tensor.\n",
        "    Usado como parte do processo de aplicação do RoPE.\n",
        "\n",
        "    Args:\n",
        "        x: Tensor de entrada\n",
        "\n",
        "    Returns:\n",
        "        Tensor com metade das dimensões rotacionadas\n",
        "    \"\"\"\n",
        "    x1, x2 = x.chunk(2, dim=-1)\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Aplica os embeddings posicionais rotacionais aos tensores de query e key.\n",
        "\n",
        "    Args:\n",
        "        q: Tensor de query\n",
        "        k: Tensor de key\n",
        "        cos: Tensor de cossenos dos embeddings rotacionais\n",
        "        sin: Tensor de senos dos embeddings rotacionais\n",
        "\n",
        "    Returns:\n",
        "        Tupla de tensores q e k com RoPE aplicado\n",
        "    \"\"\"\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed"
      ],
      "metadata": {
        "id": "SbFgzhF5rnri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA MLP"
      ],
      "metadata": {
        "id": "NdiXFZkaZo4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional\n",
        "\n",
        "class LlamaMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementa a camada de Perceptron Multicamadas (MLP) do modelo LLaMA.\n",
        "\n",
        "    Esta classe realiza as transformações não-lineares nos estados ocultos do modelo,\n",
        "    utilizando projeções lineares e uma função de ativação. Suporta implementações\n",
        "    com e sem tensor parallelism.\n",
        "\n",
        "    Atributos:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        hidden_size (int): Tamanho do espaço oculto de entrada e saída.\n",
        "        intermediate_size (int): Tamanho do espaço intermediário onde ocorre a transformação principal.\n",
        "        gate_proj (nn.Linear): Projeção linear para o mecanismo de gate.\n",
        "        up_proj (nn.Linear): Projeção linear de expansão.\n",
        "        down_proj (nn.Linear): Projeção linear de contração.\n",
        "        act_fn (callable): Função de ativação não-linear.\n",
        "\n",
        "    Args:\n",
        "        config: Um objeto de configuração contendo os parâmetros do modelo.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(hidden_size=768, intermediate_size=3072)\n",
        "        >>> mlp = LlamaMLP(config)\n",
        "        >>> input_tensor = torch.randn(1, 10, 768)  # [batch_size, seq_length, hidden_size]\n",
        "        >>> output = mlp(input_tensor)\n",
        "        >>> print(output.shape)\n",
        "        torch.Size([1, 10, 768])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "\n",
        "        # Projeção de gate: hidden_size -> intermediate_size\n",
        "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
        "\n",
        "        # Projeção up: hidden_size -> intermediate_size\n",
        "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
        "\n",
        "        # Projeção down: intermediate_size -> hidden_size\n",
        "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
        "\n",
        "        # Função de ativação (geralmente SiLU/Swish)\n",
        "        self.act_fn = nn.SiLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward da camada MLP.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Tensor de entrada com shape [batch_size, seq_length, hidden_size].\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor de saída com shape [batch_size, seq_length, hidden_size].\n",
        "\n",
        "        Raises:\n",
        "            ValueError: Se as dimensões do tensor de entrada não forem compatíveis.\n",
        "        \"\"\"\n",
        "        # Verificação das dimensões de entrada\n",
        "        if x.dim() != 3 or x.size(-1) != self.hidden_size:\n",
        "            raise ValueError(f\"Entrada esperada de shape [batch_size, seq_length, {self.hidden_size}], \"\n",
        "                             f\"mas recebeu {x.shape}\")\n",
        "\n",
        "        if self.config.pretraining_tp > 1:\n",
        "            # Implementação para tensor parallelism (TP)\n",
        "            slice = self.intermediate_size // self.config.pretraining_tp\n",
        "\n",
        "            # Divide os pesos das projeções em fatias\n",
        "            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n",
        "            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n",
        "            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n",
        "\n",
        "            # Aplica as projeções em paralelo\n",
        "            # Cada operação: [batch_size, seq_length, slice]\n",
        "            gate_proj = torch.cat(\n",
        "                [nn.functional.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)],\n",
        "                dim=-1\n",
        "            )\n",
        "            up_proj = torch.cat(\n",
        "                [nn.functional.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)],\n",
        "                dim=-1\n",
        "            )\n",
        "\n",
        "            # Aplica a função de ativação e multiplicação elemento a elemento\n",
        "            # Dimensão: [batch_size, seq_length, intermediate_size]\n",
        "            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n",
        "\n",
        "            # Aplica a projeção down em paralelo\n",
        "            # Cada operação: [batch_size, seq_length, hidden_size // pretraining_tp]\n",
        "            down_proj = [\n",
        "                nn.functional.linear(intermediate_states[i], down_proj_slices[i])\n",
        "                for i in range(self.config.pretraining_tp)\n",
        "            ]\n",
        "\n",
        "            # Soma os resultados das projeções down\n",
        "            # Dimensão final: [batch_size, seq_length, hidden_size]\n",
        "            down_proj = sum(down_proj)\n",
        "\n",
        "        else:\n",
        "            # Implementação padrão sem tensor parallelism\n",
        "\n",
        "            # Aplica as projeções gate e up\n",
        "            # Dimensões: [batch_size, seq_length, intermediate_size]\n",
        "            gate_proj = self.gate_proj(x)\n",
        "            up_proj = self.up_proj(x)\n",
        "\n",
        "            # Aplica a função de ativação no gate e multiplica pelo resultado de up\n",
        "            # Dimensão: [batch_size, seq_length, intermediate_size]\n",
        "            intermediate_states = self.act_fn(gate_proj) * up_proj\n",
        "\n",
        "            # Aplica a projeção down\n",
        "            # Dimensão final: [batch_size, seq_length, hidden_size]\n",
        "            down_proj = self.down_proj(intermediate_states)\n",
        "\n",
        "        return down_proj\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"LlamaMLP(hidden_size={self.hidden_size}, \"\n",
        "                f\"intermediate_size={self.intermediate_size}, \"\n",
        "                f\"act_fn={self.act_fn.__class__.__name__})\")"
      ],
      "metadata": {
        "id": "jf2uEPW8vBiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA RMS Norm"
      ],
      "metadata": {
        "id": "4h29-LUbZul5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LlamaRMSNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    LlamaRMSNorm é uma variante de normalização de camada utilizada no modelo LLaMA.\n",
        "\n",
        "    Esta normalização usa a raiz quadrada da média dos quadrados (RMS) para normalizar\n",
        "    os inputs, em vez da média e variância usadas na normalização de camada padrão.\n",
        "\n",
        "    Atributos:\n",
        "        weight (nn.Parameter): Parâmetro aprendível para escala.\n",
        "        variance_epsilon (float): Pequeno valor adicionado ao denominador para estabilidade numérica.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): Dimensão do espaço oculto a ser normalizado.\n",
        "        eps (float, opcional): Epsilon para estabilidade numérica. Padrão é 1e-6.\n",
        "\n",
        "    Forma do Input:\n",
        "        - Input: (batch_size, seq_length, hidden_size)\n",
        "        - Output: (batch_size, seq_length, hidden_size)\n",
        "\n",
        "    Exemplo:\n",
        "        >>> rms_norm = LlamaRMSNorm(hidden_size=768, eps=1e-6)\n",
        "        >>> input_tensor = torch.randn(32, 50, 768)  # (batch_size, seq_length, hidden_size)\n",
        "        >>> normalized_tensor = rms_norm(input_tensor)\n",
        "        >>> print(normalized_tensor.shape)\n",
        "        torch.Size([32, 50, 768])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Aplica a normalização RMS ao tensor de entrada.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Tensor de entrada a ser normalizado.\n",
        "                Shape: (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor normalizado.\n",
        "                Shape: (batch_size, seq_length, hidden_size)\n",
        "        \"\"\"\n",
        "        input_dtype = hidden_states.dtype\n",
        "        hidden_states = hidden_states.to(torch.float32)\n",
        "\n",
        "        # Calcula a variância (média dos quadrados)\n",
        "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
        "\n",
        "        # Normaliza usando RMS\n",
        "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
        "\n",
        "        # Aplica o peso aprendível\n",
        "        return (self.weight * hidden_states).to(input_dtype)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        \"\"\"\n",
        "        Retorna uma representação de string dos principais parâmetros.\n",
        "\n",
        "        Returns:\n",
        "            str: String representando os parâmetros do módulo.\n",
        "        \"\"\"\n",
        "        return f\"hidden_size={self.weight.numel()}, eps={self.variance_epsilon}\""
      ],
      "metadata": {
        "id": "y7nm-P9EU2Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Attention"
      ],
      "metadata": {
        "id": "p1247aOeZxZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vanilla Attention"
      ],
      "metadata": {
        "id": "olhbgD_QZ1Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class LlamaAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementa o mecanismo de atenção multi-cabeça do modelo LLaMA.\n",
        "\n",
        "    Esta classe realiza a operação de auto-atenção, permitindo que o modelo foque\n",
        "    em diferentes partes da sequência de entrada. Suporta diferentes implementações\n",
        "    de atenção e otimizações como grouped-query attention.\n",
        "\n",
        "    Atributos:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        layer_idx (int): Índice da camada atual.\n",
        "        hidden_size (int): Dimensão do espaço oculto.\n",
        "        num_heads (int): Número de cabeças de atenção.\n",
        "        head_dim (int): Dimensão de cada cabeça de atenção.\n",
        "        num_key_value_heads (int): Número de cabeças para key e value (pode ser menor que num_heads).\n",
        "        max_position_embeddings (int): Número máximo de posições para embeddings.\n",
        "        rotary_emb (LlamaRotaryEmbedding): Instância para aplicar embeddings rotacionais.\n",
        "\n",
        "    Args:\n",
        "        config: Um objeto de configuração contendo os parâmetros do modelo.\n",
        "        layer_idx (Optional[int]): Índice da camada. Necessário para algumas otimizações.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(hidden_size=512, num_attention_heads=8)\n",
        "        >>> attention = LlamaAttention(config, layer_idx=0)\n",
        "        >>> hidden_states = torch.randn(1, 10, 512)  # [batch_size, seq_length, hidden_size]\n",
        "        >>> attention_mask = torch.ones(1, 1, 10, 10)  # [batch_size, 1, seq_length, seq_length]\n",
        "        >>> output, _ = attention(hidden_states, attention_mask=attention_mask)\n",
        "        >>> print(output.shape)\n",
        "        torch.Size([1, 10, 512])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "        # Verifica se as dimensões são compatíveis\n",
        "        if self.head_dim * self.num_heads != self.hidden_size:\n",
        "            raise ValueError(f\"hidden_size deve ser divisível por num_heads. \"\n",
        "                             f\"Got {self.hidden_size} e {self.num_heads}.\")\n",
        "\n",
        "        # Projections para query, key, value e output\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        \"\"\"Reshape and transpose tensor for attention computation.\"\"\"\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: bool = False,\n",
        "        use_cache: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do mecanismo de atenção.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Estados ocultos de entrada. Shape [batch_size, seq_length, hidden_size]\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção. Shape [batch_size, 1, tgt_seq_length, src_seq_length]\n",
        "            position_ids (Optional[torch.LongTensor]): IDs das posições. Shape [batch_size, seq_length]\n",
        "            past_key_value (Optional[Tuple[torch.Tensor]]): Cache de estados passados para geração autoregressiva.\n",
        "            output_attentions (bool): Se True, retorna os pesos de atenção.\n",
        "            use_cache (bool): Se True, retorna o cache para uso futuro.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "                - Estados ocultos atualizados\n",
        "                - Pesos de atenção (opcional)\n",
        "                - Novo cache de estados (opcional)\n",
        "        \"\"\"\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        # Calcula query, key, value\n",
        "        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, head_dim]\n",
        "\n",
        "        # Aplica RoPE (Rotary Position Embedding)\n",
        "        query_states, key_states = self.rotary_emb(query_states, key_states, position_ids)\n",
        "\n",
        "        # Lida com o cache de estados passados para geração autoregressiva\n",
        "        if past_key_value is not None:\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "\n",
        "        past_key_value = (key_states, value_states) if use_cache else None\n",
        "\n",
        "        # Repete key e value para cada grupo de query em grouped-query attention\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, head_dim]\n",
        "\n",
        "        # Calcula os scores de atenção\n",
        "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, seq_length]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attn_weights = attn_weights + attention_mask\n",
        "\n",
        "        # Normaliza os pesos de atenção\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "\n",
        "        # Calcula o output da atenção\n",
        "        attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, head_dim]\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
        "\n",
        "        # [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Projeção final\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        if not output_attentions:\n",
        "            attn_weights = None\n",
        "\n",
        "        return attn_output, attn_weights, past_key_value\n",
        "\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Repete os estados de key e value para grouped-query attention.\n",
        "\n",
        "    Args:\n",
        "        hidden_states (torch.Tensor): Estados de entrada [batch, num_key_value_heads, seqlen, head_dim]\n",
        "        n_rep (int): Número de repetições\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Estados repetidos [batch, num_attention_heads, seqlen, head_dim]\n",
        "    \"\"\"\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
      ],
      "metadata": {
        "id": "lSN7BH0Hv7VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sdpa Attention"
      ],
      "metadata": {
        "id": "2ZKJw_elZ3s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple\n",
        "import warnings\n",
        "\n",
        "class LlamaSdpaAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementação otimizada do mecanismo de atenção do LLaMA usando\n",
        "    scaled_dot_product_attention (SDPA) do PyTorch.\n",
        "\n",
        "    Esta classe implementa a atenção multi-cabeça com suporte a\n",
        "    Rotary Position Embedding (RoPE) e atenção agrupada.\n",
        "\n",
        "    Atributos:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        layer_idx (int): Índice da camada atual.\n",
        "        hidden_size (int): Dimensão do espaço oculto.\n",
        "        num_heads (int): Número de cabeças de atenção.\n",
        "        head_dim (int): Dimensão de cada cabeça de atenção.\n",
        "        num_key_value_heads (int): Número de cabeças para key e value (pode ser menor que num_heads).\n",
        "        max_position_embeddings (int): Número máximo de posições para embeddings.\n",
        "        rotary_emb (LlamaRotaryEmbedding): Instância para aplicar embeddings rotacionais.\n",
        "\n",
        "    Args:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        layer_idx (Optional[int]): Índice da camada. Necessário para algumas otimizações.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
        "            raise ValueError(\n",
        "                f\"hidden_size deve ser divisível por num_heads. \"\n",
        "                f\"Got {self.hidden_size} e {self.num_heads}.\"\n",
        "            )\n",
        "\n",
        "        # Projeções lineares para Q, K, V e O\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_position_embeddings=self.max_position_embeddings,\n",
        "            base=config.rope_theta,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: bool = False,\n",
        "        use_cache: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do mecanismo de atenção.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Estados ocultos de entrada.\n",
        "                Shape: [batch_size, seq_length, hidden_size]\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção.\n",
        "                Shape: [batch_size, 1, tgt_seq_length, src_seq_length]\n",
        "            position_ids (Optional[torch.LongTensor]): IDs das posições.\n",
        "                Shape: [batch_size, seq_length]\n",
        "            past_key_value (Optional[Tuple[torch.Tensor]]): Cache de estados passados.\n",
        "            output_attentions (bool): Se True, retorna os pesos de atenção.\n",
        "            use_cache (bool): Se True, retorna o cache para uso futuro.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "                - Estados ocultos atualizados\n",
        "                - Pesos de atenção (opcional)\n",
        "                - Novo cache de estados (opcional)\n",
        "        \"\"\"\n",
        "        # Obtém as dimensões do tensor de entrada\n",
        "        # hidden_states shape: [batch_size, seq_length, hidden_size]\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        if self.config.pretraining_tp > 1:\n",
        "            # Implementação para tensor parallelism\n",
        "            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n",
        "            query_slices = self.q_proj.weight.split((self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0)\n",
        "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
        "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
        "\n",
        "            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            query_states = torch.cat(query_states, dim=-1)\n",
        "\n",
        "            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            key_states = torch.cat(key_states, dim=-1)\n",
        "\n",
        "            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            value_states = torch.cat(value_states, dim=-1)\n",
        "\n",
        "        else:\n",
        "            # Projeções Q, K, V padrão\n",
        "            # Resultado: [batch_size, seq_length, num_heads * head_dim]\n",
        "            query_states = self.q_proj(hidden_states)\n",
        "            key_states = self.k_proj(hidden_states)\n",
        "            value_states = self.v_proj(hidden_states)\n",
        "\n",
        "        # Reshape e transpõe Q, K, V\n",
        "        # Resultado: [batch_size, num_heads, seq_length, head_dim]\n",
        "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calcula os embeddings rotacionais\n",
        "        # cos e sin: [1, seq_length, head_dim]\n",
        "        cos, sin = self.rotary_emb(value_states, seq_len=q_len)\n",
        "\n",
        "        # Aplica RoPE (Rotary Position Embedding) a Q e K\n",
        "        # query_states, key_states: [batch_size, num_heads, seq_length, head_dim]\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
        "\n",
        "        # Lida com o cache de estados passados para geração autoregressiva\n",
        "        if past_key_value is not None:\n",
        "            # Concatena estados passados com os atuais\n",
        "            # key_states, value_states: [batch_size, num_heads, seq_length + past_length, head_dim]\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "\n",
        "        # Prepara o cache para a próxima iteração se necessário\n",
        "        past_key_value = (key_states, value_states) if use_cache else None\n",
        "\n",
        "        # Repete K e V para atenção agrupada (grouped-query attention)\n",
        "        # key_states, value_states: [batch_size, num_heads, seq_length, head_dim]\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        # Aplica a atenção usando scaled_dot_product_attention\n",
        "        # attn_output: [batch_size, num_heads, seq_length, head_dim]\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            query_states, key_states, value_states,\n",
        "            attn_mask=attention_mask,\n",
        "            dropout_p=self.config.attention_dropout if self.training else 0.0,\n",
        "            is_causal=False\n",
        "        )\n",
        "\n",
        "        # Reorganiza o tensor de saída\n",
        "        # attn_output: [batch_size, seq_length, num_heads * head_dim]\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
        "\n",
        "        # Aplica a projeção de saída\n",
        "        # attn_output: [batch_size, seq_length, hidden_size]\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        if output_attentions:\n",
        "            warnings.warn(\"output_attentions=True não é suportado para SDPA no momento.\")\n",
        "            attn_weights = None\n",
        "        else:\n",
        "            attn_weights = None\n",
        "\n",
        "        return attn_output, attn_weights, past_key_value"
      ],
      "metadata": {
        "id": "LDQ8ZVGbXlNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Flash Attention 2"
      ],
      "metadata": {
        "id": "toLzxx4MZ9-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash_attn --quiet"
      ],
      "metadata": {
        "id": "9swlCZ6WYbwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, Tuple\n",
        "from flash_attn import flash_attn_func, flash_attn_varlen_func\n",
        "import warnings\n",
        "\n",
        "class LlamaFlashAttention2(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementação do mecanismo de atenção do LLaMA usando Flash Attention 2.\n",
        "    Esta versão é otimizada para eficiência em memória e velocidade.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
        "            raise ValueError(f\"hidden_size deve ser divisível por num_heads.\")\n",
        "\n",
        "        # Inicializa as projeções lineares para Q, K, V e O\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_position_embeddings=self.max_position_embeddings,\n",
        "            base=config.rope_theta,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: bool = False,\n",
        "        use_cache: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        # hidden_states shape: [batch_size, seq_length, hidden_size]\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        # Aplica as projeções lineares para Q, K, V\n",
        "        # Shapes: [batch_size, seq_length, num_heads * head_dim]\n",
        "        query_states = self.q_proj(hidden_states)\n",
        "        key_states = self.k_proj(hidden_states)\n",
        "        value_states = self.v_proj(hidden_states)\n",
        "\n",
        "        # Reshape e transpõe para [batch_size, num_heads, seq_length, head_dim]\n",
        "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calcula os embeddings rotacionais\n",
        "        cos, sin = self.rotary_emb(value_states, seq_len=q_len)\n",
        "        # Aplica RoPE (Rotary Position Embedding)\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
        "\n",
        "        # Lida com o cache de estados passados para geração autoregressiva\n",
        "        if past_key_value is not None:\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        past_key_value = (key_states, value_states) if use_cache else None\n",
        "\n",
        "        # Repete K e V para atenção agrupada (grouped-query attention)\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        # Prepara os tensores para Flash Attention\n",
        "        q, k, v = query_states, key_states, value_states\n",
        "\n",
        "        # Converte q, k, v para o formato esperado por Flash Attention\n",
        "        # [batch_size, seq_length, num_heads, head_dim]\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Aplica Flash Attention\n",
        "        if attention_mask is None:\n",
        "            # Usa a versão padrão do Flash Attention quando não há máscara\n",
        "            attn_output = flash_attn_func(q, k, v, dropout_p=self.config.attention_dropout if self.training else 0.0, causal=True)\n",
        "        else:\n",
        "            # Usa a versão com comprimento variável quando há máscara\n",
        "            attn_output, _ = flash_attn_varlen_func(\n",
        "                q, k, v,\n",
        "                cu_seqlens_q=attention_mask,\n",
        "                cu_seqlens_k=attention_mask,\n",
        "                max_seqlen_q=q_len,\n",
        "                max_seqlen_k=q_len,\n",
        "                dropout_p=self.config.attention_dropout if self.training else 0.0,\n",
        "                causal=True\n",
        "            )\n",
        "\n",
        "        # Reshape e aplica a projeção de saída\n",
        "        # [batch_size, seq_length, hidden_size]\n",
        "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        if output_attentions:\n",
        "            warnings.warn(\"output_attentions=True não é suportado para Flash Attention.\")\n",
        "            attn_weights = None\n",
        "        else:\n",
        "            attn_weights = None\n",
        "\n",
        "        return attn_output, attn_weights, past_key_value"
      ],
      "metadata": {
        "id": "v3AqKNbBYCuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Decoder Layer"
      ],
      "metadata": {
        "id": "eDCXFvglaDFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class LlamaDecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementa uma camada do decodificador do modelo LLaMA.\n",
        "\n",
        "    Esta classe combina os mecanismos de atenção e feed-forward network (MLP),\n",
        "    formando um bloco completo do transformer decodificador. Inclui normalizações\n",
        "    de camada e conexões residuais.\n",
        "\n",
        "    Atributos:\n",
        "        hidden_size (int): Dimensão do espaço oculto.\n",
        "        self_attn (LlamaAttention): Mecanismo de auto-atenção.\n",
        "        mlp (LlamaMLP): Rede feed-forward.\n",
        "        input_layernorm (LlamaRMSNorm): Normalização de camada para entrada.\n",
        "        post_attention_layernorm (LlamaRMSNorm): Normalização após a atenção.\n",
        "\n",
        "    Args:\n",
        "        config (LlamaConfig): Configuração do modelo LLaMA.\n",
        "        layer_idx (int): Índice da camada atual.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(hidden_size=512, intermediate_size=2048, num_attention_heads=8)\n",
        "        >>> layer = LlamaDecoderLayer(config, layer_idx=0)\n",
        "        >>> hidden_states = torch.randn(1, 10, 512)  # [batch_size, seq_length, hidden_size]\n",
        "        >>> attention_mask = torch.ones(1, 1, 10, 10)  # [batch_size, 1, seq_length, seq_length]\n",
        "        >>> outputs = layer(hidden_states, attention_mask=attention_mask)\n",
        "        >>> print(outputs[0].shape)\n",
        "        torch.Size([1, 10, 512])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
        "        self.mlp = LlamaMLP(config)\n",
        "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward de uma camada do decodificador.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Estados ocultos de entrada.\n",
        "                Shape [batch_size, seq_length, hidden_size]\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção.\n",
        "                Shape [batch_size, 1, tgt_seq_length, src_seq_length]\n",
        "            position_ids (Optional[torch.LongTensor]): IDs das posições.\n",
        "                Shape [batch_size, seq_length]\n",
        "            past_key_value (Optional[Tuple[torch.Tensor]]): Cache de estados passados para\n",
        "                geração autoregressiva.\n",
        "            output_attentions (Optional[bool]): Se True, retorna os pesos de atenção.\n",
        "            use_cache (Optional[bool]): Se True, retorna o cache para uso futuro.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "                - Estados ocultos atualizados\n",
        "                - Tupla contendo os novos cache de estados (se use_cache=True)\n",
        "        \"\"\"\n",
        "        # Shape de hidden_states: [batch_size, seq_length, hidden_size]\n",
        "        residual = hidden_states\n",
        "\n",
        "        # Normalização de camada na entrada\n",
        "        hidden_states = self.input_layernorm(hidden_states)\n",
        "        # Shape após normalização: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Self Attention\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_value=past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "            use_cache=use_cache,\n",
        "        )\n",
        "        # Shape após atenção: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Conexão residual após a atenção\n",
        "        hidden_states = residual + hidden_states\n",
        "        # Shape após conexão residual: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Normalização de camada após a atenção\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
        "        # Shape após normalização: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # MLP (Feed-Forward Network)\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        # Shape após MLP: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Conexão residual após o MLP\n",
        "        hidden_states = residual + hidden_states\n",
        "        # Shape final: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights,)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"LlamaDecoderLayer(hidden_size={self.hidden_size})\""
      ],
      "metadata": {
        "id": "PQ5ZrqsbxQie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Pre-Trained Model"
      ],
      "metadata": {
        "id": "-IK_BO3TaHfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedModel\n",
        "from transformers.modeling_utils import PretrainedConfig\n",
        "from typing import Union, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LlamaPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Classe base abstrata para modelos pré-treinados LLaMA.\n",
        "\n",
        "    Esta classe herda de `PreTrainedModel` e implementa funcionalidades específicas\n",
        "    para modelos LLaMA, incluindo inicialização de pesos e configurações de otimização.\n",
        "\n",
        "    Atributos:\n",
        "        config_class (Type[PretrainedConfig]): Classe de configuração para modelos LLaMA.\n",
        "        base_model_prefix (str): Prefixo usado para nomear o modelo base.\n",
        "        supports_gradient_checkpointing (bool): Indica suporte a checkpointing de gradiente.\n",
        "        _no_split_modules (List[str]): Lista de módulos que não devem ser divididos durante\n",
        "                                       o processamento paralelo.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> from transformers import LlamaConfig\n",
        "        >>> class MyLlamaModel(LlamaPreTrainedModel):\n",
        "        ...     def __init__(self, config):\n",
        "        ...         super().__init__(config)\n",
        "        ...         # Implementação do modelo\n",
        "        ...\n",
        "        >>> config = LlamaConfig()\n",
        "        >>> model = MyLlamaModel(config)\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = LlamaConfig\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _no_split_modules = [\"LlamaDecoderLayer\"]\n",
        "    _skip_keys_device_placement = [\"past_key_values\"]\n",
        "    _supports_flash_attn_2 = True\n",
        "    _supports_sdpa = True\n",
        "    _supports_cache_class = True\n",
        "    _supports_quantized_cache = True\n",
        "    _supports_static_cache = True\n",
        "\n",
        "    def __init__(self, config: LlamaConfig, *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "\n",
        "    def _init_weights(self, module: nn.Module):\n",
        "        \"\"\"\n",
        "        Inicializa os pesos do módulo.\n",
        "\n",
        "        Esta função é chamada para cada submódulo durante a inicialização do modelo.\n",
        "        Implementa a estratégia de inicialização de pesos específica para modelos LLaMA.\n",
        "\n",
        "        Args:\n",
        "            module (nn.Module): O módulo cujos pesos serão inicializados.\n",
        "        \"\"\"\n",
        "        std = self.config.initializer_range\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module: nn.Module, value: bool = False):\n",
        "        \"\"\"\n",
        "        Configura o checkpointing de gradiente para o módulo.\n",
        "\n",
        "        O checkpointing de gradiente pode ser usado para economizar memória durante o treinamento,\n",
        "        recalculando os gradientes durante a passagem backward em vez de armazená-los.\n",
        "\n",
        "        Args:\n",
        "            module (nn.Module): O módulo para configurar o checkpointing.\n",
        "            value (bool): Se True, ativa o checkpointing de gradiente.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (LlamaDecoderLayer, LlamaModel)):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "    def gradient_checkpointing_enable(self):\n",
        "        \"\"\"\n",
        "        Ativa o checkpointing de gradiente para todo o modelo.\n",
        "        \"\"\"\n",
        "        self.apply(lambda module: self._set_gradient_checkpointing(module, value=True))\n",
        "\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        \"\"\"\n",
        "        Desativa o checkpointing de gradiente para todo o modelo.\n",
        "        \"\"\"\n",
        "        self.apply(lambda module: self._set_gradient_checkpointing(module, value=False))\n",
        "\n",
        "    def enable_input_require_grads(self):\n",
        "        \"\"\"\n",
        "        Configura o modelo para permitir gradientes nos inputs.\n",
        "\n",
        "        Isso é necessário para técnicas como adversarial training.\n",
        "        \"\"\"\n",
        "        def make_inputs_require_grads(module, input, output):\n",
        "            output.requires_grad_(True)\n",
        "\n",
        "        self._require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n",
        "\n",
        "    def disable_input_require_grads(self):\n",
        "        \"\"\"\n",
        "        Remove a configuração que permite gradientes nos inputs.\n",
        "        \"\"\"\n",
        "        self._require_grads_hook.remove()\n",
        "\n",
        "    def get_position_embeddings(self) -> Optional[Union[nn.Embedding, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Retorna as embeddings de posição do modelo, se existirem.\n",
        "\n",
        "        Returns:\n",
        "            Optional[Union[nn.Embedding, torch.Tensor]]: As embeddings de posição ou None.\n",
        "        \"\"\"\n",
        "        if hasattr(self, \"rotary_emb\"):\n",
        "            return self.rotary_emb\n",
        "        return None\n",
        "\n",
        "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
        "        \"\"\"\n",
        "        Redimensiona as embeddings de posição do modelo.\n",
        "\n",
        "        Args:\n",
        "            new_num_position_embeddings (int): O novo número de posições.\n",
        "\n",
        "        Raises:\n",
        "            NotImplementedError: Esta funcionalidade não está implementada para modelos LLaMA.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\n",
        "            f\"{self.__class__.__name__} não suporta o redimensionamento das embeddings de posição.\"\n",
        "        )\n",
        "\n",
        "    def get_output_embeddings(self) -> Optional[nn.Module]:\n",
        "        \"\"\"\n",
        "        Retorna as embeddings de saída do modelo, se existirem.\n",
        "\n",
        "        Returns:\n",
        "            Optional[nn.Module]: As embeddings de saída ou None.\n",
        "        \"\"\"\n",
        "        return None  # LLaMA não usa embeddings de saída por padrão\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings: Optional[nn.Module]):\n",
        "        \"\"\"\n",
        "        Define novas embeddings de saída para o modelo.\n",
        "\n",
        "        Args:\n",
        "            new_embeddings (Optional[nn.Module]): As novas embeddings de saída.\n",
        "\n",
        "        Raises:\n",
        "            NotImplementedError: Esta funcionalidade não está implementada para modelos LLaMA.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\n",
        "            f\"{self.__class__.__name__} não suporta a mudança das embeddings de saída.\"\n",
        "        )"
      ],
      "metadata": {
        "id": "EcLz7dF_xjtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Model"
      ],
      "metadata": {
        "id": "oWIpC2lxaLN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, Tuple, Union, List\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
        "\n",
        "class LlamaModel(LlamaPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Modelo base LLaMA.\n",
        "\n",
        "    Esta classe implementa a estrutura principal do modelo LLaMA, incluindo\n",
        "    as camadas de embedding, as camadas do decodificador e a normalização final.\n",
        "\n",
        "    Atributos:\n",
        "        config (LlamaConfig): Configuração do modelo.\n",
        "        padding_idx (int): Índice do token de padding.\n",
        "        vocab_size (int): Tamanho do vocabulário.\n",
        "        embed_tokens (nn.Embedding): Camada de embedding para tokens.\n",
        "        layers (nn.ModuleList): Lista de camadas do decodificador.\n",
        "        norm (LlamaRMSNorm): Camada de normalização final.\n",
        "        gradient_checkpointing (bool): Se o checkpointing de gradiente está ativado.\n",
        "\n",
        "    Args:\n",
        "        config (LlamaConfig): Configuração do modelo LLaMA.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> from transformers import LlamaConfig\n",
        "        >>> config = LlamaConfig()\n",
        "        >>> model = LlamaModel(config)\n",
        "        >>> input_ids = torch.randint(0, config.vocab_size, (1, 10))\n",
        "        >>> outputs = model(input_ids)\n",
        "        >>> last_hidden_states = outputs.last_hidden_state\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: LlamaConfig):\n",
        "        super().__init__(config)\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
        "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do modelo LLaMA.\n",
        "\n",
        "        Args:\n",
        "            input_ids (Optional[torch.LongTensor]): Tensor de IDs de tokens de entrada.\n",
        "                Shape: (batch_size, sequence_length)\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção para os tokens de entrada.\n",
        "                Shape: (batch_size, sequence_length)\n",
        "            position_ids (Optional[torch.LongTensor]): IDs de posição para os tokens de entrada.\n",
        "                Shape: (batch_size, sequence_length)\n",
        "            past_key_values (Optional[List[torch.FloatTensor]]): Lista de tensores contendo estados passados\n",
        "                para uso em geração incremental.\n",
        "            inputs_embeds (Optional[torch.FloatTensor]): Embeddings pré-computados para substituir input_ids.\n",
        "                Shape: (batch_size, sequence_length, hidden_size)\n",
        "            use_cache (Optional[bool]): Se deve retornar um cache para geração incremental.\n",
        "            output_attentions (Optional[bool]): Se deve retornar todas as atenções.\n",
        "            output_hidden_states (Optional[bool]): Se deve retornar todos os estados ocultos.\n",
        "            return_dict (Optional[bool]): Se deve retornar um dicionário ao invés de uma tupla.\n",
        "\n",
        "        Returns:\n",
        "            Union[Tuple, BaseModelOutputWithPast]: Saída do modelo, incluindo últimos estados ocultos,\n",
        "                past_key_values (se use_cache=True), e opcionalmente todos os estados ocultos e atenções.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # Recuperar embeddings de entrada\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"Você não pode especificar tanto input_ids quanto inputs_embeds ao mesmo tempo\")\n",
        "        elif input_ids is not None:\n",
        "            # input_ids shape: (batch_size, sequence_length)\n",
        "            batch_size, seq_length = input_ids.shape\n",
        "            inputs_embeds = self.embed_tokens(input_ids)\n",
        "            # inputs_embeds shape: (batch_size, sequence_length, hidden_size)\n",
        "        elif inputs_embeds is not None:\n",
        "            batch_size, seq_length, _ = inputs_embeds.shape\n",
        "        else:\n",
        "            raise ValueError(\"Você deve especificar ou input_ids ou inputs_embeds\")\n",
        "\n",
        "        # Gerar position_ids se não fornecidos\n",
        "        if position_ids is None:\n",
        "            # position_ids shape: (batch_size, sequence_length)\n",
        "            position_ids = torch.arange(seq_length, dtype=torch.long, device=inputs_embeds.device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Gerar máscara de atenção se não fornecida\n",
        "        if attention_mask is None:\n",
        "            # attention_mask shape: (batch_size, sequence_length)\n",
        "            attention_mask = torch.ones((batch_size, seq_length), device=inputs_embeds.device)\n",
        "\n",
        "        # Converter máscara de atenção para o formato correto (expandido para todas as cabeças)\n",
        "        # extended_attention_mask shape: (batch_size, 1, 1, sequence_length)\n",
        "        extended_attention_mask = self.get_extended_attention_mask(attention_mask, (batch_size, seq_length))\n",
        "\n",
        "        # hidden_states shape: (batch_size, sequence_length, hidden_size)\n",
        "        hidden_states = inputs_embeds\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "\n",
        "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "                layer_outputs = self._gradient_checkpointing_func(\n",
        "                    decoder_layer.__call__,\n",
        "                    hidden_states,\n",
        "                    extended_attention_mask,\n",
        "                    position_ids,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                    use_cache,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=extended_attention_mask,\n",
        "                    position_ids=position_ids,\n",
        "                    past_key_value=past_key_value,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                )\n",
        "\n",
        "            # layer_outputs[0] shape: (batch_size, sequence_length, hidden_size)\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                # next_decoder_cache shape (para cada camada):\n",
        "                # (2, batch_size, num_heads, sequence_length, head_dim)\n",
        "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                # all_self_attentions shape (para cada camada):\n",
        "                # (batch_size, num_heads, sequence_length, sequence_length)\n",
        "                all_self_attentions += (layer_outputs[1],)\n",
        "\n",
        "        # Normalização final\n",
        "        # hidden_states shape: (batch_size, sequence_length, hidden_size)\n",
        "        hidden_states = self.norm(hidden_states)\n",
        "\n",
        "        # Adicionar últimos estados ocultos\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        next_cache = next_decoder_cache if use_cache else None\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attentions] if v is not None)\n",
        "\n",
        "        return BaseModelOutputWithPast(\n",
        "            last_hidden_state=hidden_states,  # (batch_size, sequence_length, hidden_size)\n",
        "            past_key_values=next_cache,       # Lista de tensores, cada um com shape:\n",
        "                                              # (2, batch_size, num_heads, sequence_length, head_dim)\n",
        "            hidden_states=all_hidden_states,  # Tupla de tensores, cada um com shape:\n",
        "                                              # (batch_size, sequence_length, hidden_size)\n",
        "            attentions=all_self_attentions,   # Tupla de tensores, cada um com shape:\n",
        "                                              # (batch_size, num_heads, sequence_length, sequence_length)\n",
        "        )"
      ],
      "metadata": {
        "id": "2IUNbEY6208o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Implmentations"
      ],
      "metadata": {
        "id": "600DUO0saOKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Causal Language Modeling"
      ],
      "metadata": {
        "id": "QerDXIzeaUHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from typing import Optional, Tuple, Union, List\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "\n",
        "class LlamaForCausalLM(LlamaPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Modelo LLaMA para Modelagem de Linguagem Causal.\n",
        "\n",
        "    Esta classe implementa o modelo LLaMA específico para tarefas de geração de texto,\n",
        "    adicionando uma camada de saída linear (lm_head) ao modelo base LLaMA.\n",
        "\n",
        "    Atributos:\n",
        "        model (LlamaModel): O modelo base LLaMA.\n",
        "        lm_head (nn.Linear): Camada linear para projetar estados ocultos no espaço do vocabulário.\n",
        "        vocab_size (int): Tamanho do vocabulário do modelo.\n",
        "\n",
        "    Args:\n",
        "        config (LlamaConfig): Configuração do modelo LLaMA.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> from transformers import LlamaConfig, LlamaTokenizer\n",
        "        >>> config = LlamaConfig()\n",
        "        >>> model = LlamaForCausalLM(config)\n",
        "        >>> tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "        >>> inputs = tokenizer(\"Olá, como vai?\", return_tensors=\"pt\")\n",
        "        >>> outputs = model(**inputs)\n",
        "        >>> logits = outputs.logits\n",
        "    \"\"\"\n",
        "\n",
        "    _tied_weights_keys = [\"lm_head.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = LlamaModel(config)\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Inicializa pesos e aplica processamento final\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.embed_tokens = value\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def set_decoder(self, decoder):\n",
        "        self.model = decoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do modelo.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.LongTensor): IDs dos tokens de entrada.\n",
        "            attention_mask (torch.Tensor, opcional): Máscara de atenção.\n",
        "            position_ids (torch.LongTensor, opcional): IDs das posições.\n",
        "            past_key_values (List[torch.FloatTensor], opcional): Valores passados para uso em geração incremental.\n",
        "            inputs_embeds (torch.FloatTensor, opcional): Embeddings de entrada pré-computados.\n",
        "            labels (torch.LongTensor, opcional): Rótulos para cálculo de perda.\n",
        "            use_cache (bool, opcional): Se deve usar cache para geração incremental.\n",
        "            output_attentions (bool, opcional): Se deve retornar todas as atenções.\n",
        "            output_hidden_states (bool, opcional): Se deve retornar todos os estados ocultos.\n",
        "            return_dict (bool, opcional): Se deve retornar um dicionário ou tupla.\n",
        "\n",
        "        Returns:\n",
        "            Union[Tuple, CausalLMOutputWithPast]: Saída do modelo, incluindo logits, loss (se labels fornecidos),\n",
        "                                                  past_key_values (se use_cache=True), e opcionalmente\n",
        "                                                  hidden_states e attentions.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "        logits = self.lm_head(hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
        "            shift_labels = shift_labels.view(-1)\n",
        "            # Enable model parallelism\n",
        "            shift_labels = shift_labels.to(shift_logits.device)\n",
        "            loss = loss_fct(shift_logits, shift_labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
        "    ):\n",
        "        if past_key_values:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
        "        if inputs_embeds is not None and past_key_values is None:\n",
        "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
        "        else:\n",
        "            model_inputs = {\"input_ids\": input_ids}\n",
        "\n",
        "        model_inputs.update(\n",
        "            {\n",
        "                \"past_key_values\": past_key_values,\n",
        "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"position_ids\": kwargs.get(\"position_ids\"),\n",
        "            }\n",
        "        )\n",
        "        return model_inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past_key_values, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past_key_values:\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
        "            )\n",
        "        return reordered_past"
      ],
      "metadata": {
        "id": "ZQX07y71ye30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Fine-Tunning Trainer"
      ],
      "metadata": {
        "id": "k3E3eexBAvmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from typing import Dict, List, Optional, Union\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "\n",
        "class LlamaSFTTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    LlamaSFTTrainer é uma classe personalizada para treinar modelos LlamaForCausalLM.\n",
        "    Herda da classe Trainer do Hugging Face e implementa funcionalidades específicas\n",
        "    para o treinamento de modelos LLaMA.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: LlamaForCausalLM,\n",
        "        args: TrainingArguments,\n",
        "        train_dataset: Optional[Dataset] = None,\n",
        "        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n",
        "        tokenizer: Optional[LlamaTokenizer] = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(model, args, train_dataset, eval_dataset, tokenizer, **kwargs)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        \"\"\"\n",
        "        Calcula a perda para o modelo LLaMA.\n",
        "        \"\"\"\n",
        "        if \"labels\" in inputs:\n",
        "            labels = inputs.pop(\"labels\")\n",
        "        else:\n",
        "            labels = None\n",
        "\n",
        "        # Passa os inputs pelo modelo\n",
        "        # inputs: Dict[str, torch.Tensor] onde cada tensor tem shape [batch_size, seq_length]\n",
        "        # outputs: Objeto contendo loss e logits, ambos com shape [batch_size, seq_length, vocab_size]\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        if labels is not None and self.label_smoother:\n",
        "            loss = self.label_smoother(outputs, labels)\n",
        "        else:\n",
        "            loss = outputs.loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def prediction_step(\n",
        "        self,\n",
        "        model: LlamaForCausalLM,\n",
        "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
        "        prediction_loss_only: bool,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Realiza um passo de predição/avaliação no modelo.\n",
        "        \"\"\"\n",
        "        # Se não estiver gerando predições ou se estiver calculando apenas a perda, usa o método da classe pai\n",
        "        if not self.args.predict_with_generate or prediction_loss_only:\n",
        "            return super().prediction_step(\n",
        "                model,\n",
        "                inputs,\n",
        "                prediction_loss_only=prediction_loss_only,\n",
        "                ignore_keys=ignore_keys\n",
        "            )\n",
        "\n",
        "        # Prepara os inputs para o modelo\n",
        "        has_labels = \"labels\" in inputs\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        # Configura os parâmetros para geração de texto\n",
        "        gen_kwargs = self._gen_kwargs.copy()\n",
        "        if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:\n",
        "            gen_kwargs[\"max_length\"] = self.model.config.max_length\n",
        "\n",
        "        gen_kwargs[\"num_beams\"] = gen_kwargs.get(\"num_beams\", self.model.config.num_beams)\n",
        "\n",
        "        # Gera tokens usando o modelo\n",
        "        # input_ids: [batch_size, seq_length]\n",
        "        # attention_mask: [batch_size, seq_length]\n",
        "        # generated_tokens: [batch_size, max_length]\n",
        "        generated_tokens = self.model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            **gen_kwargs,\n",
        "        )\n",
        "\n",
        "        # Faz o padding dos tokens gerados se necessário\n",
        "        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
        "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
        "            # generated_tokens após padding: [batch_size, max_length]\n",
        "\n",
        "        # Calcula a perda se houver labels\n",
        "        with torch.no_grad():\n",
        "            if has_labels:\n",
        "                with self.autocast_smart_context_manager():\n",
        "                    # outputs: Objeto contendo loss e logits, ambos com shape [batch_size, seq_length, vocab_size]\n",
        "                    outputs = model(**inputs)\n",
        "                if self.label_smoother is not None:\n",
        "                    # loss: escalar tensor []\n",
        "                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n",
        "                else:\n",
        "                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
        "            else:\n",
        "                loss = None\n",
        "\n",
        "        if self.args.prediction_loss_only:\n",
        "            return (loss, None, None)\n",
        "\n",
        "\n",
        "        # Prepara as labels para retorno\n",
        "        labels = inputs[\"labels\"]\n",
        "        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n",
        "            labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n",
        "            # labels após padding: [batch_size, max_length]\n",
        "\n",
        "        # Retorna a perda, tokens gerados e labels\n",
        "        return (loss, generated_tokens, labels)\n",
        "\n",
        "    def _pad_tensors_to_max_len(self, tensor, max_length):\n",
        "        \"\"\"\n",
        "        Faz o padding de um tensor para um comprimento máximo especificado.\n",
        "        \"\"\"\n",
        "        # Verifica se o tokenizer está disponível\n",
        "        if self.tokenizer is None:\n",
        "            raise ValueError(\"Tokenizer é necessário para fazer o padding dos tensores até o comprimento máximo.\")\n",
        "\n",
        "        # Se o tensor já tem o comprimento máximo ou maior, retorna sem modificação\n",
        "        if tensor.shape[-1] >= max_length:\n",
        "            return tensor\n",
        "\n",
        "        # Faz o padding do tensor\n",
        "        # tensor: [batch_size, seq_length]\n",
        "        padded_tensor = self.tokenizer.pad(\n",
        "            {\"input_ids\": tensor},\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )[\"input_ids\"]\n",
        "        # padded_tensor: [batch_size, max_length]\n",
        "\n",
        "        return padded_tensor\n"
      ],
      "metadata": {
        "id": "Xge7QBIQAvAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "74d12LiJal2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "# Carregar modelo e tokenizador\n",
        "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Preparar entrada\n",
        "input_text = \"Olá, como você está?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Gerar texto\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "\n",
        "# Decodificar saída\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "3RteoxBV1Aqc",
        "outputId": "40861ab4-a744-43d6-9b88-6a8864cd76eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Force download failed due to the above error.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1753\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1675\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    320\u001b[0m             )\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-66e99540-1f6b519170f10281236b6017;6464a062-ed01-4797-b592-e2d05e0a45bf)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-hf to ask for access.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2b34f2c9ba2f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Carregar modelo e tokenizador\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta-llama/Llama-2-7b-hf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta-llama/Llama-2-7b-hf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3245\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3246\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3247\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   3248\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3249\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcustom_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         )\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1241\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;31m# From now on, etag, commit_hash, url and size are not None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1843\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot pass 'force_download=True' when offline mode is enabled.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Force download failed due to the above error.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0;31m# No head call + couldn't find an appropriate file on disk => raise an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Force download failed due to the above error."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bvvI97zy2Oyh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Como aproveitar ao máximo sua assinatura do Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}