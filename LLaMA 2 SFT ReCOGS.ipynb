{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegomrodrigues/my_llama_2/blob/main/LLaMA%202%20SFT%20ReCOGS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Custom LLaMA Implementation"
      ],
      "metadata": {
        "id": "H6nCbDvlarog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Config"
      ],
      "metadata": {
        "id": "qK6iKRA7Zeht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PretrainedConfig\n",
        "from typing import Dict, Any, Optional, Union\n",
        "\n",
        "class LlamaConfig(PretrainedConfig):\n",
        "    \"\"\"\n",
        "    Configuração para o modelo LLaMA.\n",
        "\n",
        "    Esta classe define todos os parâmetros necessários para construir e configurar\n",
        "    um modelo LLaMA. Herda de PretrainedConfig da biblioteca Transformers.\n",
        "\n",
        "    Atributos:\n",
        "        vocab_size (int): Tamanho do vocabulário do modelo.\n",
        "        hidden_size (int): Dimensão dos vetores de estado oculto e embeddings.\n",
        "        intermediate_size (int): Dimensão da camada intermediária no MLP.\n",
        "        num_hidden_layers (int): Número de camadas de transformer no modelo.\n",
        "        num_attention_heads (int): Número de cabeças de atenção em cada camada.\n",
        "        num_key_value_heads (int): Número de cabeças para key e value (para atenção agrupada).\n",
        "        hidden_act (str): Função de ativação usada no MLP.\n",
        "        max_position_embeddings (int): Número máximo de posições para embeddings.\n",
        "        initializer_range (float): Desvio padrão da distribuição normal para inicialização de pesos.\n",
        "        rms_norm_eps (float): Epsilon usado na normalização RMS.\n",
        "        use_cache (bool): Se deve usar cache para geração incremental.\n",
        "        pad_token_id (int): ID do token de padding.\n",
        "        bos_token_id (int): ID do token de início de sequência.\n",
        "        eos_token_id (int): ID do token de fim de sequência.\n",
        "        pretraining_tp (int): Grau de paralelismo de tensor usado no pré-treinamento.\n",
        "        tie_word_embeddings (bool): Se deve compartilhar pesos entre embeddings de entrada e saída.\n",
        "        rope_theta (float): Valor theta para RoPE (Rotary Position Embedding).\n",
        "        rope_scaling (Dict[str, Any]): Configuração de escala para RoPE.\n",
        "        attention_bias (bool): Se deve usar bias nos cálculos de atenção.\n",
        "        attention_dropout (float): Taxa de dropout aplicada na camada de atenção.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(\n",
        "        ...     vocab_size=32000,\n",
        "        ...     hidden_size=4096,\n",
        "        ...     intermediate_size=11008,\n",
        "        ...     num_hidden_layers=32,\n",
        "        ...     num_attention_heads=32,\n",
        "        ... )\n",
        "        >>> print(config)\n",
        "    \"\"\"\n",
        "\n",
        "    model_type = \"llama\"\n",
        "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int = 32000,\n",
        "        hidden_size: int = 4096,\n",
        "        intermediate_size: int = 11008,\n",
        "        num_hidden_layers: int = 32,\n",
        "        num_attention_heads: int = 32,\n",
        "        num_key_value_heads: Optional[int] = None,\n",
        "        hidden_act: str = \"silu\",\n",
        "        rotary_emb_base: float = 10000.0,\n",
        "        rotary_emb_fraction: float = 1.0,\n",
        "        max_position_embeddings: int = 2048,\n",
        "        initializer_range: float = 0.02,\n",
        "        rms_norm_eps: float = 1e-6,\n",
        "        use_cache: bool = True,\n",
        "        pad_token_id: int = -1,\n",
        "        bos_token_id: int = 1,\n",
        "        eos_token_id: int = 2,\n",
        "        pretraining_tp: int = 1,\n",
        "        tie_word_embeddings: bool = False,\n",
        "        rope_theta: float = 10000.0,\n",
        "        rope_scaling: Optional[Dict[str, Union[float, str]]] = None,\n",
        "        attention_bias: bool = False,\n",
        "        mlp_bias: bool = False,\n",
        "        attention_dropout: float = 0.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.hidden_size = hidden_size\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.num_key_value_heads = num_key_value_heads if num_key_value_heads is not None else num_attention_heads\n",
        "        self.hidden_act = hidden_act\n",
        "        self.rotary_emb_base = rotary_emb_base\n",
        "        self.rotary_emb_fraction = rotary_emb_fraction\n",
        "        self.initializer_range = initializer_range\n",
        "        self.rms_norm_eps = rms_norm_eps\n",
        "        self.pretraining_tp = pretraining_tp\n",
        "        self.use_cache = use_cache\n",
        "        self.rope_theta = rope_theta\n",
        "        self.rope_scaling = rope_scaling\n",
        "        self.attention_bias = attention_bias\n",
        "        self.mlp_bias = mlp_bias\n",
        "        self.attention_dropout = attention_dropout\n",
        "\n",
        "        super().__init__(\n",
        "            pad_token_id=pad_token_id,\n",
        "            bos_token_id=bos_token_id,\n",
        "            eos_token_id=eos_token_id,\n",
        "            tie_word_embeddings=tie_word_embeddings,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def head_dim(self) -> int:\n",
        "        \"\"\"\n",
        "        Retorna a dimensão de cada cabeça de atenção.\n",
        "\n",
        "        Returns:\n",
        "            int: Dimensão de cada cabeça de atenção.\n",
        "        \"\"\"\n",
        "        return self.hidden_size // self.num_attention_heads\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Converte a configuração para um dicionário.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: Dicionário contendo todos os parâmetros da configuração.\n",
        "        \"\"\"\n",
        "        output = super().to_dict()\n",
        "        output[\"head_dim\"] = self.head_dim\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"LlamaConfig(vocab_size={self.vocab_size}, \"\n",
        "                f\"hidden_size={self.hidden_size}, \"\n",
        "                f\"intermediate_size={self.intermediate_size}, \"\n",
        "                f\"num_hidden_layers={self.num_hidden_layers}, \"\n",
        "                f\"num_attention_heads={self.num_attention_heads}, \"\n",
        "                f\"max_position_embeddings={self.max_position_embeddings})\")"
      ],
      "metadata": {
        "id": "JteD23VEpx7_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Rotary Embedding"
      ],
      "metadata": {
        "id": "wSnw58m_ZlLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class LlamaRotaryEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,  # Dimensão do embedding\n",
        "        max_position_embeddings: int = 2048,  # Comprimento máximo da sequência\n",
        "        base: float = 10000.0,  # Base para o cálculo das frequências\n",
        "        device: Optional[torch.device] = None,\n",
        "        rope_scaling: Optional[Dict[str, Union[float, str]]] = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.base = base\n",
        "\n",
        "        # Calcula as frequências inversas para RoPE\n",
        "        # Dimensão: [dim/2]\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "        if rope_scaling is not None:\n",
        "            scaling_type = rope_scaling[\"type\"]\n",
        "            scaling_factor = rope_scaling[\"factor\"]\n",
        "            if scaling_type == \"linear\":\n",
        "                self.seq_len_scaling = scaling_factor\n",
        "            else:\n",
        "                raise ValueError(f\"Tipo de scaling desconhecido: {scaling_type}\")\n",
        "        else:\n",
        "            self.seq_len_scaling = 1.0\n",
        "\n",
        "        # Cache para sequência máxima\n",
        "        self.max_seq_len_cached = max_position_embeddings\n",
        "\n",
        "        # Para garantir compatibilidade com HF\n",
        "        self._build_cache()\n",
        "\n",
        "    def _build_cache(self):\n",
        "        seq_len = self.max_seq_len_cached\n",
        "        # Dimensão: [max_seq_len_cached]\n",
        "        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
        "        if self.seq_len_scaling != 1.0:\n",
        "            t = t / self.seq_len_scaling\n",
        "\n",
        "        # Dimensão: [max_seq_len_cached, dim/2]\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "\n",
        "        # Dimensão: [max_seq_len_cached, dim]\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "\n",
        "        # Dimensão: [1, 1, max_seq_len_cached, dim]\n",
        "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, seq_len: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Calcula os embeddings rotacionais para as posições dadas.\n",
        "\n",
        "        Args:\n",
        "            x: Tensor de entrada (batch_size, seq_len, num_heads, head_dim)\n",
        "            position_ids: IDs das posições (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            Tupla de tensores cosseno e seno para os embeddings rotacionais\n",
        "        \"\"\"\n",
        "        if seq_len is None:\n",
        "            seq_len = x.shape[2]\n",
        "\n",
        "        # Verifica se é necessário recalcular o cache para sequências mais longas\n",
        "        if seq_len > self.max_seq_len_cached:\n",
        "            self._update_cache(seq_len)\n",
        "\n",
        "        # Seleciona os valores de cosseno e seno correspondentes às posições\n",
        "        # Dimensão: [1, 1, seq_len, dim]\n",
        "        cos = self.cos_cached[:, :, :seq_len, :]\n",
        "        sin = self.sin_cached[:, :, :seq_len, :]\n",
        "\n",
        "        return (cos.to(x.device), sin.to(x.device))\n",
        "\n",
        "\n",
        "    def _update_cache(self, max_position: int):\n",
        "        \"\"\"\n",
        "        Atualiza o cache de cossenos e senos para uma sequência mais longa.\n",
        "\n",
        "        Args:\n",
        "            max_position: Nova posição máxima a ser suportada\n",
        "        \"\"\"\n",
        "        self.max_seq_len_cached = max_position\n",
        "        # Dimensão: [max_seq_len_cached]\n",
        "        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
        "        # Dimensão: [max_seq_len_cached, dim/2]\n",
        "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "        if self.rope_type == \"linear\":\n",
        "            freqs = freqs * self.scaling_factor\n",
        "        # Dimensão: [max_seq_len_cached, dim]\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        # Dimensão: [1, 1, max_seq_len_cached, dim]\n",
        "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n",
        "\n",
        "\n",
        "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Rotaciona metade das dimensões do tensor.\n",
        "    Usado como parte do processo de aplicação do RoPE.\n",
        "\n",
        "    Args:\n",
        "        x: Tensor de entrada\n",
        "\n",
        "    Returns:\n",
        "        Tensor com metade das dimensões rotacionadas\n",
        "    \"\"\"\n",
        "    x1, x2 = x.chunk(2, dim=-1)\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Aplica os embeddings posicionais rotacionais aos tensores de query e key.\n",
        "\n",
        "    Args:\n",
        "        q: Tensor de query\n",
        "        k: Tensor de key\n",
        "        cos: Tensor de cossenos dos embeddings rotacionais\n",
        "        sin: Tensor de senos dos embeddings rotacionais\n",
        "\n",
        "    Returns:\n",
        "        Tupla de tensores q e k com RoPE aplicado\n",
        "    \"\"\"\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed"
      ],
      "metadata": {
        "id": "SbFgzhF5rnri"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA MLP"
      ],
      "metadata": {
        "id": "NdiXFZkaZo4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional\n",
        "\n",
        "class LlamaMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementa a camada de Perceptron Multicamadas (MLP) do modelo LLaMA.\n",
        "\n",
        "    Esta classe realiza as transformações não-lineares nos estados ocultos do modelo,\n",
        "    utilizando projeções lineares e uma função de ativação. Suporta implementações\n",
        "    com e sem tensor parallelism.\n",
        "\n",
        "    Atributos:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        hidden_size (int): Tamanho do espaço oculto de entrada e saída.\n",
        "        intermediate_size (int): Tamanho do espaço intermediário onde ocorre a transformação principal.\n",
        "        gate_proj (nn.Linear): Projeção linear para o mecanismo de gate.\n",
        "        up_proj (nn.Linear): Projeção linear de expansão.\n",
        "        down_proj (nn.Linear): Projeção linear de contração.\n",
        "        act_fn (callable): Função de ativação não-linear.\n",
        "\n",
        "    Args:\n",
        "        config: Um objeto de configuração contendo os parâmetros do modelo.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(hidden_size=768, intermediate_size=3072)\n",
        "        >>> mlp = LlamaMLP(config)\n",
        "        >>> input_tensor = torch.randn(1, 10, 768)  # [batch_size, seq_length, hidden_size]\n",
        "        >>> output = mlp(input_tensor)\n",
        "        >>> print(output.shape)\n",
        "        torch.Size([1, 10, 768])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "\n",
        "        # Projeção de gate: hidden_size -> intermediate_size\n",
        "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
        "\n",
        "        # Projeção up: hidden_size -> intermediate_size\n",
        "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
        "\n",
        "        # Projeção down: intermediate_size -> hidden_size\n",
        "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
        "\n",
        "        # Função de ativação (geralmente SiLU/Swish)\n",
        "        self.act_fn = nn.SiLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward da camada MLP.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Tensor de entrada com shape [batch_size, seq_length, hidden_size].\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor de saída com shape [batch_size, seq_length, hidden_size].\n",
        "\n",
        "        Raises:\n",
        "            ValueError: Se as dimensões do tensor de entrada não forem compatíveis.\n",
        "        \"\"\"\n",
        "        # Verificação das dimensões de entrada\n",
        "        if x.dim() != 3 or x.size(-1) != self.hidden_size:\n",
        "            raise ValueError(f\"Entrada esperada de shape [batch_size, seq_length, {self.hidden_size}], \"\n",
        "                             f\"mas recebeu {x.shape}\")\n",
        "\n",
        "        if self.config.pretraining_tp > 1:\n",
        "            # Implementação para tensor parallelism (TP)\n",
        "            slice = self.intermediate_size // self.config.pretraining_tp\n",
        "\n",
        "            # Divide os pesos das projeções em fatias\n",
        "            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n",
        "            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n",
        "            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n",
        "\n",
        "            # Aplica as projeções em paralelo\n",
        "            # Cada operação: [batch_size, seq_length, slice]\n",
        "            gate_proj = torch.cat(\n",
        "                [nn.functional.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)],\n",
        "                dim=-1\n",
        "            )\n",
        "            up_proj = torch.cat(\n",
        "                [nn.functional.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)],\n",
        "                dim=-1\n",
        "            )\n",
        "\n",
        "            # Aplica a função de ativação e multiplicação elemento a elemento\n",
        "            # Dimensão: [batch_size, seq_length, intermediate_size]\n",
        "            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n",
        "\n",
        "            # Aplica a projeção down em paralelo\n",
        "            # Cada operação: [batch_size, seq_length, hidden_size // pretraining_tp]\n",
        "            down_proj = [\n",
        "                nn.functional.linear(intermediate_states[i], down_proj_slices[i])\n",
        "                for i in range(self.config.pretraining_tp)\n",
        "            ]\n",
        "\n",
        "            # Soma os resultados das projeções down\n",
        "            # Dimensão final: [batch_size, seq_length, hidden_size]\n",
        "            down_proj = sum(down_proj)\n",
        "\n",
        "        else:\n",
        "            # Implementação padrão sem tensor parallelism\n",
        "\n",
        "            # Aplica as projeções gate e up\n",
        "            # Dimensões: [batch_size, seq_length, intermediate_size]\n",
        "            gate_proj = self.gate_proj(x)\n",
        "            up_proj = self.up_proj(x)\n",
        "\n",
        "            # Aplica a função de ativação no gate e multiplica pelo resultado de up\n",
        "            # Dimensão: [batch_size, seq_length, intermediate_size]\n",
        "            intermediate_states = self.act_fn(gate_proj) * up_proj\n",
        "\n",
        "            # Aplica a projeção down\n",
        "            # Dimensão final: [batch_size, seq_length, hidden_size]\n",
        "            down_proj = self.down_proj(intermediate_states)\n",
        "\n",
        "        return down_proj\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"LlamaMLP(hidden_size={self.hidden_size}, \"\n",
        "                f\"intermediate_size={self.intermediate_size}, \"\n",
        "                f\"act_fn={self.act_fn.__class__.__name__})\")"
      ],
      "metadata": {
        "id": "jf2uEPW8vBiO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA RMS Norm"
      ],
      "metadata": {
        "id": "4h29-LUbZul5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LlamaRMSNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    LlamaRMSNorm é uma variante de normalização de camada utilizada no modelo LLaMA.\n",
        "\n",
        "    Esta normalização usa a raiz quadrada da média dos quadrados (RMS) para normalizar\n",
        "    os inputs, em vez da média e variância usadas na normalização de camada padrão.\n",
        "\n",
        "    Atributos:\n",
        "        weight (nn.Parameter): Parâmetro aprendível para escala.\n",
        "        variance_epsilon (float): Pequeno valor adicionado ao denominador para estabilidade numérica.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): Dimensão do espaço oculto a ser normalizado.\n",
        "        eps (float, opcional): Epsilon para estabilidade numérica. Padrão é 1e-6.\n",
        "\n",
        "    Forma do Input:\n",
        "        - Input: (batch_size, seq_length, hidden_size)\n",
        "        - Output: (batch_size, seq_length, hidden_size)\n",
        "\n",
        "    Exemplo:\n",
        "        >>> rms_norm = LlamaRMSNorm(hidden_size=768, eps=1e-6)\n",
        "        >>> input_tensor = torch.randn(32, 50, 768)  # (batch_size, seq_length, hidden_size)\n",
        "        >>> normalized_tensor = rms_norm(input_tensor)\n",
        "        >>> print(normalized_tensor.shape)\n",
        "        torch.Size([32, 50, 768])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Aplica a normalização RMS ao tensor de entrada.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Tensor de entrada a ser normalizado.\n",
        "                Shape: (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Tensor normalizado.\n",
        "                Shape: (batch_size, seq_length, hidden_size)\n",
        "        \"\"\"\n",
        "        input_dtype = hidden_states.dtype\n",
        "        hidden_states = hidden_states.to(torch.float32)\n",
        "\n",
        "        # Calcula a variância (média dos quadrados)\n",
        "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
        "\n",
        "        # Normaliza usando RMS\n",
        "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
        "\n",
        "        # Aplica o peso aprendível\n",
        "        return (self.weight * hidden_states).to(input_dtype)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        \"\"\"\n",
        "        Retorna uma representação de string dos principais parâmetros.\n",
        "\n",
        "        Returns:\n",
        "            str: String representando os parâmetros do módulo.\n",
        "        \"\"\"\n",
        "        return f\"hidden_size={self.weight.numel()}, eps={self.variance_epsilon}\""
      ],
      "metadata": {
        "id": "y7nm-P9EU2Sb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Attention"
      ],
      "metadata": {
        "id": "p1247aOeZxZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vanilla Attention"
      ],
      "metadata": {
        "id": "olhbgD_QZ1Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class LlamaAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementa o mecanismo de atenção multi-cabeça do modelo LLaMA.\n",
        "\n",
        "    Esta classe realiza a operação de auto-atenção, permitindo que o modelo foque\n",
        "    em diferentes partes da sequência de entrada. Suporta diferentes implementações\n",
        "    de atenção e otimizações como grouped-query attention.\n",
        "\n",
        "    Atributos:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        layer_idx (int): Índice da camada atual.\n",
        "        hidden_size (int): Dimensão do espaço oculto.\n",
        "        num_heads (int): Número de cabeças de atenção.\n",
        "        head_dim (int): Dimensão de cada cabeça de atenção.\n",
        "        num_key_value_heads (int): Número de cabeças para key e value (pode ser menor que num_heads).\n",
        "        max_position_embeddings (int): Número máximo de posições para embeddings.\n",
        "        rotary_emb (LlamaRotaryEmbedding): Instância para aplicar embeddings rotacionais.\n",
        "\n",
        "    Args:\n",
        "        config: Um objeto de configuração contendo os parâmetros do modelo.\n",
        "        layer_idx (Optional[int]): Índice da camada. Necessário para algumas otimizações.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(hidden_size=512, num_attention_heads=8)\n",
        "        >>> attention = LlamaAttention(config, layer_idx=0)\n",
        "        >>> hidden_states = torch.randn(1, 10, 512)  # [batch_size, seq_length, hidden_size]\n",
        "        >>> attention_mask = torch.ones(1, 1, 10, 10)  # [batch_size, 1, seq_length, seq_length]\n",
        "        >>> output, _ = attention(hidden_states, attention_mask=attention_mask)\n",
        "        >>> print(output.shape)\n",
        "        torch.Size([1, 10, 512])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "        # Verifica se as dimensões são compatíveis\n",
        "        if self.head_dim * self.num_heads != self.hidden_size:\n",
        "            raise ValueError(f\"hidden_size deve ser divisível por num_heads. \"\n",
        "                             f\"Got {self.hidden_size} e {self.num_heads}.\")\n",
        "\n",
        "        # Projections para query, key, value e output\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_position_embeddings=self.max_position_embeddings,\n",
        "            base=config.rotary_emb_base,\n",
        "            rope_scaling=config.rope_scaling\n",
        "        )\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        \"\"\"Reshape and transpose tensor for attention computation.\"\"\"\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: bool = False,\n",
        "        use_cache: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do mecanismo de atenção.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Estados ocultos de entrada. Shape [batch_size, seq_length, hidden_size]\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção. Shape [batch_size, 1, tgt_seq_length, src_seq_length]\n",
        "            position_ids (Optional[torch.LongTensor]): IDs das posições. Shape [batch_size, seq_length]\n",
        "            past_key_value (Optional[Tuple[torch.Tensor]]): Cache de estados passados para geração autoregressiva.\n",
        "            output_attentions (bool): Se True, retorna os pesos de atenção.\n",
        "            use_cache (bool): Se True, retorna o cache para uso futuro.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "                - Estados ocultos atualizados\n",
        "                - Pesos de atenção (opcional)\n",
        "                - Novo cache de estados (opcional)\n",
        "        \"\"\"\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        # Calcula query, key, value\n",
        "        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, head_dim]\n",
        "\n",
        "        # Aplica RoPE (Rotary Position Embedding)\n",
        "        cos, sin = self.rotary_emb(query_states)\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "        # Lida com o cache de estados passados para geração autoregressiva\n",
        "        if past_key_value is not None:\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "\n",
        "        past_key_value = (key_states, value_states) if use_cache else None\n",
        "\n",
        "        # Repete key e value para cada grupo de query em grouped-query attention\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, head_dim]\n",
        "\n",
        "        # Calcula os scores de atenção\n",
        "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, seq_length]\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            attn_weights = attn_weights + attention_mask\n",
        "\n",
        "        # Normaliza os pesos de atenção\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "\n",
        "        # Calcula o output da atenção\n",
        "        attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "        # [batch_size, num_heads, seq_length, head_dim]\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
        "\n",
        "        # [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Projeção final\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        if not output_attentions:\n",
        "            attn_weights = None\n",
        "\n",
        "        return attn_output, attn_weights, past_key_value\n",
        "\n",
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Repete os estados de key e value para grouped-query attention.\n",
        "\n",
        "    Args:\n",
        "        hidden_states (torch.Tensor): Estados de entrada [batch, num_key_value_heads, seqlen, head_dim]\n",
        "        n_rep (int): Número de repetições\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Estados repetidos [batch, num_attention_heads, seqlen, head_dim]\n",
        "    \"\"\"\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    if n_rep == 1:\n",
        "        return hidden_states\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
      ],
      "metadata": {
        "id": "lSN7BH0Hv7VF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sdpa Attention"
      ],
      "metadata": {
        "id": "2ZKJw_elZ3s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple\n",
        "import warnings\n",
        "\n",
        "class LlamaSdpaAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementação otimizada do mecanismo de atenção do LLaMA usando\n",
        "    scaled_dot_product_attention (SDPA) do PyTorch.\n",
        "\n",
        "    Esta classe implementa a atenção multi-cabeça com suporte a\n",
        "    Rotary Position Embedding (RoPE) e atenção agrupada.\n",
        "\n",
        "    Atributos:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        layer_idx (int): Índice da camada atual.\n",
        "        hidden_size (int): Dimensão do espaço oculto.\n",
        "        num_heads (int): Número de cabeças de atenção.\n",
        "        head_dim (int): Dimensão de cada cabeça de atenção.\n",
        "        num_key_value_heads (int): Número de cabeças para key e value (pode ser menor que num_heads).\n",
        "        max_position_embeddings (int): Número máximo de posições para embeddings.\n",
        "        rotary_emb (LlamaRotaryEmbedding): Instância para aplicar embeddings rotacionais.\n",
        "\n",
        "    Args:\n",
        "        config: Configuração do modelo LLaMA.\n",
        "        layer_idx (Optional[int]): Índice da camada. Necessário para algumas otimizações.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
        "            raise ValueError(\n",
        "                f\"hidden_size deve ser divisível por num_heads. \"\n",
        "                f\"Got {self.hidden_size} e {self.num_heads}.\"\n",
        "            )\n",
        "\n",
        "        # Projeções lineares para Q, K, V e O\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_position_embeddings=self.max_position_embeddings,\n",
        "            base=config.rope_theta,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: bool = False,\n",
        "        use_cache: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do mecanismo de atenção.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Estados ocultos de entrada.\n",
        "                Shape: [batch_size, seq_length, hidden_size]\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção.\n",
        "                Shape: [batch_size, 1, tgt_seq_length, src_seq_length]\n",
        "            position_ids (Optional[torch.LongTensor]): IDs das posições.\n",
        "                Shape: [batch_size, seq_length]\n",
        "            past_key_value (Optional[Tuple[torch.Tensor]]): Cache de estados passados.\n",
        "            output_attentions (bool): Se True, retorna os pesos de atenção.\n",
        "            use_cache (bool): Se True, retorna o cache para uso futuro.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "                - Estados ocultos atualizados\n",
        "                - Pesos de atenção (opcional)\n",
        "                - Novo cache de estados (opcional)\n",
        "        \"\"\"\n",
        "        # Obtém as dimensões do tensor de entrada\n",
        "        # hidden_states shape: [batch_size, seq_length, hidden_size]\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        if self.config.pretraining_tp > 1:\n",
        "            # Implementação para tensor parallelism\n",
        "            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n",
        "            query_slices = self.q_proj.weight.split((self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0)\n",
        "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
        "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
        "\n",
        "            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            query_states = torch.cat(query_states, dim=-1)\n",
        "\n",
        "            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            key_states = torch.cat(key_states, dim=-1)\n",
        "\n",
        "            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n",
        "            value_states = torch.cat(value_states, dim=-1)\n",
        "\n",
        "        else:\n",
        "            # Projeções Q, K, V padrão\n",
        "            # Resultado: [batch_size, seq_length, num_heads * head_dim]\n",
        "            query_states = self.q_proj(hidden_states)\n",
        "            key_states = self.k_proj(hidden_states)\n",
        "            value_states = self.v_proj(hidden_states)\n",
        "\n",
        "        # Reshape e transpõe Q, K, V\n",
        "        # Resultado: [batch_size, num_heads, seq_length, head_dim]\n",
        "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calcula os embeddings rotacionais\n",
        "        # cos e sin: [1, seq_length, head_dim]\n",
        "        cos, sin = self.rotary_emb(value_states, seq_len=q_len)\n",
        "\n",
        "        # Aplica RoPE (Rotary Position Embedding) a Q e K\n",
        "        # query_states, key_states: [batch_size, num_heads, seq_length, head_dim]\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
        "\n",
        "        # Lida com o cache de estados passados para geração autoregressiva\n",
        "        if past_key_value is not None:\n",
        "            # Concatena estados passados com os atuais\n",
        "            # key_states, value_states: [batch_size, num_heads, seq_length + past_length, head_dim]\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "\n",
        "        # Prepara o cache para a próxima iteração se necessário\n",
        "        past_key_value = (key_states, value_states) if use_cache else None\n",
        "\n",
        "        # Repete K e V para atenção agrupada (grouped-query attention)\n",
        "        # key_states, value_states: [batch_size, num_heads, seq_length, head_dim]\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        # Aplica a atenção usando scaled_dot_product_attention\n",
        "        # attn_output: [batch_size, num_heads, seq_length, head_dim]\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            query_states, key_states, value_states,\n",
        "            attn_mask=attention_mask,\n",
        "            dropout_p=self.config.attention_dropout if self.training else 0.0,\n",
        "            is_causal=False\n",
        "        )\n",
        "\n",
        "        # Reorganiza o tensor de saída\n",
        "        # attn_output: [batch_size, seq_length, num_heads * head_dim]\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
        "\n",
        "        # Aplica a projeção de saída\n",
        "        # attn_output: [batch_size, seq_length, hidden_size]\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        if output_attentions:\n",
        "            warnings.warn(\"output_attentions=True não é suportado para SDPA no momento.\")\n",
        "            attn_weights = None\n",
        "        else:\n",
        "            attn_weights = None\n",
        "\n",
        "        return attn_output, attn_weights, past_key_value"
      ],
      "metadata": {
        "id": "LDQ8ZVGbXlNS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Flash Attention 2"
      ],
      "metadata": {
        "id": "toLzxx4MZ9-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash_attn --quiet"
      ],
      "metadata": {
        "id": "9swlCZ6WYbwA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, Tuple\n",
        "from flash_attn import flash_attn_func, flash_attn_varlen_func\n",
        "import warnings\n",
        "\n",
        "class LlamaFlashAttention2(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementação do mecanismo de atenção do LLaMA usando Flash Attention 2.\n",
        "    Esta versão é otimizada para eficiência em memória e velocidade.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_idx = layer_idx\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.num_key_value_heads = config.num_key_value_heads\n",
        "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "        self.max_position_embeddings = config.max_position_embeddings\n",
        "\n",
        "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
        "            raise ValueError(f\"hidden_size deve ser divisível por num_heads.\")\n",
        "\n",
        "        # Inicializa as projeções lineares para Q, K, V e O\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "\n",
        "        self.rotary_emb = LlamaRotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_position_embeddings=self.max_position_embeddings,\n",
        "            base=config.rope_theta,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: bool = False,\n",
        "        use_cache: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        # hidden_states shape: [batch_size, seq_length, hidden_size]\n",
        "        bsz, q_len, _ = hidden_states.size()\n",
        "\n",
        "        # Aplica as projeções lineares para Q, K, V\n",
        "        # Shapes: [batch_size, seq_length, num_heads * head_dim]\n",
        "        query_states = self.q_proj(hidden_states)\n",
        "        key_states = self.k_proj(hidden_states)\n",
        "        value_states = self.v_proj(hidden_states)\n",
        "\n",
        "        # Reshape e transpõe para [batch_size, num_heads, seq_length, head_dim]\n",
        "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Calcula os embeddings rotacionais\n",
        "        cos, sin = self.rotary_emb(value_states, seq_len=q_len)\n",
        "        # Aplica RoPE (Rotary Position Embedding)\n",
        "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
        "\n",
        "        # Lida com o cache de estados passados para geração autoregressiva\n",
        "        if past_key_value is not None:\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        past_key_value = (key_states, value_states) if use_cache else None\n",
        "\n",
        "        # Repete K e V para atenção agrupada (grouped-query attention)\n",
        "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "        # Prepara os tensores para Flash Attention\n",
        "        q, k, v = query_states, key_states, value_states\n",
        "\n",
        "        # Converte q, k, v para o formato esperado por Flash Attention\n",
        "        # [batch_size, seq_length, num_heads, head_dim]\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Aplica Flash Attention\n",
        "        if attention_mask is None:\n",
        "            # Usa a versão padrão do Flash Attention quando não há máscara\n",
        "            attn_output = flash_attn_func(q, k, v, dropout_p=self.config.attention_dropout if self.training else 0.0, causal=True)\n",
        "        else:\n",
        "            # Usa a versão com comprimento variável quando há máscara\n",
        "            attn_output, _ = flash_attn_varlen_func(\n",
        "                q, k, v,\n",
        "                cu_seqlens_q=attention_mask,\n",
        "                cu_seqlens_k=attention_mask,\n",
        "                max_seqlen_q=q_len,\n",
        "                max_seqlen_k=q_len,\n",
        "                dropout_p=self.config.attention_dropout if self.training else 0.0,\n",
        "                causal=True\n",
        "            )\n",
        "\n",
        "        # Reshape e aplica a projeção de saída\n",
        "        # [batch_size, seq_length, hidden_size]\n",
        "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
        "        attn_output = self.o_proj(attn_output)\n",
        "\n",
        "        if output_attentions:\n",
        "            warnings.warn(\"output_attentions=True não é suportado para Flash Attention.\")\n",
        "            attn_weights = None\n",
        "        else:\n",
        "            attn_weights = None\n",
        "\n",
        "        return attn_output, attn_weights, past_key_value"
      ],
      "metadata": {
        "id": "v3AqKNbBYCuj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Decoder Layer"
      ],
      "metadata": {
        "id": "eDCXFvglaDFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class LlamaDecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementa uma camada do decodificador do modelo LLaMA.\n",
        "\n",
        "    Esta classe combina os mecanismos de atenção e feed-forward network (MLP),\n",
        "    formando um bloco completo do transformer decodificador. Inclui normalizações\n",
        "    de camada e conexões residuais.\n",
        "\n",
        "    Atributos:\n",
        "        hidden_size (int): Dimensão do espaço oculto.\n",
        "        self_attn (LlamaAttention): Mecanismo de auto-atenção.\n",
        "        mlp (LlamaMLP): Rede feed-forward.\n",
        "        input_layernorm (LlamaRMSNorm): Normalização de camada para entrada.\n",
        "        post_attention_layernorm (LlamaRMSNorm): Normalização após a atenção.\n",
        "\n",
        "    Args:\n",
        "        config (LlamaConfig): Configuração do modelo LLaMA.\n",
        "        layer_idx (int): Índice da camada atual.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> config = LlamaConfig(hidden_size=512, intermediate_size=2048, num_attention_heads=8)\n",
        "        >>> layer = LlamaDecoderLayer(config, layer_idx=0)\n",
        "        >>> hidden_states = torch.randn(1, 10, 512)  # [batch_size, seq_length, hidden_size]\n",
        "        >>> attention_mask = torch.ones(1, 1, 10, 10)  # [batch_size, 1, seq_length, seq_length]\n",
        "        >>> outputs = layer(hidden_states, attention_mask=attention_mask)\n",
        "        >>> print(outputs[0].shape)\n",
        "        torch.Size([1, 10, 512])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: LlamaConfig):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.self_attn = LlamaAttention(config=config)\n",
        "        self.mlp = LlamaMLP(config)\n",
        "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward de uma camada do decodificador.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (torch.Tensor): Estados ocultos de entrada.\n",
        "                Shape [batch_size, seq_length, hidden_size]\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção.\n",
        "                Shape [batch_size, 1, tgt_seq_length, src_seq_length]\n",
        "            position_ids (Optional[torch.LongTensor]): IDs das posições.\n",
        "                Shape [batch_size, seq_length]\n",
        "            past_key_value (Optional[Tuple[torch.Tensor]]): Cache de estados passados para\n",
        "                geração autoregressiva.\n",
        "            output_attentions (Optional[bool]): Se True, retorna os pesos de atenção.\n",
        "            use_cache (Optional[bool]): Se True, retorna o cache para uso futuro.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "                - Estados ocultos atualizados\n",
        "                - Tupla contendo os novos cache de estados (se use_cache=True)\n",
        "        \"\"\"\n",
        "        # Shape de hidden_states: [batch_size, seq_length, hidden_size]\n",
        "        residual = hidden_states\n",
        "\n",
        "        # Normalização de camada na entrada\n",
        "        hidden_states = self.input_layernorm(hidden_states)\n",
        "        # Shape após normalização: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Self Attention\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_value=past_key_value,\n",
        "            output_attentions=output_attentions,\n",
        "            use_cache=use_cache,\n",
        "        )\n",
        "        # Shape após atenção: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Conexão residual após a atenção\n",
        "        hidden_states = residual + hidden_states\n",
        "        # Shape após conexão residual: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Normalização de camada após a atenção\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
        "        # Shape após normalização: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # MLP (Feed-Forward Network)\n",
        "        hidden_states = self.mlp(hidden_states)\n",
        "        # Shape após MLP: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        # Conexão residual após o MLP\n",
        "        hidden_states = residual + hidden_states\n",
        "        # Shape final: [batch_size, seq_length, hidden_size]\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights,)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"LlamaDecoderLayer(hidden_size={self.hidden_size})\""
      ],
      "metadata": {
        "id": "PQ5ZrqsbxQie"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Pre-Trained Model"
      ],
      "metadata": {
        "id": "-IK_BO3TaHfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedModel\n",
        "from transformers.modeling_utils import PretrainedConfig\n",
        "from typing import Union, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LlamaPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Classe base abstrata para modelos pré-treinados LLaMA.\n",
        "\n",
        "    Esta classe herda de `PreTrainedModel` e implementa funcionalidades específicas\n",
        "    para modelos LLaMA, incluindo inicialização de pesos e configurações de otimização.\n",
        "\n",
        "    Atributos:\n",
        "        config_class (Type[PretrainedConfig]): Classe de configuração para modelos LLaMA.\n",
        "        base_model_prefix (str): Prefixo usado para nomear o modelo base.\n",
        "        supports_gradient_checkpointing (bool): Indica suporte a checkpointing de gradiente.\n",
        "        _no_split_modules (List[str]): Lista de módulos que não devem ser divididos durante\n",
        "                                       o processamento paralelo.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> from transformers import LlamaConfig\n",
        "        >>> class MyLlamaModel(LlamaPreTrainedModel):\n",
        "        ...     def __init__(self, config):\n",
        "        ...         super().__init__(config)\n",
        "        ...         # Implementação do modelo\n",
        "        ...\n",
        "        >>> config = LlamaConfig()\n",
        "        >>> model = MyLlamaModel(config)\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = LlamaConfig\n",
        "    base_model_prefix = \"model\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _no_split_modules = [\"LlamaDecoderLayer\"]\n",
        "    _skip_keys_device_placement = [\"past_key_values\"]\n",
        "    _supports_flash_attn_2 = True\n",
        "    _supports_sdpa = True\n",
        "    _supports_cache_class = True\n",
        "    _supports_quantized_cache = True\n",
        "    _supports_static_cache = True\n",
        "\n",
        "    def __init__(self, config: LlamaConfig, *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "\n",
        "    def _init_weights(self, module: nn.Module):\n",
        "        \"\"\"\n",
        "        Inicializa os pesos do módulo.\n",
        "\n",
        "        Esta função é chamada para cada submódulo durante a inicialização do modelo.\n",
        "        Implementa a estratégia de inicialização de pesos específica para modelos LLaMA.\n",
        "\n",
        "        Args:\n",
        "            module (nn.Module): O módulo cujos pesos serão inicializados.\n",
        "        \"\"\"\n",
        "        std = self.config.initializer_range\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module: nn.Module, value: bool = False):\n",
        "        \"\"\"\n",
        "        Configura o checkpointing de gradiente para o módulo.\n",
        "\n",
        "        O checkpointing de gradiente pode ser usado para economizar memória durante o treinamento,\n",
        "        recalculando os gradientes durante a passagem backward em vez de armazená-los.\n",
        "\n",
        "        Args:\n",
        "            module (nn.Module): O módulo para configurar o checkpointing.\n",
        "            value (bool): Se True, ativa o checkpointing de gradiente.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (LlamaDecoderLayer, LlamaModel)):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "    def gradient_checkpointing_enable(self):\n",
        "        \"\"\"\n",
        "        Ativa o checkpointing de gradiente para todo o modelo.\n",
        "        \"\"\"\n",
        "        self.apply(lambda module: self._set_gradient_checkpointing(module, value=True))\n",
        "\n",
        "    def gradient_checkpointing_disable(self):\n",
        "        \"\"\"\n",
        "        Desativa o checkpointing de gradiente para todo o modelo.\n",
        "        \"\"\"\n",
        "        self.apply(lambda module: self._set_gradient_checkpointing(module, value=False))\n",
        "\n",
        "    def enable_input_require_grads(self):\n",
        "        \"\"\"\n",
        "        Configura o modelo para permitir gradientes nos inputs.\n",
        "\n",
        "        Isso é necessário para técnicas como adversarial training.\n",
        "        \"\"\"\n",
        "        def make_inputs_require_grads(module, input, output):\n",
        "            output.requires_grad_(True)\n",
        "\n",
        "        self._require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n",
        "\n",
        "    def disable_input_require_grads(self):\n",
        "        \"\"\"\n",
        "        Remove a configuração que permite gradientes nos inputs.\n",
        "        \"\"\"\n",
        "        self._require_grads_hook.remove()\n",
        "\n",
        "    def get_position_embeddings(self) -> Optional[Union[nn.Embedding, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Retorna as embeddings de posição do modelo, se existirem.\n",
        "\n",
        "        Returns:\n",
        "            Optional[Union[nn.Embedding, torch.Tensor]]: As embeddings de posição ou None.\n",
        "        \"\"\"\n",
        "        if hasattr(self, \"rotary_emb\"):\n",
        "            return self.rotary_emb\n",
        "        return None\n",
        "\n",
        "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n",
        "        \"\"\"\n",
        "        Redimensiona as embeddings de posição do modelo.\n",
        "\n",
        "        Args:\n",
        "            new_num_position_embeddings (int): O novo número de posições.\n",
        "\n",
        "        Raises:\n",
        "            NotImplementedError: Esta funcionalidade não está implementada para modelos LLaMA.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\n",
        "            f\"{self.__class__.__name__} não suporta o redimensionamento das embeddings de posição.\"\n",
        "        )\n",
        "\n",
        "    def get_output_embeddings(self) -> Optional[nn.Module]:\n",
        "        \"\"\"\n",
        "        Retorna as embeddings de saída do modelo, se existirem.\n",
        "\n",
        "        Returns:\n",
        "            Optional[nn.Module]: As embeddings de saída ou None.\n",
        "        \"\"\"\n",
        "        return None  # LLaMA não usa embeddings de saída por padrão\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings: Optional[nn.Module]):\n",
        "        \"\"\"\n",
        "        Define novas embeddings de saída para o modelo.\n",
        "\n",
        "        Args:\n",
        "            new_embeddings (Optional[nn.Module]): As novas embeddings de saída.\n",
        "\n",
        "        Raises:\n",
        "            NotImplementedError: Esta funcionalidade não está implementada para modelos LLaMA.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\n",
        "            f\"{self.__class__.__name__} não suporta a mudança das embeddings de saída.\"\n",
        "        )"
      ],
      "metadata": {
        "id": "EcLz7dF_xjtp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Model"
      ],
      "metadata": {
        "id": "oWIpC2lxaLN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Optional, Tuple, Union, List\n",
        "from transformers.modeling_outputs import BaseModelOutputWithPast\n",
        "\n",
        "class LlamaModel(LlamaPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Modelo base LLaMA.\n",
        "\n",
        "    Esta classe implementa a estrutura principal do modelo LLaMA, incluindo\n",
        "    as camadas de embedding, as camadas do decodificador e a normalização final.\n",
        "\n",
        "    Atributos:\n",
        "        config (LlamaConfig): Configuração do modelo.\n",
        "        padding_idx (int): Índice do token de padding.\n",
        "        vocab_size (int): Tamanho do vocabulário.\n",
        "        embed_tokens (nn.Embedding): Camada de embedding para tokens.\n",
        "        layers (nn.ModuleList): Lista de camadas do decodificador.\n",
        "        norm (LlamaRMSNorm): Camada de normalização final.\n",
        "        gradient_checkpointing (bool): Se o checkpointing de gradiente está ativado.\n",
        "\n",
        "    Args:\n",
        "        config (LlamaConfig): Configuração do modelo LLaMA.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> from transformers import LlamaConfig\n",
        "        >>> config = LlamaConfig()\n",
        "        >>> model = LlamaModel(config)\n",
        "        >>> input_ids = torch.randint(0, config.vocab_size, (1, 10))\n",
        "        >>> outputs = model(input_ids)\n",
        "        >>> last_hidden_states = outputs.last_hidden_state\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: LlamaConfig):\n",
        "        super().__init__(config)\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
        "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _i1 in range(config.num_hidden_layers)])\n",
        "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "        self.gradient_checkpointing = False\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do modelo LLaMA.\n",
        "\n",
        "        Args:\n",
        "            input_ids (Optional[torch.LongTensor]): Tensor de IDs de tokens de entrada.\n",
        "                Shape: (batch_size, sequence_length)\n",
        "            attention_mask (Optional[torch.Tensor]): Máscara de atenção para os tokens de entrada.\n",
        "                Shape: (batch_size, sequence_length)\n",
        "            position_ids (Optional[torch.LongTensor]): IDs de posição para os tokens de entrada.\n",
        "                Shape: (batch_size, sequence_length)\n",
        "            past_key_values (Optional[List[torch.FloatTensor]]): Lista de tensores contendo estados passados\n",
        "                para uso em geração incremental.\n",
        "            inputs_embeds (Optional[torch.FloatTensor]): Embeddings pré-computados para substituir input_ids.\n",
        "                Shape: (batch_size, sequence_length, hidden_size)\n",
        "            use_cache (Optional[bool]): Se deve retornar um cache para geração incremental.\n",
        "            output_attentions (Optional[bool]): Se deve retornar todas as atenções.\n",
        "            output_hidden_states (Optional[bool]): Se deve retornar todos os estados ocultos.\n",
        "            return_dict (Optional[bool]): Se deve retornar um dicionário ao invés de uma tupla.\n",
        "\n",
        "        Returns:\n",
        "            Union[Tuple, BaseModelOutputWithPast]: Saída do modelo, incluindo últimos estados ocultos,\n",
        "                past_key_values (se use_cache=True), e opcionalmente todos os estados ocultos e atenções.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # Recuperar embeddings de entrada\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"Você não pode especificar tanto input_ids quanto inputs_embeds ao mesmo tempo\")\n",
        "        elif input_ids is not None:\n",
        "            # input_ids shape: (batch_size, sequence_length)\n",
        "            batch_size, seq_length = input_ids.shape\n",
        "            inputs_embeds = self.embed_tokens(input_ids)\n",
        "            # inputs_embeds shape: (batch_size, sequence_length, hidden_size)\n",
        "        elif inputs_embeds is not None:\n",
        "            batch_size, seq_length, _ = inputs_embeds.shape\n",
        "        else:\n",
        "            raise ValueError(\"Você deve especificar ou input_ids ou inputs_embeds\")\n",
        "\n",
        "        # Gerar position_ids se não fornecidos\n",
        "        if position_ids is None:\n",
        "            # position_ids shape: (batch_size, sequence_length)\n",
        "            position_ids = torch.arange(seq_length, dtype=torch.long, device=inputs_embeds.device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Gerar máscara de atenção se não fornecida\n",
        "        if attention_mask is None:\n",
        "            # attention_mask shape: (batch_size, sequence_length)\n",
        "            attention_mask = torch.ones((batch_size, seq_length), device=inputs_embeds.device)\n",
        "\n",
        "        # Converter máscara de atenção para o formato correto (expandido para todas as cabeças)\n",
        "        # extended_attention_mask shape: (batch_size, 1, 1, sequence_length)\n",
        "        extended_attention_mask = self.get_extended_attention_mask(attention_mask, (batch_size, seq_length))\n",
        "\n",
        "        # hidden_states shape: (batch_size, sequence_length, hidden_size)\n",
        "        hidden_states = inputs_embeds\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "\n",
        "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "                layer_outputs = self._gradient_checkpointing_func(\n",
        "                    decoder_layer.__call__,\n",
        "                    hidden_states,\n",
        "                    extended_attention_mask,\n",
        "                    position_ids,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                    use_cache,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=extended_attention_mask,\n",
        "                    position_ids=position_ids,\n",
        "                    past_key_value=past_key_value,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                )\n",
        "\n",
        "            # layer_outputs[0] shape: (batch_size, sequence_length, hidden_size)\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                # next_decoder_cache shape (para cada camada):\n",
        "                # (2, batch_size, num_heads, sequence_length, head_dim)\n",
        "                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                # all_self_attentions shape (para cada camada):\n",
        "                # (batch_size, num_heads, sequence_length, sequence_length)\n",
        "                all_self_attentions += (layer_outputs[1],)\n",
        "\n",
        "        # Normalização final\n",
        "        # hidden_states shape: (batch_size, sequence_length, hidden_size)\n",
        "        hidden_states = self.norm(hidden_states)\n",
        "\n",
        "        # Adicionar últimos estados ocultos\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        next_cache = next_decoder_cache if use_cache else None\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attentions] if v is not None)\n",
        "\n",
        "        return BaseModelOutputWithPast(\n",
        "            last_hidden_state=hidden_states,  # (batch_size, sequence_length, hidden_size)\n",
        "            past_key_values=next_cache,       # Lista de tensores, cada um com shape:\n",
        "                                              # (2, batch_size, num_heads, sequence_length, head_dim)\n",
        "            hidden_states=all_hidden_states,  # Tupla de tensores, cada um com shape:\n",
        "                                              # (batch_size, sequence_length, hidden_size)\n",
        "            attentions=all_self_attentions,   # Tupla de tensores, cada um com shape:\n",
        "                                              # (batch_size, num_heads, sequence_length, sequence_length)\n",
        "        )"
      ],
      "metadata": {
        "id": "2IUNbEY6208o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLaMA Implmentations"
      ],
      "metadata": {
        "id": "600DUO0saOKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Causal Language Modeling"
      ],
      "metadata": {
        "id": "QerDXIzeaUHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from typing import Optional, Tuple, Union, List\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "\n",
        "class LlamaForCausalLM(LlamaPreTrainedModel):\n",
        "    \"\"\"\n",
        "    Modelo LLaMA para Modelagem de Linguagem Causal.\n",
        "\n",
        "    Esta classe implementa o modelo LLaMA específico para tarefas de geração de texto,\n",
        "    adicionando uma camada de saída linear (lm_head) ao modelo base LLaMA.\n",
        "\n",
        "    Atributos:\n",
        "        model (LlamaModel): O modelo base LLaMA.\n",
        "        lm_head (nn.Linear): Camada linear para projetar estados ocultos no espaço do vocabulário.\n",
        "        vocab_size (int): Tamanho do vocabulário do modelo.\n",
        "\n",
        "    Args:\n",
        "        config (LlamaConfig): Configuração do modelo LLaMA.\n",
        "\n",
        "    Exemplo:\n",
        "        >>> from transformers import LlamaConfig, LlamaTokenizer\n",
        "        >>> config = LlamaConfig()\n",
        "        >>> model = LlamaForCausalLM(config)\n",
        "        >>> tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "        >>> inputs = tokenizer(\"Olá, como vai?\", return_tensors=\"pt\")\n",
        "        >>> outputs = model(**inputs)\n",
        "        >>> logits = outputs.logits\n",
        "    \"\"\"\n",
        "\n",
        "    _tied_weights_keys = [\"lm_head.weight\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.model = LlamaModel(config)\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Inicializa pesos e aplica processamento final\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.model.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.model.embed_tokens = value\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def set_decoder(self, decoder):\n",
        "        self.model = decoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "        \"\"\"\n",
        "        Realiza a passagem forward do modelo.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.LongTensor): IDs dos tokens de entrada.\n",
        "            attention_mask (torch.Tensor, opcional): Máscara de atenção.\n",
        "            position_ids (torch.LongTensor, opcional): IDs das posições.\n",
        "            past_key_values (List[torch.FloatTensor], opcional): Valores passados para uso em geração incremental.\n",
        "            inputs_embeds (torch.FloatTensor, opcional): Embeddings de entrada pré-computados.\n",
        "            labels (torch.LongTensor, opcional): Rótulos para cálculo de perda.\n",
        "            use_cache (bool, opcional): Se deve usar cache para geração incremental.\n",
        "            output_attentions (bool, opcional): Se deve retornar todas as atenções.\n",
        "            output_hidden_states (bool, opcional): Se deve retornar todos os estados ocultos.\n",
        "            return_dict (bool, opcional): Se deve retornar um dicionário ou tupla.\n",
        "\n",
        "        Returns:\n",
        "            Union[Tuple, CausalLMOutputWithPast]: Saída do modelo, incluindo logits, loss (se labels fornecidos),\n",
        "                                                  past_key_values (se use_cache=True), e opcionalmente\n",
        "                                                  hidden_states e attentions.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            position_ids=position_ids,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "        logits = self.lm_head(hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
        "            shift_labels = shift_labels.view(-1)\n",
        "            # Enable model parallelism\n",
        "            shift_labels = shift_labels.to(shift_logits.device)\n",
        "            loss = loss_fct(shift_logits, shift_labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithPast(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids, max_length=30, temperature=1.0, do_sample=False, **kwargs):\n",
        "        \"\"\"\n",
        "        Generates text by iteratively predicting the next token.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.LongTensor): Input token IDs.\n",
        "            max_length (int): Maximum length of the generated sequence.\n",
        "            temperature (float): Sampling temperature.\n",
        "            do_sample (bool): If True, sample next token; else, use greedy decoding.\n",
        "\n",
        "        Returns:\n",
        "            torch.LongTensor: Generated token IDs.\n",
        "        \"\"\"\n",
        "        generated = input_ids\n",
        "        past_key_values = None\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            outputs = self.forward(\n",
        "                input_ids=generated[:, -1:],\n",
        "                past_key_values=past_key_values,\n",
        "                attention_mask=None,\n",
        "                use_cache=True,\n",
        "                **kwargs\n",
        "            )\n",
        "            next_token_logits = outputs.logits[:, -1, :] / temperature\n",
        "\n",
        "            if do_sample:\n",
        "                # Sample from the distribution\n",
        "                probabilities = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "            else:\n",
        "                # Greedy decoding\n",
        "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "            generated = torch.cat([generated, next_token], dim=-1)\n",
        "            past_key_values = outputs.past_key_values\n",
        "\n",
        "            # Optional: Stop if EOS token is generated\n",
        "            if next_token.item() == self.config.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return generated\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n",
        "    ):\n",
        "        if past_key_values:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
        "        if inputs_embeds is not None and past_key_values is None:\n",
        "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
        "        else:\n",
        "            model_inputs = {\"input_ids\": input_ids}\n",
        "\n",
        "        model_inputs.update(\n",
        "            {\n",
        "                \"past_key_values\": past_key_values,\n",
        "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
        "                \"attention_mask\": attention_mask,\n",
        "                \"position_ids\": kwargs.get(\"position_ids\"),\n",
        "            }\n",
        "        )\n",
        "        return model_inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past_key_values, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past_key_values:\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
        "            )\n",
        "        return reordered_past"
      ],
      "metadata": {
        "id": "ZQX07y71ye30"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReCOGS Dataset and Utilities\n",
        "\n"
      ],
      "metadata": {
        "id": "k3E3eexBAvmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # This library is our indicator that the required installs\n",
        "    # need to be done.\n",
        "    import datasets\n",
        "except ModuleNotFoundError:\n",
        "    !git clone https://github.com/cgpotts/cs224u/\n",
        "    !pip install -r cs224u/requirements.txt\n",
        "    import sys\n",
        "    sys.path.append(\"cs224u\")"
      ],
      "metadata": {
        "id": "PFkYjxbVjiZN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wget\n",
        "\n",
        "SRC_DIRNAME = os.path.join(\"data\", \"recogs\")\n",
        "\n",
        "if not os.path.exists(SRC_DIRNAME):\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    wget.download('https://web.stanford.edu/class/cs224u/data/recogs.tgz', out='data/')\n",
        "    !tar xvf data/recogs.tgz -C data/"
      ],
      "metadata": {
        "id": "NPzp-BXcg0IP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_split(filename):\n",
        "    return pd.read_csv(\n",
        "        filename,\n",
        "        delimiter=\"\\t\",\n",
        "        names=['input', 'output', 'category'])"
      ],
      "metadata": {
        "id": "aCI3NS17g1fu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = {}\n",
        "\n",
        "for splitname in (\"train\", \"dev\", \"gen\"):\n",
        "    dataset[splitname] = load_split(f\"{SRC_DIRNAME}/{splitname}.tsv\")"
      ],
      "metadata": {
        "id": "UVqa6-mOhAHW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset['train'] = Dataset.from_pandas(dataset['train'])\n",
        "dataset['dev'] = Dataset.from_pandas(dataset['dev'].head(64))\n",
        "dataset['gen'] = Dataset.from_pandas(dataset['gen'])"
      ],
      "metadata": {
        "id": "2r9E-Lhb6X-j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruction(sample, include_response=True):\n",
        "    \"\"\"\n",
        "    Formats a single sample for instruction fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        sample (dict): A dictionary with 'input' and optionally 'output'.\n",
        "        include_response (bool): Whether to include the response in the formatted string.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted instruction string.\n",
        "    \"\"\"\n",
        "    if include_response and 'output' in sample:\n",
        "        return (\n",
        "            f\"### Instruction:\\nTranslate English sentences into logical form.\\n\\n\"\n",
        "            f\"### Input:\\n{sample['input']}\\n\\n\"\n",
        "            f\"### Response:\\n{sample['output']}\\n\"\n",
        "        )\n",
        "    else:\n",
        "        return (\n",
        "            f\"### Instruction:\\nTranslate English sentences into logical form.\\n\\n\"\n",
        "            f\"### Input:\\n{sample['input']}\\n\\n\"\n",
        "            f\"### Response:\\n\"\n",
        "        )"
      ],
      "metadata": {
        "id": "YsgsxQ2ChC4X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funções auxiliares para recogs_exact_match\n",
        "from collections import defaultdict\n",
        "from itertools import product\n",
        "import re\n",
        "\n",
        "def recogs_exact_match(gold, pred, flag=\"000000\"):\n",
        "    gold = normalize_formula(gold)\n",
        "    pred = normalize_formula(pred)\n",
        "    gold_conj_set = get_conj_set(gold)\n",
        "    # Loop over all viable mappings from pred_vars to gold_vars:\n",
        "    for this_map in _candidate_variable_maps(gold, pred):\n",
        "        phi = pred\n",
        "        for sourcevar, targetvar in this_map.items():\n",
        "            # The flag makes sure we don't accidentally do a chain\n",
        "            # of replacements via successive changes in situations\n",
        "            # where the domain and range of `this_map` share vars.\n",
        "            phi = variable_change(phi, sourcevar, targetvar, flag=flag)\n",
        "        phi = phi.replace(flag, \"\")\n",
        "        phi_conj_set = get_conj_set(phi)\n",
        "        # This step assumes that we have no conjuncts that are\n",
        "        # tautologies, contradictions, or equality predications. If\n",
        "        # such are introduced, they need to be identified ahead of\n",
        "        # time and treated separately -- tautologies would be removed,\n",
        "        # contradictions would reduce to comparisons of only those\n",
        "        # conjuncts, and equality statements would call for special\n",
        "        # handling related to variables mapping.\n",
        "        if phi_conj_set == gold_conj_set:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def normalize_formula(phi):\n",
        "    return phi.replace(\" \", \"\").replace(\"AND\" , \" AND \")\n",
        "\n",
        "binary_pred_re = re.compile(r\"\"\"\n",
        "    (\\w+)\n",
        "    \\s*\n",
        "    \\(\n",
        "    \\s*\n",
        "    (\\d+)\n",
        "    \\s*\n",
        "    ,\n",
        "    \\s*\n",
        "    (\\d+)\n",
        "    \\s*\n",
        "    \\)\"\"\", re.VERBOSE)\n",
        "\n",
        "unary_pred_re = re.compile(r\"\"\"\n",
        "    (\\w+)\n",
        "    \\s*\n",
        "    \\(\n",
        "    \\s*\n",
        "    (\\d+)\n",
        "    \\s*\n",
        "    \\)\"\"\", re.VERBOSE)\n",
        "\n",
        "def _candidate_variable_maps(gold, pred):\n",
        "    # This creates a mapping from tuples of predicates into their\n",
        "    # associated variables. These serve as equivalence classes over\n",
        "    # variables that could possibly be translations of each other.\n",
        "    gold_map = _map_get_preds_to_vars(gold)\n",
        "    pred_map = _map_get_preds_to_vars(pred)\n",
        "\n",
        "    # For each prediction variable, get the set of potential\n",
        "    # translations for it:\n",
        "    pred2gold = defaultdict(list)\n",
        "    for preds, pvars in pred_map.items():\n",
        "        gvars = gold_map[preds]\n",
        "        for pvar in pvars:\n",
        "            pred2gold[pvar] = gold_map[preds]\n",
        "\n",
        "    # Variable sets:\n",
        "    gold_vars = set(get_variables(gold))\n",
        "    pred_vars = set(get_variables(pred))\n",
        "\n",
        "    # Now generate potentially viable mappings:\n",
        "    for vals in list(product(*list(pred2gold.values()))):\n",
        "        d = dict(zip(pred2gold.keys(), vals))\n",
        "        if set(d.keys()) == pred_vars and set(d.values()) == gold_vars:\n",
        "            yield d\n",
        "\n",
        "def _map_get_preds_to_vars(phi):\n",
        "    var2pred = defaultdict(list)\n",
        "    for pred, var in unary_pred_re.findall(phi):\n",
        "        var2pred[var].append(pred)\n",
        "    # We could do somewhat less search by specializing to first and\n",
        "    # second position for these predicates, but I think it's fine\n",
        "    # as-is.\n",
        "    for pred, var1, var2 in binary_pred_re.findall(phi):\n",
        "        var2pred[var1].append(pred)\n",
        "        var2pred[var2].append(pred)\n",
        "    pred2var = defaultdict(list)\n",
        "    for var, preds in var2pred.items():\n",
        "        pred2var[tuple(sorted(preds))].append(var)\n",
        "    return pred2var\n",
        "\n",
        "def get_variables(phi):\n",
        "    variable_re = re.compile(r\"(\\d+)\")\n",
        "    return variable_re.findall(phi)\n",
        "\n",
        "def get_conj_set(phi):\n",
        "    conj_splitter_re  = re.compile(r\"\\s*(?:AND|;)\\s*\")\n",
        "    return set(conj_splitter_re.split(phi))\n",
        "\n",
        "def variable_change(phi, sourcevar, targetvar, flag=\"000000\"):\n",
        "    replace_re = re.compile(rf\"\\b{sourcevar}\\b\")\n",
        "    return replace_re.sub(f\"{flag}{targetvar}\", phi)\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    predictions, labels = eval_preds\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    exact_matches = []\n",
        "    for pred, label in zip(decoded_preds, decoded_labels):\n",
        "        # Extrair a resposta gerada pelo modelo\n",
        "        pred_response = pred.split(\"### Response:\")[-1].strip()\n",
        "        # Extrair a resposta de referência\n",
        "        label_response = label.split(\"### Response:\")[-1].strip()\n",
        "        # Calcular o match exato usando recogs_exact_match\n",
        "        match = recogs_exact_match(label_response, pred_response)\n",
        "        exact_matches.append(int(match))\n",
        "\n",
        "    accuracy = sum(exact_matches) / len(exact_matches)\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "def compute_metrics(tokenizer, eval_preds):\n",
        "    predictions, labels = eval_preds\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    exact_matches = []\n",
        "    for pred, label in zip(decoded_preds, decoded_labels):\n",
        "        pred_response = pred.split(\"### Response:\")[-1].strip()\n",
        "        label_response = label.split(\"### Response:\")[-1].strip()\n",
        "        match = recogs_exact_match(label_response, pred_response)\n",
        "        exact_matches.append(int(match))\n",
        "\n",
        "    accuracy = sum(exact_matches) / len(exact_matches) if exact_matches else 0\n",
        "    return {\"accuracy\": accuracy}\n"
      ],
      "metadata": {
        "id": "fuHEuQFqLpHC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "74d12LiJal2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U peft trl bitsandbytes safetensors accelerate wandb --quiet"
      ],
      "metadata": {
        "id": "6kPQRsR7iIgG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128"
      ],
      "metadata": {
        "id": "O4y4qUBm5xs4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from huggingface_hub import notebook_login\n",
        "#notebook_login()"
      ],
      "metadata": {
        "id": "6iyvJX3GfOfk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "import random\n",
        "\n",
        "def collate_fn(batch, tokenizer, max_length):\n",
        "    \"\"\"\n",
        "    Collate function to prepare inputs and labels.\n",
        "\n",
        "    Args:\n",
        "        batch (list): List of samples from the dataset.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer instance.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing input_ids, attention_mask, and labels.\n",
        "    \"\"\"\n",
        "    formatted_batch = [format_instruction(item, include_response=True) for item in batch]\n",
        "    encoded = tokenizer.batch_encode_plus(\n",
        "        formatted_batch,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = encoded[\"input_ids\"]\n",
        "    attention_mask = encoded[\"attention_mask\"]\n",
        "    labels = input_ids.clone()\n",
        "\n",
        "    # Tokenize the response prefix to identify where to start computing loss\n",
        "    response_prefix = \"### Response:\\n\"\n",
        "    response_ids = tokenizer.encode(response_prefix, add_special_tokens=False)\n",
        "    response_len = len(response_ids)\n",
        "\n",
        "    for i in range(input_ids.size(0)):\n",
        "        # Find the start index of the response\n",
        "        response_start = (input_ids[i] == response_ids[0]).nonzero(as_tuple=True)[0]\n",
        "        if len(response_start) > 0:\n",
        "            response_start = response_start[0].item()\n",
        "            # Verify that the response_ids match\n",
        "            if input_ids[i, response_start:response_start + response_len].tolist() == response_ids:\n",
        "                # Mask all tokens before the response\n",
        "                labels[i, :response_start + response_len] = -100\n",
        "        else:\n",
        "            # If response prefix not found, mask all tokens\n",
        "            labels[i] = -100\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments:\n",
        "    model_name: str\n",
        "    output_dir: str\n",
        "    num_epochs: int\n",
        "    batch_size: int\n",
        "    learning_rate: float\n",
        "    max_grad_norm: float\n",
        "    warmup_ratio: float\n",
        "    eval_steps: int\n",
        "    save_steps: int\n",
        "    log_steps: int\n",
        "    max_seq_length: int = 512\n",
        "    gradient_accumulation_steps: int = 1\n",
        "    fp16: bool = False\n",
        "    bf16: bool = True\n",
        "    optim: str = \"adamw_torch\"\n",
        "    lr_scheduler_type: str = \"linear\"\n",
        "    weight_decay: float = 0.01\n",
        "    push_to_hub: bool = False\n",
        "    hub_model_id: Optional[str] = None\n",
        "\n",
        "    lora_rank: int = 64\n",
        "    lora_alpha: int = 64\n",
        "    lora_dropout: float = 0.1\n",
        "    lora_bias: str = \"none\"\n",
        "\n",
        "def train(\n",
        "    args: TrainingArguments,\n",
        "    dataset,\n",
        "    compute_metrics,\n",
        "    format_instruction\n",
        "):\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=f\"{args.output_dir}-fine-tuning\", name=args.output_dir)\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(args.model_name, use_fast=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Load model with 4-bit quantization\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        args.model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        use_cache=False,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model.config.pretraining_tp = 1\n",
        "\n",
        "    # Prepare model for k-bit training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Apply LoRA\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=args.lora_alpha,\n",
        "        lora_dropout=args.lora_dropout,\n",
        "        r=args.lora_rank,\n",
        "        bias=args.lora_bias,\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Prepare datasets\n",
        "    train_dataset = dataset['train']\n",
        "    eval_dataset = dataset['dev']\n",
        "\n",
        "    # Define collate function with tokenizer and fixed max_length\n",
        "    def collate_fn_train(batch):\n",
        "        return collate_fn(batch, tokenizer, max_length=args.max_seq_length)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn_train\n",
        "    )\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn_train  # For metrics computation\n",
        "    )\n",
        "\n",
        "    # Prepare optimizer and scheduler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "    num_training_steps = args.num_epochs * len(train_dataloader)\n",
        "    num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
        "\n",
        "    # Training loop\n",
        "    global_step = 0\n",
        "    model.train()\n",
        "    for epoch in range(args.num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{args.num_epochs}\")\n",
        "\n",
        "        for step, batch in progress_bar:\n",
        "            input_ids = batch[\"input_ids\"].to(model.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
        "            labels = batch[\"labels\"].to(model.device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss / args.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            total_loss += loss.item() * args.gradient_accumulation_steps\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f\"{total_loss/(step+1):.5f}\"\n",
        "            })\n",
        "\n",
        "            if (global_step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % args.log_steps == 0:\n",
        "                wandb.log({\n",
        "                    \"train_loss\": loss.item(),\n",
        "                    \"lr\": scheduler.get_last_lr()[0]\n",
        "                }, step=global_step)\n",
        "\n",
        "            if global_step % args.eval_steps == 0:\n",
        "                # Call evaluate and receive generated samples\n",
        "                eval_loss, eval_metrics, generated_samples = evaluate(\n",
        "                    model, tokenizer, eval_dataloader, compute_metrics, num_samples=3, max_length=args.max_seq_length\n",
        "                )\n",
        "                wandb.log({\"eval_loss\": eval_loss, **eval_metrics}, step=global_step)\n",
        "\n",
        "                # Print generated samples to console\n",
        "                print(f\"\\n=== Evaluation at step {global_step} ===\")\n",
        "                for idx, sample in enumerate(generated_samples, 1):\n",
        "                    print(f\"Sample {idx}:\")\n",
        "                    print(f\"Prompt: {sample['prompt']}\")\n",
        "                    print(f\"Generated: {sample['generated']}\\n\")\n",
        "\n",
        "                # Log generated samples to wandb as a table\n",
        "                table = wandb.Table(columns=[\"Prompt\", \"Generated\"])\n",
        "                for sample in generated_samples:\n",
        "                    table.add_data(sample[\"prompt\"], sample[\"generated\"])\n",
        "                wandb.log({\"generated_samples\": table}, step=global_step)\n",
        "\n",
        "                model.train()\n",
        "\n",
        "            if global_step % args.save_steps == 0:\n",
        "                model.save_pretrained(f\"{args.output_dir}/checkpoint-{global_step}\")\n",
        "                tokenizer.save_pretrained(f\"{args.output_dir}/checkpoint-{global_step}\")\n",
        "\n",
        "    # Final evaluation\n",
        "    eval_loss, eval_metrics, generated_samples = evaluate(\n",
        "        model, tokenizer, eval_dataloader, compute_metrics, num_samples=3, max_length=args.max_seq_length\n",
        "    )\n",
        "    wandb.log({\"final_eval_loss\": eval_loss, **eval_metrics}, step=global_step)\n",
        "\n",
        "    # Print final generated samples\n",
        "    print(f\"\\n=== Final Evaluation at step {global_step} ===\")\n",
        "    for idx, sample in enumerate(generated_samples, 1):\n",
        "        print(f\"Sample {idx}:\")\n",
        "        print(f\"Prompt: {sample['prompt']}\")\n",
        "        print(f\"Generated: {sample['generated']}\\n\")\n",
        "\n",
        "    # Log final generated samples to wandb\n",
        "    table = wandb.Table(columns=[\"Prompt\", \"Generated\"])\n",
        "    for sample in generated_samples:\n",
        "        table.add_data(sample[\"prompt\"], sample[\"generated\"])\n",
        "    wandb.log({\"final_generated_samples\": table}, step=global_step)\n",
        "\n",
        "    # Save final model\n",
        "    model.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "    # Push to hub if specified\n",
        "    if args.push_to_hub and args.hub_model_id:\n",
        "        model.push_to_hub(args.hub_model_id)\n",
        "        tokenizer.push_to_hub(args.hub_model_id)\n",
        "\n",
        "def evaluate(model, tokenizer, eval_dataloader, compute_metrics, sample_indices=None, num_samples=3, max_length=512):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the evaluation dataset and generates samples.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer instance.\n",
        "        eval_dataloader (DataLoader): DataLoader for evaluation data.\n",
        "        compute_metrics (function): Function to compute metrics.\n",
        "        sample_indices (list, optional): List of indices from the eval_dataset to generate samples.\n",
        "        num_samples (int): Number of random samples to generate.\n",
        "        max_length (int): Maximum generation length.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average loss, metrics dictionary, and list of generated samples.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_exact_matches = 0\n",
        "    total_examples = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    generated_samples = []\n",
        "\n",
        "    # If sample_indices not provided, randomly select num_samples from eval_dataset\n",
        "    if sample_indices is None:\n",
        "        # Assuming eval_dataloader.dataset supports indexing\n",
        "        dataset_size = len(eval_dataloader.dataset)\n",
        "        sample_indices = random.sample(range(dataset_size), num_samples)\n",
        "\n",
        "    samples_for_generation = [eval_dataloader.dataset[idx] for idx in sample_indices]\n",
        "\n",
        "    # Pre-format the prompts for generation\n",
        "    prompts_for_generation = [\n",
        "        format_instruction(sample, include_response=False)\n",
        "        for sample in samples_for_generation\n",
        "    ]\n",
        "\n",
        "    # Tokenize the prompts for generation\n",
        "    encoded_prompts = tokenizer.batch_encode_plus(\n",
        "        prompts_for_generation,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    generation_input_ids = encoded_prompts[\"input_ids\"].to(model.device)\n",
        "    generation_attention_mask = encoded_prompts[\"attention_mask\"].to(model.device)\n",
        "\n",
        "    # Generate responses for the selected samples\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=generation_input_ids,\n",
        "            attention_mask=None,\n",
        "            max_length=128,\n",
        "            do_sample=False,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Extract the generated responses\n",
        "    for prompt, generated_text in zip(prompts_for_generation, generated_texts):\n",
        "        # Extract only the generated response\n",
        "        generated_response = generated_text.split(\"### Response:\\n\")[-1].strip()\n",
        "        generated_samples.append({\"prompt\": prompt, \"generated\": generated_response})\n",
        "\n",
        "    # Now evaluate loss and metrics batch-wise\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(model.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
        "            labels = batch[\"labels\"].to(model.device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item() * input_ids.size(0)  # Accumulate loss weighted by batch size\n",
        "\n",
        "            # Compute predictions\n",
        "            preds = torch.argmax(outputs.logits, dim=-1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "            # Compute exact matches for the batch\n",
        "            decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            for pred, label in zip(decoded_preds, decoded_labels):\n",
        "                pred_response = pred.split(\"### Response:\")[-1].strip()\n",
        "                label_response = label.split(\"### Response:\")[-1].strip()\n",
        "                match = recogs_exact_match(label_response, pred_response)\n",
        "                total_exact_matches += int(match)\n",
        "                total_examples += 1\n",
        "\n",
        "    # Compute metrics using the accumulated exact matches\n",
        "    accuracy = total_exact_matches / total_examples if total_examples > 0 else 0.0\n",
        "    metrics = {\"accuracy\": accuracy}\n",
        "\n",
        "    return (total_loss / total_examples, metrics, generated_samples)\n",
        "\n",
        "# Usage example\n",
        "args = TrainingArguments(\n",
        "    model_name=\"meta-llama/Llama-2-7b-hf\",\n",
        "    output_dir=\"llama-7b-hf-int4-recogs\",\n",
        "    num_epochs=3,\n",
        "    batch_size=4,\n",
        "    max_seq_length=128,\n",
        "    learning_rate=2e-4,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    eval_steps=100,\n",
        "    save_steps=250,\n",
        "    log_steps=32,\n",
        "    lora_rank=256,\n",
        "    lora_alpha=256,\n",
        "    lora_dropout=0.1,\n",
        "    lora_bias=\"all\",\n",
        "    gradient_accumulation_steps=4,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"diegomrodrigues/llama-7b-hf-int4-recogs\"\n",
        ")\n",
        "\n",
        "train(\n",
        "    args,\n",
        "    dataset=dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    format_instruction=format_instruction\n",
        ")\n"
      ],
      "metadata": {
        "id": "3F9kn_2jgDm-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "46ba513a493c44bdb10e39db27bdf597",
            "0676be91348b46eea4a393d8187726fe",
            "a85c13e1613c4370b72b68e3ff65838e",
            "f6c474b8295b46008a59d7d995ea2547",
            "7db2250c62e147e4b71b5d668fe3f8bd",
            "9fa9c2bcf97e4f638187a24e19208747",
            "150cd36782f34c46868f0fcfa9d143d0",
            "a9c08f5da6774e92bb8ab8779faadac8",
            "77e1c98a25144d148c3abdcd2582d3f2",
            "fea6cd8fd436425ea31f780ecb4fc815",
            "f091cc8676a34b2f81fff50357774589"
          ]
        },
        "outputId": "34f9f15e-6b6d-44cc-e53c-8421df4ab845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiego-mrodrigues11\u001b[0m (\u001b[33mnone-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240928_234401-vee38ka9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/none-ai/llama-7b-hf-int4-recogs-fine-tuning/runs/vee38ka9' target=\"_blank\">llama-7b-hf-int4-recogs</a></strong> to <a href='https://wandb.ai/none-ai/llama-7b-hf-int4-recogs-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/none-ai/llama-7b-hf-int4-recogs-fine-tuning' target=\"_blank\">https://wandb.ai/none-ai/llama-7b-hf-int4-recogs-fine-tuning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/none-ai/llama-7b-hf-int4-recogs-fine-tuning/runs/vee38ka9' target=\"_blank\">https://wandb.ai/none-ai/llama-7b-hf-int4-recogs-fine-tuning/runs/vee38ka9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46ba513a493c44bdb10e39db27bdf597"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from peft import PeftModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_and_evaluate_model(checkpoint_path, dataset, batch_size=8, max_length=512):\n",
        "    # Load tokenizer\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(checkpoint_path)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    # Load base model\n",
        "    base_model = LlamaForCausalLM.from_pretrained(\n",
        "        checkpoint_path,\n",
        "        load_in_8bit=False,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    # Load LoRA weights\n",
        "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare evaluation dataset\n",
        "    eval_dataset = dataset['dev']\n",
        "\n",
        "    # Define collate function\n",
        "    def collate_fn_eval(batch):\n",
        "        return collate_fn(batch, tokenizer, max_length=max_length)\n",
        "\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn_eval\n",
        "    )\n",
        "\n",
        "    # Run evaluation\n",
        "    total_loss = 0.0\n",
        "    total_exact_matches = 0\n",
        "    total_examples = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    generated_samples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(model.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
        "            labels = batch[\"labels\"].to(model.device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "            # Compute predictions\n",
        "            preds = torch.argmax(outputs.logits, dim=-1)\n",
        "            all_preds.append(preds.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "            # Compute exact matches for the batch\n",
        "            decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            for pred, label in zip(decoded_preds, decoded_labels):\n",
        "                pred_response = pred.split(\"### Response:\")[-1].strip()\n",
        "                label_response = label.split(\"### Response:\")[-1].strip()\n",
        "                match = recogs_exact_match(label_response, pred_response)\n",
        "                total_exact_matches += int(match)\n",
        "                total_examples += 1\n",
        "\n",
        "    # Compute metrics\n",
        "    average_loss = total_loss / total_examples\n",
        "    accuracy = total_exact_matches / total_examples if total_examples > 0 else 0.0\n",
        "\n",
        "    # Generate samples (optional)\n",
        "    num_samples = 15\n",
        "    sample_indices = random.sample(range(len(eval_dataset)), num_samples)\n",
        "    samples_for_generation = [eval_dataset[idx] for idx in sample_indices]\n",
        "    prompts_for_generation = [\n",
        "        format_instruction(sample, include_response=False)\n",
        "        for sample in samples_for_generation\n",
        "    ]\n",
        "\n",
        "    encoded_prompts = tokenizer.batch_encode_plus(\n",
        "        prompts_for_generation,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    generation_input_ids = encoded_prompts[\"input_ids\"].to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=generation_input_ids,\n",
        "        max_length=128,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    for prompt, generated_text in zip(prompts_for_generation, generated_texts):\n",
        "        generated_response = generated_text.split(\"### Response:\\n\")[-1].strip()\n",
        "        generated_samples.append({\"prompt\": prompt, \"generated\": generated_response})\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Evaluation Results:\")\n",
        "    print(f\"Average Loss: {average_loss:.4f}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    print(\"\\nGenerated Samples:\")\n",
        "    for idx, sample in enumerate(generated_samples, 1):\n",
        "        print(f\"Sample {idx}:\")\n",
        "        print(f\"Prompt: {sample['prompt']}\")\n",
        "        print(f\"Generated: {sample['generated']}\\n\")\n",
        "\n",
        "    return average_loss, accuracy, generated_samples\n",
        "\n",
        "checkpoint_path = \"/content/llama-7b-hf-int4-recogs/checkpoint-9750\"\n",
        "\n",
        "dataset['dev'] = Dataset.from_pandas(dataset['dev'])\n",
        "\n",
        "loss, accuracy, samples = load_and_evaluate_model(checkpoint_path, dataset)"
      ],
      "metadata": {
        "id": "b9fHP3PVgDqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "from peft import PeftModel, AutoPeftModelForCausalLM\n",
        "from huggingface_hub import HfApi, create_repo\n",
        "import os\n",
        "\n",
        "def upload_to_huggingface_hub(checkpoint_path, repo_id):\n",
        "    # Load tokenizer\n",
        "    tokenizer = LlamaTokenizer.from_pretrained(checkpoint_path)\n",
        "\n",
        "    # Load base model\n",
        "    base_model =  AutoPeftModelForCausalLM.from_pretrained(\n",
        "        checkpoint_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    # Load LoRA weights\n",
        "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
        "\n",
        "    # Merge LoRA weights with base model\n",
        "    merged_model = model.merge_and_unload()\n",
        "\n",
        "    # Save the merged model and tokenizer to a temporary directory\n",
        "    temp_save_directory = \"temp_save_directory\"\n",
        "    merged_model.save_pretrained(temp_save_directory)\n",
        "    tokenizer.save_pretrained(temp_save_directory)\n",
        "\n",
        "    # Push the model to the Hugging Face Hub\n",
        "    merged_model.push_to_hub(repo_id)\n",
        "    tokenizer.push_to_hub(repo_id)\n",
        "\n",
        "    print(f\"Model and tokenizer successfully uploaded to {repo_id}\")\n",
        "\n",
        "    # Clean up temporary directory\n",
        "    for root, dirs, files in os.walk(temp_save_directory, topdown=False):\n",
        "        for name in files:\n",
        "            os.remove(os.path.join(root, name))\n",
        "        for name in dirs:\n",
        "            os.rmdir(os.path.join(root, name))\n",
        "    os.rmdir(temp_save_directory)\n",
        "\n",
        "    print(\"Temporary directory cleaned up\")\n",
        "\n",
        "\n",
        "checkpoint_path = \"/content/llama-7b-hf-int4-recogs/checkpoint-9750\"\n",
        "\n",
        "upload_to_huggingface_hub(checkpoint_path, args.hub_model_id)"
      ],
      "metadata": {
        "id": "Wc7oB4oG-mg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Preparar entrada\n",
        "input_text = \"1 + 1 = 2, 2 + 1 = 3, 4 + 4 =\"\n",
        "#input_text = \"Hey, are you conscious? Can you talk to me?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Gerar texto\n",
        "outputs = model.generate(inputs.input_ids, max_length=60, do_sample=True)\n",
        "\n",
        "#predicted_token_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "print(generated_text)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "GUmrlUXJGu9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NtETrcGkTLPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ChXiden17MMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Como aproveitar ao máximo sua assinatura do Colab",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46ba513a493c44bdb10e39db27bdf597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0676be91348b46eea4a393d8187726fe",
              "IPY_MODEL_a85c13e1613c4370b72b68e3ff65838e",
              "IPY_MODEL_f6c474b8295b46008a59d7d995ea2547"
            ],
            "layout": "IPY_MODEL_7db2250c62e147e4b71b5d668fe3f8bd"
          }
        },
        "0676be91348b46eea4a393d8187726fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fa9c2bcf97e4f638187a24e19208747",
            "placeholder": "​",
            "style": "IPY_MODEL_150cd36782f34c46868f0fcfa9d143d0",
            "value": "Loading checkpoint shards:   0%"
          }
        },
        "a85c13e1613c4370b72b68e3ff65838e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9c08f5da6774e92bb8ab8779faadac8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77e1c98a25144d148c3abdcd2582d3f2",
            "value": 0
          }
        },
        "f6c474b8295b46008a59d7d995ea2547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fea6cd8fd436425ea31f780ecb4fc815",
            "placeholder": "​",
            "style": "IPY_MODEL_f091cc8676a34b2f81fff50357774589",
            "value": " 0/2 [00:00&lt;?, ?it/s]"
          }
        },
        "7db2250c62e147e4b71b5d668fe3f8bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fa9c2bcf97e4f638187a24e19208747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "150cd36782f34c46868f0fcfa9d143d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9c08f5da6774e92bb8ab8779faadac8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77e1c98a25144d148c3abdcd2582d3f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fea6cd8fd436425ea31f780ecb4fc815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f091cc8676a34b2f81fff50357774589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}